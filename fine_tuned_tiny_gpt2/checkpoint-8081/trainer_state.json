{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 8081,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006187353050365054,
      "grad_norm": 0.38433608412742615,
      "learning_rate": 4.9975250587798547e-05,
      "loss": 10.6464,
      "step": 5
    },
    {
      "epoch": 0.0012374706100730108,
      "grad_norm": 0.33642077445983887,
      "learning_rate": 4.994431382254672e-05,
      "loss": 10.6475,
      "step": 10
    },
    {
      "epoch": 0.0018562059151095163,
      "grad_norm": 0.2791602313518524,
      "learning_rate": 4.9913377057294893e-05,
      "loss": 10.6556,
      "step": 15
    },
    {
      "epoch": 0.0024749412201460217,
      "grad_norm": 0.4341129958629608,
      "learning_rate": 4.988244029204307e-05,
      "loss": 10.6482,
      "step": 20
    },
    {
      "epoch": 0.003093676525182527,
      "grad_norm": 0.495815247297287,
      "learning_rate": 4.985150352679124e-05,
      "loss": 10.6534,
      "step": 25
    },
    {
      "epoch": 0.0037124118302190325,
      "grad_norm": 0.5159855484962463,
      "learning_rate": 4.9820566761539414e-05,
      "loss": 10.6504,
      "step": 30
    },
    {
      "epoch": 0.004331147135255537,
      "grad_norm": 0.2894953191280365,
      "learning_rate": 4.978962999628759e-05,
      "loss": 10.6425,
      "step": 35
    },
    {
      "epoch": 0.004949882440292043,
      "grad_norm": 0.32430794835090637,
      "learning_rate": 4.975869323103576e-05,
      "loss": 10.6413,
      "step": 40
    },
    {
      "epoch": 0.0055686177453285485,
      "grad_norm": 0.38937753438949585,
      "learning_rate": 4.972775646578394e-05,
      "loss": 10.6442,
      "step": 45
    },
    {
      "epoch": 0.006187353050365054,
      "grad_norm": 0.3509448766708374,
      "learning_rate": 4.9696819700532114e-05,
      "loss": 10.6544,
      "step": 50
    },
    {
      "epoch": 0.006806088355401559,
      "grad_norm": 0.4220117926597595,
      "learning_rate": 4.966588293528029e-05,
      "loss": 10.6414,
      "step": 55
    },
    {
      "epoch": 0.007424823660438065,
      "grad_norm": 0.5998044610023499,
      "learning_rate": 4.963494617002846e-05,
      "loss": 10.6454,
      "step": 60
    },
    {
      "epoch": 0.00804355896547457,
      "grad_norm": 0.4515741467475891,
      "learning_rate": 4.960400940477664e-05,
      "loss": 10.6461,
      "step": 65
    },
    {
      "epoch": 0.008662294270511075,
      "grad_norm": 0.37269946932792664,
      "learning_rate": 4.9573072639524815e-05,
      "loss": 10.6429,
      "step": 70
    },
    {
      "epoch": 0.009281029575547581,
      "grad_norm": 0.3156626224517822,
      "learning_rate": 4.954213587427299e-05,
      "loss": 10.6366,
      "step": 75
    },
    {
      "epoch": 0.009899764880584087,
      "grad_norm": 0.43253329396247864,
      "learning_rate": 4.951119910902116e-05,
      "loss": 10.6398,
      "step": 80
    },
    {
      "epoch": 0.010518500185620592,
      "grad_norm": 0.2990410327911377,
      "learning_rate": 4.948026234376934e-05,
      "loss": 10.6543,
      "step": 85
    },
    {
      "epoch": 0.011137235490657097,
      "grad_norm": 0.6944503784179688,
      "learning_rate": 4.9449325578517516e-05,
      "loss": 10.6448,
      "step": 90
    },
    {
      "epoch": 0.011755970795693602,
      "grad_norm": 0.9673563241958618,
      "learning_rate": 4.941838881326569e-05,
      "loss": 10.6353,
      "step": 95
    },
    {
      "epoch": 0.012374706100730107,
      "grad_norm": 0.5590534806251526,
      "learning_rate": 4.938745204801386e-05,
      "loss": 10.6352,
      "step": 100
    },
    {
      "epoch": 0.012993441405766613,
      "grad_norm": 0.6501448750495911,
      "learning_rate": 4.9356515282762036e-05,
      "loss": 10.6387,
      "step": 105
    },
    {
      "epoch": 0.013612176710803118,
      "grad_norm": 0.6892060041427612,
      "learning_rate": 4.932557851751021e-05,
      "loss": 10.6333,
      "step": 110
    },
    {
      "epoch": 0.014230912015839623,
      "grad_norm": 0.4665723145008087,
      "learning_rate": 4.929464175225838e-05,
      "loss": 10.6439,
      "step": 115
    },
    {
      "epoch": 0.01484964732087613,
      "grad_norm": 0.6141244769096375,
      "learning_rate": 4.9263704987006556e-05,
      "loss": 10.6301,
      "step": 120
    },
    {
      "epoch": 0.015468382625912635,
      "grad_norm": 0.3958069980144501,
      "learning_rate": 4.9232768221754736e-05,
      "loss": 10.6295,
      "step": 125
    },
    {
      "epoch": 0.01608711793094914,
      "grad_norm": 0.6620494723320007,
      "learning_rate": 4.920183145650291e-05,
      "loss": 10.6335,
      "step": 130
    },
    {
      "epoch": 0.016705853235985644,
      "grad_norm": 0.6846432685852051,
      "learning_rate": 4.917089469125108e-05,
      "loss": 10.6255,
      "step": 135
    },
    {
      "epoch": 0.01732458854102215,
      "grad_norm": 0.7015087604522705,
      "learning_rate": 4.913995792599926e-05,
      "loss": 10.6274,
      "step": 140
    },
    {
      "epoch": 0.017943323846058658,
      "grad_norm": 0.4941796362400055,
      "learning_rate": 4.910902116074744e-05,
      "loss": 10.6327,
      "step": 145
    },
    {
      "epoch": 0.018562059151095163,
      "grad_norm": 0.3403337001800537,
      "learning_rate": 4.907808439549561e-05,
      "loss": 10.6235,
      "step": 150
    },
    {
      "epoch": 0.019180794456131668,
      "grad_norm": 0.33198481798171997,
      "learning_rate": 4.9047147630243784e-05,
      "loss": 10.6184,
      "step": 155
    },
    {
      "epoch": 0.019799529761168173,
      "grad_norm": 0.4340223968029022,
      "learning_rate": 4.901621086499196e-05,
      "loss": 10.6261,
      "step": 160
    },
    {
      "epoch": 0.02041826506620468,
      "grad_norm": 0.3958792984485626,
      "learning_rate": 4.898527409974014e-05,
      "loss": 10.6145,
      "step": 165
    },
    {
      "epoch": 0.021037000371241184,
      "grad_norm": 0.42065858840942383,
      "learning_rate": 4.895433733448831e-05,
      "loss": 10.622,
      "step": 170
    },
    {
      "epoch": 0.02165573567627769,
      "grad_norm": 0.326460063457489,
      "learning_rate": 4.8923400569236485e-05,
      "loss": 10.6259,
      "step": 175
    },
    {
      "epoch": 0.022274470981314194,
      "grad_norm": 0.31072089076042175,
      "learning_rate": 4.889246380398466e-05,
      "loss": 10.6164,
      "step": 180
    },
    {
      "epoch": 0.0228932062863507,
      "grad_norm": 0.4563029408454895,
      "learning_rate": 4.886152703873284e-05,
      "loss": 10.612,
      "step": 185
    },
    {
      "epoch": 0.023511941591387205,
      "grad_norm": 0.3688024878501892,
      "learning_rate": 4.883059027348101e-05,
      "loss": 10.6168,
      "step": 190
    },
    {
      "epoch": 0.02413067689642371,
      "grad_norm": 0.44829609990119934,
      "learning_rate": 4.879965350822918e-05,
      "loss": 10.6062,
      "step": 195
    },
    {
      "epoch": 0.024749412201460215,
      "grad_norm": 0.32970112562179565,
      "learning_rate": 4.876871674297735e-05,
      "loss": 10.6101,
      "step": 200
    },
    {
      "epoch": 0.02536814750649672,
      "grad_norm": 0.31493252515792847,
      "learning_rate": 4.873777997772553e-05,
      "loss": 10.6071,
      "step": 205
    },
    {
      "epoch": 0.025986882811533225,
      "grad_norm": 0.5008566975593567,
      "learning_rate": 4.8706843212473705e-05,
      "loss": 10.6127,
      "step": 210
    },
    {
      "epoch": 0.02660561811656973,
      "grad_norm": 0.32802286744117737,
      "learning_rate": 4.867590644722188e-05,
      "loss": 10.6061,
      "step": 215
    },
    {
      "epoch": 0.027224353421606236,
      "grad_norm": 0.3918668031692505,
      "learning_rate": 4.864496968197005e-05,
      "loss": 10.6136,
      "step": 220
    },
    {
      "epoch": 0.02784308872664274,
      "grad_norm": 0.6306629776954651,
      "learning_rate": 4.861403291671823e-05,
      "loss": 10.6005,
      "step": 225
    },
    {
      "epoch": 0.028461824031679246,
      "grad_norm": 0.3564651608467102,
      "learning_rate": 4.8583096151466406e-05,
      "loss": 10.601,
      "step": 230
    },
    {
      "epoch": 0.02908055933671575,
      "grad_norm": 0.3351210057735443,
      "learning_rate": 4.855215938621458e-05,
      "loss": 10.6032,
      "step": 235
    },
    {
      "epoch": 0.02969929464175226,
      "grad_norm": 0.34113621711730957,
      "learning_rate": 4.852122262096275e-05,
      "loss": 10.5998,
      "step": 240
    },
    {
      "epoch": 0.030318029946788765,
      "grad_norm": 0.36292964220046997,
      "learning_rate": 4.849028585571093e-05,
      "loss": 10.5999,
      "step": 245
    },
    {
      "epoch": 0.03093676525182527,
      "grad_norm": 0.3466629087924957,
      "learning_rate": 4.8459349090459107e-05,
      "loss": 10.5955,
      "step": 250
    },
    {
      "epoch": 0.03155550055686177,
      "grad_norm": 0.3260418772697449,
      "learning_rate": 4.842841232520728e-05,
      "loss": 10.5981,
      "step": 255
    },
    {
      "epoch": 0.03217423586189828,
      "grad_norm": 0.32981377840042114,
      "learning_rate": 4.8397475559955454e-05,
      "loss": 10.6018,
      "step": 260
    },
    {
      "epoch": 0.03279297116693478,
      "grad_norm": 0.32741114497184753,
      "learning_rate": 4.8366538794703634e-05,
      "loss": 10.6114,
      "step": 265
    },
    {
      "epoch": 0.03341170647197129,
      "grad_norm": 0.33609309792518616,
      "learning_rate": 4.833560202945181e-05,
      "loss": 10.587,
      "step": 270
    },
    {
      "epoch": 0.03403044177700779,
      "grad_norm": 0.32069578766822815,
      "learning_rate": 4.830466526419998e-05,
      "loss": 10.6016,
      "step": 275
    },
    {
      "epoch": 0.0346491770820443,
      "grad_norm": 0.3411974608898163,
      "learning_rate": 4.8273728498948154e-05,
      "loss": 10.5899,
      "step": 280
    },
    {
      "epoch": 0.0352679123870808,
      "grad_norm": 0.32962891459465027,
      "learning_rate": 4.824279173369633e-05,
      "loss": 10.5982,
      "step": 285
    },
    {
      "epoch": 0.035886647692117316,
      "grad_norm": 0.32514089345932007,
      "learning_rate": 4.82118549684445e-05,
      "loss": 10.5903,
      "step": 290
    },
    {
      "epoch": 0.03650538299715382,
      "grad_norm": 0.3377555012702942,
      "learning_rate": 4.8180918203192674e-05,
      "loss": 10.591,
      "step": 295
    },
    {
      "epoch": 0.037124118302190326,
      "grad_norm": 0.35373833775520325,
      "learning_rate": 4.814998143794085e-05,
      "loss": 10.596,
      "step": 300
    },
    {
      "epoch": 0.03774285360722683,
      "grad_norm": 0.4389787018299103,
      "learning_rate": 4.811904467268903e-05,
      "loss": 10.5892,
      "step": 305
    },
    {
      "epoch": 0.038361588912263336,
      "grad_norm": 0.33526870608329773,
      "learning_rate": 4.80881079074372e-05,
      "loss": 10.5812,
      "step": 310
    },
    {
      "epoch": 0.03898032421729984,
      "grad_norm": 0.34910786151885986,
      "learning_rate": 4.8057171142185375e-05,
      "loss": 10.5855,
      "step": 315
    },
    {
      "epoch": 0.03959905952233635,
      "grad_norm": 0.3489217460155487,
      "learning_rate": 4.802623437693355e-05,
      "loss": 10.5855,
      "step": 320
    },
    {
      "epoch": 0.04021779482737285,
      "grad_norm": 0.48960599303245544,
      "learning_rate": 4.799529761168172e-05,
      "loss": 10.5836,
      "step": 325
    },
    {
      "epoch": 0.04083653013240936,
      "grad_norm": 0.3157805800437927,
      "learning_rate": 4.79643608464299e-05,
      "loss": 10.5797,
      "step": 330
    },
    {
      "epoch": 0.04145526543744586,
      "grad_norm": 0.38379406929016113,
      "learning_rate": 4.7933424081178076e-05,
      "loss": 10.5812,
      "step": 335
    },
    {
      "epoch": 0.04207400074248237,
      "grad_norm": 0.3388397991657257,
      "learning_rate": 4.790248731592625e-05,
      "loss": 10.5811,
      "step": 340
    },
    {
      "epoch": 0.04269273604751887,
      "grad_norm": 0.7471635937690735,
      "learning_rate": 4.787155055067442e-05,
      "loss": 10.5854,
      "step": 345
    },
    {
      "epoch": 0.04331147135255538,
      "grad_norm": 0.36290040612220764,
      "learning_rate": 4.78406137854226e-05,
      "loss": 10.5752,
      "step": 350
    },
    {
      "epoch": 0.04393020665759188,
      "grad_norm": 0.33151572942733765,
      "learning_rate": 4.7809677020170776e-05,
      "loss": 10.5818,
      "step": 355
    },
    {
      "epoch": 0.04454894196262839,
      "grad_norm": 0.33330821990966797,
      "learning_rate": 4.777874025491895e-05,
      "loss": 10.5743,
      "step": 360
    },
    {
      "epoch": 0.045167677267664894,
      "grad_norm": 0.3402567207813263,
      "learning_rate": 4.774780348966712e-05,
      "loss": 10.5729,
      "step": 365
    },
    {
      "epoch": 0.0457864125727014,
      "grad_norm": 0.5003851056098938,
      "learning_rate": 4.7716866724415297e-05,
      "loss": 10.5745,
      "step": 370
    },
    {
      "epoch": 0.046405147877737904,
      "grad_norm": 0.3325635492801666,
      "learning_rate": 4.768592995916347e-05,
      "loss": 10.5761,
      "step": 375
    },
    {
      "epoch": 0.04702388318277441,
      "grad_norm": 0.33814266324043274,
      "learning_rate": 4.7654993193911643e-05,
      "loss": 10.5732,
      "step": 380
    },
    {
      "epoch": 0.047642618487810914,
      "grad_norm": 0.31436648964881897,
      "learning_rate": 4.762405642865982e-05,
      "loss": 10.5666,
      "step": 385
    },
    {
      "epoch": 0.04826135379284742,
      "grad_norm": 0.34613728523254395,
      "learning_rate": 4.7593119663408e-05,
      "loss": 10.5708,
      "step": 390
    },
    {
      "epoch": 0.048880089097883925,
      "grad_norm": 0.33626705408096313,
      "learning_rate": 4.756218289815617e-05,
      "loss": 10.5667,
      "step": 395
    },
    {
      "epoch": 0.04949882440292043,
      "grad_norm": 0.3408365547657013,
      "learning_rate": 4.7531246132904344e-05,
      "loss": 10.5879,
      "step": 400
    },
    {
      "epoch": 0.050117559707956935,
      "grad_norm": 0.33166614174842834,
      "learning_rate": 4.750030936765252e-05,
      "loss": 10.5636,
      "step": 405
    },
    {
      "epoch": 0.05073629501299344,
      "grad_norm": 0.34858080744743347,
      "learning_rate": 4.74693726024007e-05,
      "loss": 10.571,
      "step": 410
    },
    {
      "epoch": 0.051355030318029946,
      "grad_norm": 0.40991687774658203,
      "learning_rate": 4.743843583714887e-05,
      "loss": 10.5567,
      "step": 415
    },
    {
      "epoch": 0.05197376562306645,
      "grad_norm": 0.5326773524284363,
      "learning_rate": 4.7407499071897045e-05,
      "loss": 10.5677,
      "step": 420
    },
    {
      "epoch": 0.052592500928102956,
      "grad_norm": 0.34831467270851135,
      "learning_rate": 4.737656230664522e-05,
      "loss": 10.5598,
      "step": 425
    },
    {
      "epoch": 0.05321123623313946,
      "grad_norm": 0.3440796732902527,
      "learning_rate": 4.73456255413934e-05,
      "loss": 10.5609,
      "step": 430
    },
    {
      "epoch": 0.053829971538175966,
      "grad_norm": 0.3454584777355194,
      "learning_rate": 4.731468877614157e-05,
      "loss": 10.5614,
      "step": 435
    },
    {
      "epoch": 0.05444870684321247,
      "grad_norm": 0.4506421685218811,
      "learning_rate": 4.7283752010889745e-05,
      "loss": 10.5677,
      "step": 440
    },
    {
      "epoch": 0.05506744214824898,
      "grad_norm": 0.3514578640460968,
      "learning_rate": 4.725281524563792e-05,
      "loss": 10.5518,
      "step": 445
    },
    {
      "epoch": 0.05568617745328548,
      "grad_norm": 0.3954203724861145,
      "learning_rate": 4.722187848038609e-05,
      "loss": 10.564,
      "step": 450
    },
    {
      "epoch": 0.05630491275832199,
      "grad_norm": 0.35762426257133484,
      "learning_rate": 4.7190941715134265e-05,
      "loss": 10.5503,
      "step": 455
    },
    {
      "epoch": 0.05692364806335849,
      "grad_norm": 0.3587406575679779,
      "learning_rate": 4.716000494988244e-05,
      "loss": 10.549,
      "step": 460
    },
    {
      "epoch": 0.057542383368395,
      "grad_norm": 0.3355630934238434,
      "learning_rate": 4.712906818463061e-05,
      "loss": 10.5619,
      "step": 465
    },
    {
      "epoch": 0.0581611186734315,
      "grad_norm": 0.3479324281215668,
      "learning_rate": 4.709813141937879e-05,
      "loss": 10.5517,
      "step": 470
    },
    {
      "epoch": 0.05877985397846801,
      "grad_norm": 0.34833410382270813,
      "learning_rate": 4.7067194654126966e-05,
      "loss": 10.5502,
      "step": 475
    },
    {
      "epoch": 0.05939858928350452,
      "grad_norm": 0.3732917010784149,
      "learning_rate": 4.703625788887514e-05,
      "loss": 10.5484,
      "step": 480
    },
    {
      "epoch": 0.060017324588541025,
      "grad_norm": 0.34865397214889526,
      "learning_rate": 4.700532112362331e-05,
      "loss": 10.5435,
      "step": 485
    },
    {
      "epoch": 0.06063605989357753,
      "grad_norm": 0.35923442244529724,
      "learning_rate": 4.697438435837149e-05,
      "loss": 10.5504,
      "step": 490
    },
    {
      "epoch": 0.061254795198614036,
      "grad_norm": 0.3505726754665375,
      "learning_rate": 4.694344759311967e-05,
      "loss": 10.5492,
      "step": 495
    },
    {
      "epoch": 0.06187353050365054,
      "grad_norm": 0.343461275100708,
      "learning_rate": 4.691251082786784e-05,
      "loss": 10.5498,
      "step": 500
    },
    {
      "epoch": 0.062492265808687046,
      "grad_norm": 0.3497248888015747,
      "learning_rate": 4.6881574062616014e-05,
      "loss": 10.5443,
      "step": 505
    },
    {
      "epoch": 0.06311100111372354,
      "grad_norm": 0.34801778197288513,
      "learning_rate": 4.6850637297364194e-05,
      "loss": 10.5499,
      "step": 510
    },
    {
      "epoch": 0.06372973641876005,
      "grad_norm": 0.3546365797519684,
      "learning_rate": 4.681970053211237e-05,
      "loss": 10.5434,
      "step": 515
    },
    {
      "epoch": 0.06434847172379655,
      "grad_norm": 0.35250282287597656,
      "learning_rate": 4.678876376686054e-05,
      "loss": 10.5454,
      "step": 520
    },
    {
      "epoch": 0.06496720702883306,
      "grad_norm": 0.3672810196876526,
      "learning_rate": 4.6757827001608714e-05,
      "loss": 10.5396,
      "step": 525
    },
    {
      "epoch": 0.06558594233386957,
      "grad_norm": 0.3531271815299988,
      "learning_rate": 4.672689023635689e-05,
      "loss": 10.5516,
      "step": 530
    },
    {
      "epoch": 0.06620467763890607,
      "grad_norm": 0.3600614070892334,
      "learning_rate": 4.669595347110506e-05,
      "loss": 10.5484,
      "step": 535
    },
    {
      "epoch": 0.06682341294394258,
      "grad_norm": 0.35729631781578064,
      "learning_rate": 4.6665016705853234e-05,
      "loss": 10.5385,
      "step": 540
    },
    {
      "epoch": 0.06744214824897908,
      "grad_norm": 0.3460046350955963,
      "learning_rate": 4.663407994060141e-05,
      "loss": 10.5369,
      "step": 545
    },
    {
      "epoch": 0.06806088355401559,
      "grad_norm": 0.3586578369140625,
      "learning_rate": 4.660314317534959e-05,
      "loss": 10.5425,
      "step": 550
    },
    {
      "epoch": 0.06867961885905209,
      "grad_norm": 0.3856300711631775,
      "learning_rate": 4.657220641009776e-05,
      "loss": 10.5315,
      "step": 555
    },
    {
      "epoch": 0.0692983541640886,
      "grad_norm": 0.3163336515426636,
      "learning_rate": 4.6541269644845935e-05,
      "loss": 10.5316,
      "step": 560
    },
    {
      "epoch": 0.0699170894691251,
      "grad_norm": 0.35090601444244385,
      "learning_rate": 4.651033287959411e-05,
      "loss": 10.5349,
      "step": 565
    },
    {
      "epoch": 0.0705358247741616,
      "grad_norm": 0.3768300712108612,
      "learning_rate": 4.647939611434229e-05,
      "loss": 10.5254,
      "step": 570
    },
    {
      "epoch": 0.07115456007919813,
      "grad_norm": 0.4794367551803589,
      "learning_rate": 4.644845934909046e-05,
      "loss": 10.5233,
      "step": 575
    },
    {
      "epoch": 0.07177329538423463,
      "grad_norm": 0.3694626986980438,
      "learning_rate": 4.6417522583838636e-05,
      "loss": 10.5369,
      "step": 580
    },
    {
      "epoch": 0.07239203068927114,
      "grad_norm": 0.3407038152217865,
      "learning_rate": 4.638658581858681e-05,
      "loss": 10.5317,
      "step": 585
    },
    {
      "epoch": 0.07301076599430764,
      "grad_norm": 0.5987392663955688,
      "learning_rate": 4.635564905333499e-05,
      "loss": 10.5288,
      "step": 590
    },
    {
      "epoch": 0.07362950129934415,
      "grad_norm": 0.35415899753570557,
      "learning_rate": 4.632471228808316e-05,
      "loss": 10.525,
      "step": 595
    },
    {
      "epoch": 0.07424823660438065,
      "grad_norm": 0.36281314492225647,
      "learning_rate": 4.6293775522831336e-05,
      "loss": 10.5242,
      "step": 600
    },
    {
      "epoch": 0.07486697190941716,
      "grad_norm": 0.35566556453704834,
      "learning_rate": 4.626283875757951e-05,
      "loss": 10.5289,
      "step": 605
    },
    {
      "epoch": 0.07548570721445366,
      "grad_norm": 0.3554597496986389,
      "learning_rate": 4.623190199232768e-05,
      "loss": 10.526,
      "step": 610
    },
    {
      "epoch": 0.07610444251949017,
      "grad_norm": 0.8278569579124451,
      "learning_rate": 4.6200965227075857e-05,
      "loss": 10.5261,
      "step": 615
    },
    {
      "epoch": 0.07672317782452667,
      "grad_norm": 0.34793296456336975,
      "learning_rate": 4.617002846182403e-05,
      "loss": 10.5253,
      "step": 620
    },
    {
      "epoch": 0.07734191312956318,
      "grad_norm": 0.364350289106369,
      "learning_rate": 4.6139091696572203e-05,
      "loss": 10.5229,
      "step": 625
    },
    {
      "epoch": 0.07796064843459968,
      "grad_norm": 0.35644757747650146,
      "learning_rate": 4.6108154931320384e-05,
      "loss": 10.5173,
      "step": 630
    },
    {
      "epoch": 0.07857938373963619,
      "grad_norm": 0.36302828788757324,
      "learning_rate": 4.607721816606856e-05,
      "loss": 10.5196,
      "step": 635
    },
    {
      "epoch": 0.0791981190446727,
      "grad_norm": 0.35328665375709534,
      "learning_rate": 4.604628140081673e-05,
      "loss": 10.5241,
      "step": 640
    },
    {
      "epoch": 0.0798168543497092,
      "grad_norm": 0.34288063645362854,
      "learning_rate": 4.6015344635564904e-05,
      "loss": 10.5256,
      "step": 645
    },
    {
      "epoch": 0.0804355896547457,
      "grad_norm": 0.3657781779766083,
      "learning_rate": 4.5984407870313084e-05,
      "loss": 10.5237,
      "step": 650
    },
    {
      "epoch": 0.08105432495978221,
      "grad_norm": 0.35372617840766907,
      "learning_rate": 4.595347110506126e-05,
      "loss": 10.5171,
      "step": 655
    },
    {
      "epoch": 0.08167306026481871,
      "grad_norm": 0.36542952060699463,
      "learning_rate": 4.592253433980943e-05,
      "loss": 10.5233,
      "step": 660
    },
    {
      "epoch": 0.08229179556985522,
      "grad_norm": 0.36575087904930115,
      "learning_rate": 4.5891597574557605e-05,
      "loss": 10.5162,
      "step": 665
    },
    {
      "epoch": 0.08291053087489172,
      "grad_norm": 0.3759434223175049,
      "learning_rate": 4.5860660809305785e-05,
      "loss": 10.5077,
      "step": 670
    },
    {
      "epoch": 0.08352926617992823,
      "grad_norm": 0.3711138367652893,
      "learning_rate": 4.582972404405396e-05,
      "loss": 10.5152,
      "step": 675
    },
    {
      "epoch": 0.08414800148496474,
      "grad_norm": 0.35776564478874207,
      "learning_rate": 4.579878727880213e-05,
      "loss": 10.5191,
      "step": 680
    },
    {
      "epoch": 0.08476673679000124,
      "grad_norm": 0.365697979927063,
      "learning_rate": 4.5767850513550305e-05,
      "loss": 10.511,
      "step": 685
    },
    {
      "epoch": 0.08538547209503775,
      "grad_norm": 0.3529924750328064,
      "learning_rate": 4.5736913748298485e-05,
      "loss": 10.4997,
      "step": 690
    },
    {
      "epoch": 0.08600420740007425,
      "grad_norm": 0.37129953503608704,
      "learning_rate": 4.570597698304666e-05,
      "loss": 10.5057,
      "step": 695
    },
    {
      "epoch": 0.08662294270511076,
      "grad_norm": 0.36773690581321716,
      "learning_rate": 4.567504021779483e-05,
      "loss": 10.5097,
      "step": 700
    },
    {
      "epoch": 0.08724167801014726,
      "grad_norm": 0.3563931882381439,
      "learning_rate": 4.5644103452543e-05,
      "loss": 10.5105,
      "step": 705
    },
    {
      "epoch": 0.08786041331518377,
      "grad_norm": 0.3719389736652374,
      "learning_rate": 4.561316668729118e-05,
      "loss": 10.5071,
      "step": 710
    },
    {
      "epoch": 0.08847914862022027,
      "grad_norm": 0.34618324041366577,
      "learning_rate": 4.558222992203935e-05,
      "loss": 10.5158,
      "step": 715
    },
    {
      "epoch": 0.08909788392525678,
      "grad_norm": 0.3729228079319,
      "learning_rate": 4.5551293156787526e-05,
      "loss": 10.5111,
      "step": 720
    },
    {
      "epoch": 0.08971661923029328,
      "grad_norm": 0.38062313199043274,
      "learning_rate": 4.55203563915357e-05,
      "loss": 10.5029,
      "step": 725
    },
    {
      "epoch": 0.09033535453532979,
      "grad_norm": 0.3418598473072052,
      "learning_rate": 4.548941962628388e-05,
      "loss": 10.5081,
      "step": 730
    },
    {
      "epoch": 0.09095408984036629,
      "grad_norm": 0.3693942129611969,
      "learning_rate": 4.545848286103205e-05,
      "loss": 10.502,
      "step": 735
    },
    {
      "epoch": 0.0915728251454028,
      "grad_norm": 0.38435161113739014,
      "learning_rate": 4.542754609578023e-05,
      "loss": 10.5038,
      "step": 740
    },
    {
      "epoch": 0.0921915604504393,
      "grad_norm": 0.3852873146533966,
      "learning_rate": 4.53966093305284e-05,
      "loss": 10.493,
      "step": 745
    },
    {
      "epoch": 0.09281029575547581,
      "grad_norm": 0.37271246314048767,
      "learning_rate": 4.536567256527658e-05,
      "loss": 10.4981,
      "step": 750
    },
    {
      "epoch": 0.09342903106051231,
      "grad_norm": 0.3930308222770691,
      "learning_rate": 4.5334735800024754e-05,
      "loss": 10.4903,
      "step": 755
    },
    {
      "epoch": 0.09404776636554882,
      "grad_norm": 0.3895424008369446,
      "learning_rate": 4.530379903477293e-05,
      "loss": 10.4938,
      "step": 760
    },
    {
      "epoch": 0.09466650167058532,
      "grad_norm": 0.3779449462890625,
      "learning_rate": 4.52728622695211e-05,
      "loss": 10.4983,
      "step": 765
    },
    {
      "epoch": 0.09528523697562183,
      "grad_norm": 0.36139070987701416,
      "learning_rate": 4.524192550426928e-05,
      "loss": 10.5031,
      "step": 770
    },
    {
      "epoch": 0.09590397228065833,
      "grad_norm": 0.3669978380203247,
      "learning_rate": 4.5210988739017454e-05,
      "loss": 10.489,
      "step": 775
    },
    {
      "epoch": 0.09652270758569484,
      "grad_norm": 0.35949385166168213,
      "learning_rate": 4.518005197376563e-05,
      "loss": 10.4882,
      "step": 780
    },
    {
      "epoch": 0.09714144289073134,
      "grad_norm": 0.370990127325058,
      "learning_rate": 4.51491152085138e-05,
      "loss": 10.4927,
      "step": 785
    },
    {
      "epoch": 0.09776017819576785,
      "grad_norm": 0.3949027955532074,
      "learning_rate": 4.5118178443261975e-05,
      "loss": 10.4819,
      "step": 790
    },
    {
      "epoch": 0.09837891350080435,
      "grad_norm": 0.39671939611434937,
      "learning_rate": 4.508724167801015e-05,
      "loss": 10.494,
      "step": 795
    },
    {
      "epoch": 0.09899764880584086,
      "grad_norm": 0.3915795385837555,
      "learning_rate": 4.505630491275832e-05,
      "loss": 10.4862,
      "step": 800
    },
    {
      "epoch": 0.09961638411087737,
      "grad_norm": 0.37651872634887695,
      "learning_rate": 4.5025368147506495e-05,
      "loss": 10.4888,
      "step": 805
    },
    {
      "epoch": 0.10023511941591387,
      "grad_norm": 0.3776291608810425,
      "learning_rate": 4.4994431382254675e-05,
      "loss": 10.4876,
      "step": 810
    },
    {
      "epoch": 0.10085385472095038,
      "grad_norm": 0.37676113843917847,
      "learning_rate": 4.496349461700285e-05,
      "loss": 10.4816,
      "step": 815
    },
    {
      "epoch": 0.10147259002598688,
      "grad_norm": 0.3871561288833618,
      "learning_rate": 4.493255785175102e-05,
      "loss": 10.4802,
      "step": 820
    },
    {
      "epoch": 0.10209132533102339,
      "grad_norm": 0.38958799839019775,
      "learning_rate": 4.4901621086499196e-05,
      "loss": 10.4758,
      "step": 825
    },
    {
      "epoch": 0.10271006063605989,
      "grad_norm": 0.3659844994544983,
      "learning_rate": 4.4870684321247376e-05,
      "loss": 10.4882,
      "step": 830
    },
    {
      "epoch": 0.1033287959410964,
      "grad_norm": 0.3962794542312622,
      "learning_rate": 4.483974755599555e-05,
      "loss": 10.479,
      "step": 835
    },
    {
      "epoch": 0.1039475312461329,
      "grad_norm": 0.3996202051639557,
      "learning_rate": 4.480881079074372e-05,
      "loss": 10.4775,
      "step": 840
    },
    {
      "epoch": 0.1045662665511694,
      "grad_norm": 0.4411140978336334,
      "learning_rate": 4.4777874025491896e-05,
      "loss": 10.4737,
      "step": 845
    },
    {
      "epoch": 0.10518500185620591,
      "grad_norm": 0.3911568820476532,
      "learning_rate": 4.4746937260240076e-05,
      "loss": 10.4763,
      "step": 850
    },
    {
      "epoch": 0.10580373716124242,
      "grad_norm": 0.3939804434776306,
      "learning_rate": 4.471600049498825e-05,
      "loss": 10.4824,
      "step": 855
    },
    {
      "epoch": 0.10642247246627892,
      "grad_norm": 0.40065327286720276,
      "learning_rate": 4.468506372973642e-05,
      "loss": 10.4704,
      "step": 860
    },
    {
      "epoch": 0.10704120777131543,
      "grad_norm": 0.3852839469909668,
      "learning_rate": 4.46541269644846e-05,
      "loss": 10.4706,
      "step": 865
    },
    {
      "epoch": 0.10765994307635193,
      "grad_norm": 0.391732394695282,
      "learning_rate": 4.462319019923277e-05,
      "loss": 10.476,
      "step": 870
    },
    {
      "epoch": 0.10827867838138844,
      "grad_norm": 0.39275768399238586,
      "learning_rate": 4.4592253433980944e-05,
      "loss": 10.485,
      "step": 875
    },
    {
      "epoch": 0.10889741368642494,
      "grad_norm": 0.40188828110694885,
      "learning_rate": 4.456131666872912e-05,
      "loss": 10.4664,
      "step": 880
    },
    {
      "epoch": 0.10951614899146145,
      "grad_norm": 0.3906530439853668,
      "learning_rate": 4.453037990347729e-05,
      "loss": 10.469,
      "step": 885
    },
    {
      "epoch": 0.11013488429649795,
      "grad_norm": 0.5073468089103699,
      "learning_rate": 4.449944313822547e-05,
      "loss": 10.4706,
      "step": 890
    },
    {
      "epoch": 0.11075361960153446,
      "grad_norm": 0.38428401947021484,
      "learning_rate": 4.4468506372973644e-05,
      "loss": 10.4643,
      "step": 895
    },
    {
      "epoch": 0.11137235490657096,
      "grad_norm": 0.4002198874950409,
      "learning_rate": 4.443756960772182e-05,
      "loss": 10.4582,
      "step": 900
    },
    {
      "epoch": 0.11199109021160747,
      "grad_norm": 0.37753650546073914,
      "learning_rate": 4.440663284246999e-05,
      "loss": 10.472,
      "step": 905
    },
    {
      "epoch": 0.11260982551664397,
      "grad_norm": 0.4722539484500885,
      "learning_rate": 4.437569607721817e-05,
      "loss": 10.4616,
      "step": 910
    },
    {
      "epoch": 0.11322856082168048,
      "grad_norm": 0.38861727714538574,
      "learning_rate": 4.4344759311966345e-05,
      "loss": 10.4694,
      "step": 915
    },
    {
      "epoch": 0.11384729612671698,
      "grad_norm": 0.4505135416984558,
      "learning_rate": 4.431382254671452e-05,
      "loss": 10.465,
      "step": 920
    },
    {
      "epoch": 0.11446603143175349,
      "grad_norm": 0.3767157793045044,
      "learning_rate": 4.428288578146269e-05,
      "loss": 10.4608,
      "step": 925
    },
    {
      "epoch": 0.11508476673679,
      "grad_norm": 0.4007627069950104,
      "learning_rate": 4.425194901621087e-05,
      "loss": 10.4586,
      "step": 930
    },
    {
      "epoch": 0.1157035020418265,
      "grad_norm": 0.410017728805542,
      "learning_rate": 4.4221012250959045e-05,
      "loss": 10.4561,
      "step": 935
    },
    {
      "epoch": 0.116322237346863,
      "grad_norm": 0.39905837178230286,
      "learning_rate": 4.419007548570722e-05,
      "loss": 10.4603,
      "step": 940
    },
    {
      "epoch": 0.11694097265189951,
      "grad_norm": 0.3920939564704895,
      "learning_rate": 4.415913872045539e-05,
      "loss": 10.4482,
      "step": 945
    },
    {
      "epoch": 0.11755970795693602,
      "grad_norm": 0.5621168613433838,
      "learning_rate": 4.4128201955203566e-05,
      "loss": 10.4593,
      "step": 950
    },
    {
      "epoch": 0.11817844326197254,
      "grad_norm": 0.3867946267127991,
      "learning_rate": 4.409726518995174e-05,
      "loss": 10.4493,
      "step": 955
    },
    {
      "epoch": 0.11879717856700904,
      "grad_norm": 0.397165447473526,
      "learning_rate": 4.406632842469991e-05,
      "loss": 10.449,
      "step": 960
    },
    {
      "epoch": 0.11941591387204555,
      "grad_norm": 0.41199249029159546,
      "learning_rate": 4.4035391659448086e-05,
      "loss": 10.4484,
      "step": 965
    },
    {
      "epoch": 0.12003464917708205,
      "grad_norm": 1.155837059020996,
      "learning_rate": 4.4004454894196266e-05,
      "loss": 10.4543,
      "step": 970
    },
    {
      "epoch": 0.12065338448211856,
      "grad_norm": 0.405983567237854,
      "learning_rate": 4.397351812894444e-05,
      "loss": 10.4527,
      "step": 975
    },
    {
      "epoch": 0.12127211978715506,
      "grad_norm": 0.3913414180278778,
      "learning_rate": 4.394258136369261e-05,
      "loss": 10.454,
      "step": 980
    },
    {
      "epoch": 0.12189085509219157,
      "grad_norm": 0.40109989047050476,
      "learning_rate": 4.391164459844079e-05,
      "loss": 10.4439,
      "step": 985
    },
    {
      "epoch": 0.12250959039722807,
      "grad_norm": 0.39342200756073,
      "learning_rate": 4.388070783318897e-05,
      "loss": 10.4476,
      "step": 990
    },
    {
      "epoch": 0.12312832570226458,
      "grad_norm": 0.6070349216461182,
      "learning_rate": 4.384977106793714e-05,
      "loss": 10.4444,
      "step": 995
    },
    {
      "epoch": 0.12374706100730108,
      "grad_norm": 0.4030889868736267,
      "learning_rate": 4.3818834302685314e-05,
      "loss": 10.4492,
      "step": 1000
    },
    {
      "epoch": 0.12436579631233759,
      "grad_norm": 0.7143388390541077,
      "learning_rate": 4.378789753743349e-05,
      "loss": 10.4451,
      "step": 1005
    },
    {
      "epoch": 0.12498453161737409,
      "grad_norm": 0.4036734700202942,
      "learning_rate": 4.375696077218167e-05,
      "loss": 10.4421,
      "step": 1010
    },
    {
      "epoch": 0.12560326692241058,
      "grad_norm": 0.4130595624446869,
      "learning_rate": 4.372602400692984e-05,
      "loss": 10.4535,
      "step": 1015
    },
    {
      "epoch": 0.1262220022274471,
      "grad_norm": 0.39537686109542847,
      "learning_rate": 4.3695087241678014e-05,
      "loss": 10.4369,
      "step": 1020
    },
    {
      "epoch": 0.1268407375324836,
      "grad_norm": 0.41328638792037964,
      "learning_rate": 4.366415047642619e-05,
      "loss": 10.4418,
      "step": 1025
    },
    {
      "epoch": 0.1274594728375201,
      "grad_norm": 0.3973396420478821,
      "learning_rate": 4.363321371117436e-05,
      "loss": 10.4385,
      "step": 1030
    },
    {
      "epoch": 0.1280782081425566,
      "grad_norm": 1.1635664701461792,
      "learning_rate": 4.3602276945922535e-05,
      "loss": 10.4384,
      "step": 1035
    },
    {
      "epoch": 0.1286969434475931,
      "grad_norm": 0.4091948866844177,
      "learning_rate": 4.357134018067071e-05,
      "loss": 10.444,
      "step": 1040
    },
    {
      "epoch": 0.12931567875262961,
      "grad_norm": 0.39811012148857117,
      "learning_rate": 4.354040341541888e-05,
      "loss": 10.4354,
      "step": 1045
    },
    {
      "epoch": 0.12993441405766612,
      "grad_norm": 0.41117289662361145,
      "learning_rate": 4.350946665016706e-05,
      "loss": 10.4252,
      "step": 1050
    },
    {
      "epoch": 0.13055314936270263,
      "grad_norm": 0.4188407063484192,
      "learning_rate": 4.3478529884915235e-05,
      "loss": 10.435,
      "step": 1055
    },
    {
      "epoch": 0.13117188466773913,
      "grad_norm": 0.4070369601249695,
      "learning_rate": 4.344759311966341e-05,
      "loss": 10.4408,
      "step": 1060
    },
    {
      "epoch": 0.13179061997277564,
      "grad_norm": 0.40299084782600403,
      "learning_rate": 4.341665635441158e-05,
      "loss": 10.438,
      "step": 1065
    },
    {
      "epoch": 0.13240935527781214,
      "grad_norm": 0.4186113476753235,
      "learning_rate": 4.3385719589159756e-05,
      "loss": 10.4343,
      "step": 1070
    },
    {
      "epoch": 0.13302809058284865,
      "grad_norm": 0.40990278124809265,
      "learning_rate": 4.3354782823907936e-05,
      "loss": 10.4298,
      "step": 1075
    },
    {
      "epoch": 0.13364682588788515,
      "grad_norm": 0.4102371335029602,
      "learning_rate": 4.332384605865611e-05,
      "loss": 10.4319,
      "step": 1080
    },
    {
      "epoch": 0.13426556119292166,
      "grad_norm": 0.40758809447288513,
      "learning_rate": 4.329290929340428e-05,
      "loss": 10.4352,
      "step": 1085
    },
    {
      "epoch": 0.13488429649795816,
      "grad_norm": 0.40716472268104553,
      "learning_rate": 4.3261972528152456e-05,
      "loss": 10.4248,
      "step": 1090
    },
    {
      "epoch": 0.13550303180299467,
      "grad_norm": 0.4061245322227478,
      "learning_rate": 4.3231035762900636e-05,
      "loss": 10.4254,
      "step": 1095
    },
    {
      "epoch": 0.13612176710803117,
      "grad_norm": 0.413320928812027,
      "learning_rate": 4.320009899764881e-05,
      "loss": 10.4267,
      "step": 1100
    },
    {
      "epoch": 0.13674050241306768,
      "grad_norm": 0.42611178755760193,
      "learning_rate": 4.316916223239698e-05,
      "loss": 10.4198,
      "step": 1105
    },
    {
      "epoch": 0.13735923771810418,
      "grad_norm": 0.41276851296424866,
      "learning_rate": 4.313822546714516e-05,
      "loss": 10.4264,
      "step": 1110
    },
    {
      "epoch": 0.1379779730231407,
      "grad_norm": 0.3995507061481476,
      "learning_rate": 4.310728870189333e-05,
      "loss": 10.4182,
      "step": 1115
    },
    {
      "epoch": 0.1385967083281772,
      "grad_norm": 0.43092700839042664,
      "learning_rate": 4.3076351936641504e-05,
      "loss": 10.4216,
      "step": 1120
    },
    {
      "epoch": 0.1392154436332137,
      "grad_norm": 0.4249956011772156,
      "learning_rate": 4.304541517138968e-05,
      "loss": 10.4287,
      "step": 1125
    },
    {
      "epoch": 0.1398341789382502,
      "grad_norm": 0.4150128662586212,
      "learning_rate": 4.301447840613785e-05,
      "loss": 10.4251,
      "step": 1130
    },
    {
      "epoch": 0.1404529142432867,
      "grad_norm": 0.40555158257484436,
      "learning_rate": 4.298354164088603e-05,
      "loss": 10.4127,
      "step": 1135
    },
    {
      "epoch": 0.1410716495483232,
      "grad_norm": 0.43821465969085693,
      "learning_rate": 4.2952604875634204e-05,
      "loss": 10.412,
      "step": 1140
    },
    {
      "epoch": 0.14169038485335975,
      "grad_norm": 0.4263504445552826,
      "learning_rate": 4.292166811038238e-05,
      "loss": 10.416,
      "step": 1145
    },
    {
      "epoch": 0.14230912015839625,
      "grad_norm": 0.42797672748565674,
      "learning_rate": 4.289073134513055e-05,
      "loss": 10.4129,
      "step": 1150
    },
    {
      "epoch": 0.14292785546343276,
      "grad_norm": 0.42493733763694763,
      "learning_rate": 4.285979457987873e-05,
      "loss": 10.4117,
      "step": 1155
    },
    {
      "epoch": 0.14354659076846926,
      "grad_norm": 0.4396042227745056,
      "learning_rate": 4.2828857814626905e-05,
      "loss": 10.4077,
      "step": 1160
    },
    {
      "epoch": 0.14416532607350577,
      "grad_norm": 0.4309457838535309,
      "learning_rate": 4.279792104937508e-05,
      "loss": 10.4054,
      "step": 1165
    },
    {
      "epoch": 0.14478406137854227,
      "grad_norm": 0.4112999737262726,
      "learning_rate": 4.276698428412325e-05,
      "loss": 10.4051,
      "step": 1170
    },
    {
      "epoch": 0.14540279668357878,
      "grad_norm": 0.4362340569496155,
      "learning_rate": 4.273604751887143e-05,
      "loss": 10.4157,
      "step": 1175
    },
    {
      "epoch": 0.14602153198861528,
      "grad_norm": 0.4243593215942383,
      "learning_rate": 4.2705110753619605e-05,
      "loss": 10.4135,
      "step": 1180
    },
    {
      "epoch": 0.1466402672936518,
      "grad_norm": 0.40205317735671997,
      "learning_rate": 4.267417398836778e-05,
      "loss": 10.4094,
      "step": 1185
    },
    {
      "epoch": 0.1472590025986883,
      "grad_norm": 0.41891878843307495,
      "learning_rate": 4.264323722311595e-05,
      "loss": 10.4042,
      "step": 1190
    },
    {
      "epoch": 0.1478777379037248,
      "grad_norm": 0.4392828345298767,
      "learning_rate": 4.261230045786413e-05,
      "loss": 10.41,
      "step": 1195
    },
    {
      "epoch": 0.1484964732087613,
      "grad_norm": 0.44222185015678406,
      "learning_rate": 4.2581363692612306e-05,
      "loss": 10.4066,
      "step": 1200
    },
    {
      "epoch": 0.1491152085137978,
      "grad_norm": 0.40562066435813904,
      "learning_rate": 4.255042692736048e-05,
      "loss": 10.4056,
      "step": 1205
    },
    {
      "epoch": 0.14973394381883431,
      "grad_norm": 0.43553948402404785,
      "learning_rate": 4.2519490162108646e-05,
      "loss": 10.402,
      "step": 1210
    },
    {
      "epoch": 0.15035267912387082,
      "grad_norm": 0.4143899381160736,
      "learning_rate": 4.2488553396856826e-05,
      "loss": 10.4107,
      "step": 1215
    },
    {
      "epoch": 0.15097141442890732,
      "grad_norm": 0.4109480381011963,
      "learning_rate": 4.2457616631605e-05,
      "loss": 10.4106,
      "step": 1220
    },
    {
      "epoch": 0.15159014973394383,
      "grad_norm": 0.41043442487716675,
      "learning_rate": 4.242667986635317e-05,
      "loss": 10.4051,
      "step": 1225
    },
    {
      "epoch": 0.15220888503898033,
      "grad_norm": 0.4207841455936432,
      "learning_rate": 4.239574310110135e-05,
      "loss": 10.3964,
      "step": 1230
    },
    {
      "epoch": 0.15282762034401684,
      "grad_norm": 0.4341220557689667,
      "learning_rate": 4.236480633584953e-05,
      "loss": 10.3955,
      "step": 1235
    },
    {
      "epoch": 0.15344635564905335,
      "grad_norm": 0.4279838800430298,
      "learning_rate": 4.23338695705977e-05,
      "loss": 10.3954,
      "step": 1240
    },
    {
      "epoch": 0.15406509095408985,
      "grad_norm": 0.4365667402744293,
      "learning_rate": 4.2302932805345874e-05,
      "loss": 10.3909,
      "step": 1245
    },
    {
      "epoch": 0.15468382625912636,
      "grad_norm": 0.4417487382888794,
      "learning_rate": 4.227199604009405e-05,
      "loss": 10.3795,
      "step": 1250
    },
    {
      "epoch": 0.15530256156416286,
      "grad_norm": 0.48295149207115173,
      "learning_rate": 4.224105927484223e-05,
      "loss": 10.3945,
      "step": 1255
    },
    {
      "epoch": 0.15592129686919937,
      "grad_norm": 0.4150393009185791,
      "learning_rate": 4.22101225095904e-05,
      "loss": 10.397,
      "step": 1260
    },
    {
      "epoch": 0.15654003217423587,
      "grad_norm": 0.4175652265548706,
      "learning_rate": 4.2179185744338574e-05,
      "loss": 10.3895,
      "step": 1265
    },
    {
      "epoch": 0.15715876747927238,
      "grad_norm": 0.4392806589603424,
      "learning_rate": 4.214824897908675e-05,
      "loss": 10.3954,
      "step": 1270
    },
    {
      "epoch": 0.15777750278430888,
      "grad_norm": 0.41934365034103394,
      "learning_rate": 4.211731221383493e-05,
      "loss": 10.3949,
      "step": 1275
    },
    {
      "epoch": 0.1583962380893454,
      "grad_norm": 0.43274468183517456,
      "learning_rate": 4.20863754485831e-05,
      "loss": 10.3857,
      "step": 1280
    },
    {
      "epoch": 0.1590149733943819,
      "grad_norm": 0.44880422949790955,
      "learning_rate": 4.2055438683331275e-05,
      "loss": 10.385,
      "step": 1285
    },
    {
      "epoch": 0.1596337086994184,
      "grad_norm": 0.42751866579055786,
      "learning_rate": 4.202450191807945e-05,
      "loss": 10.376,
      "step": 1290
    },
    {
      "epoch": 0.1602524440044549,
      "grad_norm": 0.42914333939552307,
      "learning_rate": 4.199356515282762e-05,
      "loss": 10.385,
      "step": 1295
    },
    {
      "epoch": 0.1608711793094914,
      "grad_norm": 0.42645442485809326,
      "learning_rate": 4.1962628387575795e-05,
      "loss": 10.3802,
      "step": 1300
    },
    {
      "epoch": 0.1614899146145279,
      "grad_norm": 0.4227728247642517,
      "learning_rate": 4.193169162232397e-05,
      "loss": 10.3813,
      "step": 1305
    },
    {
      "epoch": 0.16210864991956442,
      "grad_norm": 0.45802178978919983,
      "learning_rate": 4.190075485707214e-05,
      "loss": 10.3836,
      "step": 1310
    },
    {
      "epoch": 0.16272738522460092,
      "grad_norm": 0.4400152862071991,
      "learning_rate": 4.186981809182032e-05,
      "loss": 10.387,
      "step": 1315
    },
    {
      "epoch": 0.16334612052963743,
      "grad_norm": 0.4483804702758789,
      "learning_rate": 4.1838881326568496e-05,
      "loss": 10.3734,
      "step": 1320
    },
    {
      "epoch": 0.16396485583467393,
      "grad_norm": 0.4487312436103821,
      "learning_rate": 4.180794456131667e-05,
      "loss": 10.3821,
      "step": 1325
    },
    {
      "epoch": 0.16458359113971044,
      "grad_norm": 0.44959720969200134,
      "learning_rate": 4.177700779606484e-05,
      "loss": 10.3788,
      "step": 1330
    },
    {
      "epoch": 0.16520232644474694,
      "grad_norm": 0.4465975761413574,
      "learning_rate": 4.174607103081302e-05,
      "loss": 10.3728,
      "step": 1335
    },
    {
      "epoch": 0.16582106174978345,
      "grad_norm": 0.43635302782058716,
      "learning_rate": 4.1715134265561196e-05,
      "loss": 10.3764,
      "step": 1340
    },
    {
      "epoch": 0.16643979705481995,
      "grad_norm": 0.44026580452919006,
      "learning_rate": 4.168419750030937e-05,
      "loss": 10.371,
      "step": 1345
    },
    {
      "epoch": 0.16705853235985646,
      "grad_norm": 0.4467495083808899,
      "learning_rate": 4.165326073505754e-05,
      "loss": 10.3683,
      "step": 1350
    },
    {
      "epoch": 0.16767726766489296,
      "grad_norm": 0.4482422173023224,
      "learning_rate": 4.1622323969805724e-05,
      "loss": 10.3713,
      "step": 1355
    },
    {
      "epoch": 0.16829600296992947,
      "grad_norm": 0.4333834648132324,
      "learning_rate": 4.15913872045539e-05,
      "loss": 10.3673,
      "step": 1360
    },
    {
      "epoch": 0.16891473827496598,
      "grad_norm": 0.5112784504890442,
      "learning_rate": 4.156045043930207e-05,
      "loss": 10.3796,
      "step": 1365
    },
    {
      "epoch": 0.16953347358000248,
      "grad_norm": 0.42555877566337585,
      "learning_rate": 4.1529513674050244e-05,
      "loss": 10.3731,
      "step": 1370
    },
    {
      "epoch": 0.17015220888503899,
      "grad_norm": 0.4403182864189148,
      "learning_rate": 4.149857690879842e-05,
      "loss": 10.3734,
      "step": 1375
    },
    {
      "epoch": 0.1707709441900755,
      "grad_norm": 0.4352293312549591,
      "learning_rate": 4.146764014354659e-05,
      "loss": 10.3652,
      "step": 1380
    },
    {
      "epoch": 0.171389679495112,
      "grad_norm": 0.4409061670303345,
      "learning_rate": 4.1436703378294764e-05,
      "loss": 10.3622,
      "step": 1385
    },
    {
      "epoch": 0.1720084148001485,
      "grad_norm": 0.4499701261520386,
      "learning_rate": 4.140576661304294e-05,
      "loss": 10.3676,
      "step": 1390
    },
    {
      "epoch": 0.172627150105185,
      "grad_norm": 0.9979570508003235,
      "learning_rate": 4.137482984779112e-05,
      "loss": 10.3615,
      "step": 1395
    },
    {
      "epoch": 0.1732458854102215,
      "grad_norm": 0.451318621635437,
      "learning_rate": 4.134389308253929e-05,
      "loss": 10.3628,
      "step": 1400
    },
    {
      "epoch": 0.17386462071525802,
      "grad_norm": 0.4659368395805359,
      "learning_rate": 4.1312956317287465e-05,
      "loss": 10.355,
      "step": 1405
    },
    {
      "epoch": 0.17448335602029452,
      "grad_norm": 0.4547531008720398,
      "learning_rate": 4.128201955203564e-05,
      "loss": 10.3613,
      "step": 1410
    },
    {
      "epoch": 0.17510209132533103,
      "grad_norm": 0.43733713030815125,
      "learning_rate": 4.125108278678382e-05,
      "loss": 10.3568,
      "step": 1415
    },
    {
      "epoch": 0.17572082663036753,
      "grad_norm": 0.44336584210395813,
      "learning_rate": 4.122014602153199e-05,
      "loss": 10.3632,
      "step": 1420
    },
    {
      "epoch": 0.17633956193540404,
      "grad_norm": 0.461894154548645,
      "learning_rate": 4.1189209256280165e-05,
      "loss": 10.3664,
      "step": 1425
    },
    {
      "epoch": 0.17695829724044054,
      "grad_norm": 0.4490235149860382,
      "learning_rate": 4.115827249102834e-05,
      "loss": 10.3673,
      "step": 1430
    },
    {
      "epoch": 0.17757703254547705,
      "grad_norm": 0.4466021955013275,
      "learning_rate": 4.112733572577652e-05,
      "loss": 10.3572,
      "step": 1435
    },
    {
      "epoch": 0.17819576785051355,
      "grad_norm": 0.44760584831237793,
      "learning_rate": 4.109639896052469e-05,
      "loss": 10.3518,
      "step": 1440
    },
    {
      "epoch": 0.17881450315555006,
      "grad_norm": 0.44489938020706177,
      "learning_rate": 4.1065462195272866e-05,
      "loss": 10.3596,
      "step": 1445
    },
    {
      "epoch": 0.17943323846058656,
      "grad_norm": 0.44557708501815796,
      "learning_rate": 4.103452543002104e-05,
      "loss": 10.3608,
      "step": 1450
    },
    {
      "epoch": 0.18005197376562307,
      "grad_norm": 0.43989691138267517,
      "learning_rate": 4.100358866476921e-05,
      "loss": 10.3565,
      "step": 1455
    },
    {
      "epoch": 0.18067070907065957,
      "grad_norm": 0.4579300880432129,
      "learning_rate": 4.0972651899517386e-05,
      "loss": 10.3533,
      "step": 1460
    },
    {
      "epoch": 0.18128944437569608,
      "grad_norm": 0.459147185087204,
      "learning_rate": 4.094171513426556e-05,
      "loss": 10.3481,
      "step": 1465
    },
    {
      "epoch": 0.18190817968073258,
      "grad_norm": 0.45657503604888916,
      "learning_rate": 4.091077836901373e-05,
      "loss": 10.3474,
      "step": 1470
    },
    {
      "epoch": 0.1825269149857691,
      "grad_norm": 0.4473494589328766,
      "learning_rate": 4.0879841603761913e-05,
      "loss": 10.3444,
      "step": 1475
    },
    {
      "epoch": 0.1831456502908056,
      "grad_norm": 0.4559958279132843,
      "learning_rate": 4.084890483851009e-05,
      "loss": 10.3423,
      "step": 1480
    },
    {
      "epoch": 0.1837643855958421,
      "grad_norm": 0.4749903380870819,
      "learning_rate": 4.081796807325826e-05,
      "loss": 10.3396,
      "step": 1485
    },
    {
      "epoch": 0.1843831209008786,
      "grad_norm": 0.4649116098880768,
      "learning_rate": 4.0787031308006434e-05,
      "loss": 10.3396,
      "step": 1490
    },
    {
      "epoch": 0.1850018562059151,
      "grad_norm": 0.46439120173454285,
      "learning_rate": 4.0756094542754614e-05,
      "loss": 10.3431,
      "step": 1495
    },
    {
      "epoch": 0.18562059151095162,
      "grad_norm": 0.48320087790489197,
      "learning_rate": 4.072515777750279e-05,
      "loss": 10.3499,
      "step": 1500
    },
    {
      "epoch": 0.18623932681598812,
      "grad_norm": 0.451562762260437,
      "learning_rate": 4.069422101225096e-05,
      "loss": 10.3397,
      "step": 1505
    },
    {
      "epoch": 0.18685806212102463,
      "grad_norm": 0.4489356577396393,
      "learning_rate": 4.0663284246999134e-05,
      "loss": 10.3354,
      "step": 1510
    },
    {
      "epoch": 0.18747679742606113,
      "grad_norm": 0.4679035544395447,
      "learning_rate": 4.0632347481747315e-05,
      "loss": 10.3363,
      "step": 1515
    },
    {
      "epoch": 0.18809553273109764,
      "grad_norm": 0.4600922465324402,
      "learning_rate": 4.060141071649549e-05,
      "loss": 10.3368,
      "step": 1520
    },
    {
      "epoch": 0.18871426803613414,
      "grad_norm": 0.4617966413497925,
      "learning_rate": 4.057047395124366e-05,
      "loss": 10.3374,
      "step": 1525
    },
    {
      "epoch": 0.18933300334117065,
      "grad_norm": 0.47671785950660706,
      "learning_rate": 4.0539537185991835e-05,
      "loss": 10.3415,
      "step": 1530
    },
    {
      "epoch": 0.18995173864620715,
      "grad_norm": 0.47359928488731384,
      "learning_rate": 4.050860042074001e-05,
      "loss": 10.3328,
      "step": 1535
    },
    {
      "epoch": 0.19057047395124366,
      "grad_norm": 0.47022193670272827,
      "learning_rate": 4.047766365548818e-05,
      "loss": 10.3418,
      "step": 1540
    },
    {
      "epoch": 0.19118920925628016,
      "grad_norm": 0.4641624391078949,
      "learning_rate": 4.0446726890236355e-05,
      "loss": 10.3227,
      "step": 1545
    },
    {
      "epoch": 0.19180794456131667,
      "grad_norm": 0.46547624468803406,
      "learning_rate": 4.041579012498453e-05,
      "loss": 10.3365,
      "step": 1550
    },
    {
      "epoch": 0.19242667986635317,
      "grad_norm": 0.4777984619140625,
      "learning_rate": 4.038485335973271e-05,
      "loss": 10.3314,
      "step": 1555
    },
    {
      "epoch": 0.19304541517138968,
      "grad_norm": 0.46445685625076294,
      "learning_rate": 4.035391659448088e-05,
      "loss": 10.326,
      "step": 1560
    },
    {
      "epoch": 0.19366415047642618,
      "grad_norm": 0.46615761518478394,
      "learning_rate": 4.0322979829229056e-05,
      "loss": 10.3321,
      "step": 1565
    },
    {
      "epoch": 0.1942828857814627,
      "grad_norm": 0.46839484572410583,
      "learning_rate": 4.029204306397723e-05,
      "loss": 10.3312,
      "step": 1570
    },
    {
      "epoch": 0.1949016210864992,
      "grad_norm": 0.4572003185749054,
      "learning_rate": 4.026110629872541e-05,
      "loss": 10.3286,
      "step": 1575
    },
    {
      "epoch": 0.1955203563915357,
      "grad_norm": 0.4510992765426636,
      "learning_rate": 4.023016953347358e-05,
      "loss": 10.3249,
      "step": 1580
    },
    {
      "epoch": 0.1961390916965722,
      "grad_norm": 0.45604443550109863,
      "learning_rate": 4.0199232768221756e-05,
      "loss": 10.3257,
      "step": 1585
    },
    {
      "epoch": 0.1967578270016087,
      "grad_norm": 0.46505218744277954,
      "learning_rate": 4.016829600296993e-05,
      "loss": 10.3319,
      "step": 1590
    },
    {
      "epoch": 0.19737656230664521,
      "grad_norm": 0.847195029258728,
      "learning_rate": 4.013735923771811e-05,
      "loss": 10.319,
      "step": 1595
    },
    {
      "epoch": 0.19799529761168172,
      "grad_norm": 0.48296061158180237,
      "learning_rate": 4.0106422472466284e-05,
      "loss": 10.324,
      "step": 1600
    },
    {
      "epoch": 0.19861403291671823,
      "grad_norm": 0.46898847818374634,
      "learning_rate": 4.007548570721446e-05,
      "loss": 10.322,
      "step": 1605
    },
    {
      "epoch": 0.19923276822175473,
      "grad_norm": 0.46948131918907166,
      "learning_rate": 4.004454894196263e-05,
      "loss": 10.313,
      "step": 1610
    },
    {
      "epoch": 0.19985150352679124,
      "grad_norm": 0.49787166714668274,
      "learning_rate": 4.001361217671081e-05,
      "loss": 10.3127,
      "step": 1615
    },
    {
      "epoch": 0.20047023883182774,
      "grad_norm": 0.46744275093078613,
      "learning_rate": 3.998267541145898e-05,
      "loss": 10.3084,
      "step": 1620
    },
    {
      "epoch": 0.20108897413686425,
      "grad_norm": 0.48800650238990784,
      "learning_rate": 3.995173864620715e-05,
      "loss": 10.3132,
      "step": 1625
    },
    {
      "epoch": 0.20170770944190075,
      "grad_norm": 0.4887082576751709,
      "learning_rate": 3.9920801880955324e-05,
      "loss": 10.3128,
      "step": 1630
    },
    {
      "epoch": 0.20232644474693726,
      "grad_norm": 0.4705052077770233,
      "learning_rate": 3.9889865115703505e-05,
      "loss": 10.3047,
      "step": 1635
    },
    {
      "epoch": 0.20294518005197376,
      "grad_norm": 0.48447421193122864,
      "learning_rate": 3.985892835045168e-05,
      "loss": 10.3201,
      "step": 1640
    },
    {
      "epoch": 0.20356391535701027,
      "grad_norm": 0.49856576323509216,
      "learning_rate": 3.982799158519985e-05,
      "loss": 10.318,
      "step": 1645
    },
    {
      "epoch": 0.20418265066204677,
      "grad_norm": 0.4737730026245117,
      "learning_rate": 3.9797054819948025e-05,
      "loss": 10.3162,
      "step": 1650
    },
    {
      "epoch": 0.20480138596708328,
      "grad_norm": 0.4578478932380676,
      "learning_rate": 3.9766118054696205e-05,
      "loss": 10.3094,
      "step": 1655
    },
    {
      "epoch": 0.20542012127211978,
      "grad_norm": 0.4845803380012512,
      "learning_rate": 3.973518128944438e-05,
      "loss": 10.3072,
      "step": 1660
    },
    {
      "epoch": 0.2060388565771563,
      "grad_norm": 0.4712551534175873,
      "learning_rate": 3.970424452419255e-05,
      "loss": 10.3145,
      "step": 1665
    },
    {
      "epoch": 0.2066575918821928,
      "grad_norm": 0.45923087000846863,
      "learning_rate": 3.9673307758940725e-05,
      "loss": 10.3052,
      "step": 1670
    },
    {
      "epoch": 0.2072763271872293,
      "grad_norm": 0.48065024614334106,
      "learning_rate": 3.9642370993688906e-05,
      "loss": 10.3114,
      "step": 1675
    },
    {
      "epoch": 0.2078950624922658,
      "grad_norm": 0.4857586622238159,
      "learning_rate": 3.961143422843708e-05,
      "loss": 10.306,
      "step": 1680
    },
    {
      "epoch": 0.2085137977973023,
      "grad_norm": 0.4712294340133667,
      "learning_rate": 3.958049746318525e-05,
      "loss": 10.3119,
      "step": 1685
    },
    {
      "epoch": 0.2091325331023388,
      "grad_norm": 0.498907208442688,
      "learning_rate": 3.9549560697933426e-05,
      "loss": 10.3028,
      "step": 1690
    },
    {
      "epoch": 0.20975126840737532,
      "grad_norm": 0.44653400778770447,
      "learning_rate": 3.9518623932681606e-05,
      "loss": 10.3119,
      "step": 1695
    },
    {
      "epoch": 0.21037000371241182,
      "grad_norm": 0.48437121510505676,
      "learning_rate": 3.948768716742978e-05,
      "loss": 10.2956,
      "step": 1700
    },
    {
      "epoch": 0.21098873901744833,
      "grad_norm": 0.5004026889801025,
      "learning_rate": 3.945675040217795e-05,
      "loss": 10.3001,
      "step": 1705
    },
    {
      "epoch": 0.21160747432248483,
      "grad_norm": 0.47291213274002075,
      "learning_rate": 3.942581363692613e-05,
      "loss": 10.3059,
      "step": 1710
    },
    {
      "epoch": 0.21222620962752134,
      "grad_norm": 0.482158362865448,
      "learning_rate": 3.93948768716743e-05,
      "loss": 10.2967,
      "step": 1715
    },
    {
      "epoch": 0.21284494493255784,
      "grad_norm": 0.4948899745941162,
      "learning_rate": 3.9363940106422474e-05,
      "loss": 10.2953,
      "step": 1720
    },
    {
      "epoch": 0.21346368023759435,
      "grad_norm": 0.47970619797706604,
      "learning_rate": 3.933300334117065e-05,
      "loss": 10.297,
      "step": 1725
    },
    {
      "epoch": 0.21408241554263086,
      "grad_norm": 0.4907449781894684,
      "learning_rate": 3.930206657591882e-05,
      "loss": 10.3037,
      "step": 1730
    },
    {
      "epoch": 0.21470115084766736,
      "grad_norm": 0.48826485872268677,
      "learning_rate": 3.9271129810667e-05,
      "loss": 10.2888,
      "step": 1735
    },
    {
      "epoch": 0.21531988615270387,
      "grad_norm": 1.0009706020355225,
      "learning_rate": 3.9240193045415174e-05,
      "loss": 10.2945,
      "step": 1740
    },
    {
      "epoch": 0.21593862145774037,
      "grad_norm": 0.4882747530937195,
      "learning_rate": 3.920925628016335e-05,
      "loss": 10.2915,
      "step": 1745
    },
    {
      "epoch": 0.21655735676277688,
      "grad_norm": 0.4783584773540497,
      "learning_rate": 3.917831951491152e-05,
      "loss": 10.2973,
      "step": 1750
    },
    {
      "epoch": 0.21717609206781338,
      "grad_norm": 0.49595844745635986,
      "learning_rate": 3.91473827496597e-05,
      "loss": 10.2779,
      "step": 1755
    },
    {
      "epoch": 0.2177948273728499,
      "grad_norm": 0.4924027621746063,
      "learning_rate": 3.9116445984407875e-05,
      "loss": 10.2825,
      "step": 1760
    },
    {
      "epoch": 0.2184135626778864,
      "grad_norm": 0.48073527216911316,
      "learning_rate": 3.908550921915605e-05,
      "loss": 10.2819,
      "step": 1765
    },
    {
      "epoch": 0.2190322979829229,
      "grad_norm": 0.4973764717578888,
      "learning_rate": 3.905457245390422e-05,
      "loss": 10.283,
      "step": 1770
    },
    {
      "epoch": 0.2196510332879594,
      "grad_norm": 0.48824775218963623,
      "learning_rate": 3.90236356886524e-05,
      "loss": 10.2906,
      "step": 1775
    },
    {
      "epoch": 0.2202697685929959,
      "grad_norm": 0.4936484396457672,
      "learning_rate": 3.8992698923400575e-05,
      "loss": 10.2851,
      "step": 1780
    },
    {
      "epoch": 0.2208885038980324,
      "grad_norm": 0.48528558015823364,
      "learning_rate": 3.896176215814875e-05,
      "loss": 10.2945,
      "step": 1785
    },
    {
      "epoch": 0.22150723920306892,
      "grad_norm": 0.50543212890625,
      "learning_rate": 3.893082539289692e-05,
      "loss": 10.2772,
      "step": 1790
    },
    {
      "epoch": 0.22212597450810542,
      "grad_norm": 0.46845537424087524,
      "learning_rate": 3.8899888627645096e-05,
      "loss": 10.277,
      "step": 1795
    },
    {
      "epoch": 0.22274470981314193,
      "grad_norm": 0.47914227843284607,
      "learning_rate": 3.886895186239327e-05,
      "loss": 10.2793,
      "step": 1800
    },
    {
      "epoch": 0.22336344511817843,
      "grad_norm": 0.4779416620731354,
      "learning_rate": 3.883801509714144e-05,
      "loss": 10.2805,
      "step": 1805
    },
    {
      "epoch": 0.22398218042321494,
      "grad_norm": 0.4876461923122406,
      "learning_rate": 3.8807078331889616e-05,
      "loss": 10.2712,
      "step": 1810
    },
    {
      "epoch": 0.22460091572825144,
      "grad_norm": 0.4817785918712616,
      "learning_rate": 3.8776141566637796e-05,
      "loss": 10.2774,
      "step": 1815
    },
    {
      "epoch": 0.22521965103328795,
      "grad_norm": 0.5067082047462463,
      "learning_rate": 3.874520480138597e-05,
      "loss": 10.2705,
      "step": 1820
    },
    {
      "epoch": 0.22583838633832445,
      "grad_norm": 0.49893808364868164,
      "learning_rate": 3.871426803613414e-05,
      "loss": 10.2794,
      "step": 1825
    },
    {
      "epoch": 0.22645712164336096,
      "grad_norm": 0.5098926424980164,
      "learning_rate": 3.8683331270882317e-05,
      "loss": 10.275,
      "step": 1830
    },
    {
      "epoch": 0.22707585694839746,
      "grad_norm": 0.47792842984199524,
      "learning_rate": 3.865239450563049e-05,
      "loss": 10.2707,
      "step": 1835
    },
    {
      "epoch": 0.22769459225343397,
      "grad_norm": 0.4892188310623169,
      "learning_rate": 3.862145774037867e-05,
      "loss": 10.2797,
      "step": 1840
    },
    {
      "epoch": 0.22831332755847047,
      "grad_norm": 0.5357151627540588,
      "learning_rate": 3.8590520975126844e-05,
      "loss": 10.2682,
      "step": 1845
    },
    {
      "epoch": 0.22893206286350698,
      "grad_norm": 0.4970267713069916,
      "learning_rate": 3.855958420987502e-05,
      "loss": 10.2745,
      "step": 1850
    },
    {
      "epoch": 0.22955079816854349,
      "grad_norm": 0.5115457773208618,
      "learning_rate": 3.852864744462319e-05,
      "loss": 10.2632,
      "step": 1855
    },
    {
      "epoch": 0.23016953347358,
      "grad_norm": 1.8234752416610718,
      "learning_rate": 3.849771067937137e-05,
      "loss": 10.269,
      "step": 1860
    },
    {
      "epoch": 0.2307882687786165,
      "grad_norm": 0.49220311641693115,
      "learning_rate": 3.8466773914119544e-05,
      "loss": 10.2598,
      "step": 1865
    },
    {
      "epoch": 0.231407004083653,
      "grad_norm": 0.4932958483695984,
      "learning_rate": 3.843583714886772e-05,
      "loss": 10.2629,
      "step": 1870
    },
    {
      "epoch": 0.2320257393886895,
      "grad_norm": 0.49426212906837463,
      "learning_rate": 3.840490038361589e-05,
      "loss": 10.2632,
      "step": 1875
    },
    {
      "epoch": 0.232644474693726,
      "grad_norm": 0.495601624250412,
      "learning_rate": 3.8373963618364065e-05,
      "loss": 10.2657,
      "step": 1880
    },
    {
      "epoch": 0.23326320999876252,
      "grad_norm": 0.5048266053199768,
      "learning_rate": 3.834302685311224e-05,
      "loss": 10.2679,
      "step": 1885
    },
    {
      "epoch": 0.23388194530379902,
      "grad_norm": 0.5080966353416443,
      "learning_rate": 3.831209008786041e-05,
      "loss": 10.2603,
      "step": 1890
    },
    {
      "epoch": 0.23450068060883553,
      "grad_norm": 0.5241221189498901,
      "learning_rate": 3.8281153322608585e-05,
      "loss": 10.2495,
      "step": 1895
    },
    {
      "epoch": 0.23511941591387203,
      "grad_norm": 0.5271170735359192,
      "learning_rate": 3.8250216557356765e-05,
      "loss": 10.2502,
      "step": 1900
    },
    {
      "epoch": 0.23573815121890854,
      "grad_norm": 0.5193828344345093,
      "learning_rate": 3.821927979210494e-05,
      "loss": 10.2565,
      "step": 1905
    },
    {
      "epoch": 0.23635688652394507,
      "grad_norm": 0.48363545536994934,
      "learning_rate": 3.818834302685311e-05,
      "loss": 10.2549,
      "step": 1910
    },
    {
      "epoch": 0.23697562182898158,
      "grad_norm": 0.49280524253845215,
      "learning_rate": 3.8157406261601285e-05,
      "loss": 10.2592,
      "step": 1915
    },
    {
      "epoch": 0.23759435713401808,
      "grad_norm": 0.49607929587364197,
      "learning_rate": 3.8126469496349466e-05,
      "loss": 10.2519,
      "step": 1920
    },
    {
      "epoch": 0.23821309243905459,
      "grad_norm": 0.49610793590545654,
      "learning_rate": 3.809553273109764e-05,
      "loss": 10.2546,
      "step": 1925
    },
    {
      "epoch": 0.2388318277440911,
      "grad_norm": 0.5111364126205444,
      "learning_rate": 3.806459596584581e-05,
      "loss": 10.2563,
      "step": 1930
    },
    {
      "epoch": 0.2394505630491276,
      "grad_norm": 0.5111131072044373,
      "learning_rate": 3.8033659200593986e-05,
      "loss": 10.2573,
      "step": 1935
    },
    {
      "epoch": 0.2400692983541641,
      "grad_norm": 0.4994654059410095,
      "learning_rate": 3.8002722435342166e-05,
      "loss": 10.253,
      "step": 1940
    },
    {
      "epoch": 0.2406880336592006,
      "grad_norm": 0.5064947009086609,
      "learning_rate": 3.797178567009034e-05,
      "loss": 10.2586,
      "step": 1945
    },
    {
      "epoch": 0.2413067689642371,
      "grad_norm": 0.5052550435066223,
      "learning_rate": 3.794084890483851e-05,
      "loss": 10.2502,
      "step": 1950
    },
    {
      "epoch": 0.24192550426927362,
      "grad_norm": 0.7727169394493103,
      "learning_rate": 3.790991213958669e-05,
      "loss": 10.2415,
      "step": 1955
    },
    {
      "epoch": 0.24254423957431012,
      "grad_norm": 0.5281347036361694,
      "learning_rate": 3.787897537433486e-05,
      "loss": 10.2423,
      "step": 1960
    },
    {
      "epoch": 0.24316297487934663,
      "grad_norm": 0.5102444291114807,
      "learning_rate": 3.7848038609083034e-05,
      "loss": 10.2441,
      "step": 1965
    },
    {
      "epoch": 0.24378171018438313,
      "grad_norm": 0.516761064529419,
      "learning_rate": 3.781710184383121e-05,
      "loss": 10.2438,
      "step": 1970
    },
    {
      "epoch": 0.24440044548941964,
      "grad_norm": 0.4941084384918213,
      "learning_rate": 3.778616507857938e-05,
      "loss": 10.2565,
      "step": 1975
    },
    {
      "epoch": 0.24501918079445614,
      "grad_norm": 0.5216721892356873,
      "learning_rate": 3.775522831332756e-05,
      "loss": 10.2477,
      "step": 1980
    },
    {
      "epoch": 0.24563791609949265,
      "grad_norm": 0.5009232759475708,
      "learning_rate": 3.7724291548075734e-05,
      "loss": 10.2475,
      "step": 1985
    },
    {
      "epoch": 0.24625665140452915,
      "grad_norm": 0.5303865075111389,
      "learning_rate": 3.769335478282391e-05,
      "loss": 10.2356,
      "step": 1990
    },
    {
      "epoch": 0.24687538670956566,
      "grad_norm": 0.5029048323631287,
      "learning_rate": 3.766241801757208e-05,
      "loss": 10.2481,
      "step": 1995
    },
    {
      "epoch": 0.24749412201460216,
      "grad_norm": 0.5273754000663757,
      "learning_rate": 3.763148125232026e-05,
      "loss": 10.233,
      "step": 2000
    },
    {
      "epoch": 0.24811285731963867,
      "grad_norm": 0.5061234831809998,
      "learning_rate": 3.7600544487068435e-05,
      "loss": 10.2354,
      "step": 2005
    },
    {
      "epoch": 0.24873159262467517,
      "grad_norm": 0.5243982672691345,
      "learning_rate": 3.756960772181661e-05,
      "loss": 10.2319,
      "step": 2010
    },
    {
      "epoch": 0.24935032792971168,
      "grad_norm": 0.5225428342819214,
      "learning_rate": 3.753867095656478e-05,
      "loss": 10.2448,
      "step": 2015
    },
    {
      "epoch": 0.24996906323474818,
      "grad_norm": 0.5212192535400391,
      "learning_rate": 3.750773419131296e-05,
      "loss": 10.2271,
      "step": 2020
    },
    {
      "epoch": 0.2505877985397847,
      "grad_norm": 0.5261498093605042,
      "learning_rate": 3.7476797426061135e-05,
      "loss": 10.2391,
      "step": 2025
    },
    {
      "epoch": 0.25120653384482117,
      "grad_norm": 0.5136510729789734,
      "learning_rate": 3.744586066080931e-05,
      "loss": 10.2263,
      "step": 2030
    },
    {
      "epoch": 0.2518252691498577,
      "grad_norm": 0.5172993540763855,
      "learning_rate": 3.741492389555748e-05,
      "loss": 10.2477,
      "step": 2035
    },
    {
      "epoch": 0.2524440044548942,
      "grad_norm": 0.49168962240219116,
      "learning_rate": 3.7383987130305656e-05,
      "loss": 10.2329,
      "step": 2040
    },
    {
      "epoch": 0.2530627397599307,
      "grad_norm": 0.5215770602226257,
      "learning_rate": 3.735305036505383e-05,
      "loss": 10.2406,
      "step": 2045
    },
    {
      "epoch": 0.2536814750649672,
      "grad_norm": 0.5041229128837585,
      "learning_rate": 3.7322113599802e-05,
      "loss": 10.2339,
      "step": 2050
    },
    {
      "epoch": 0.2543002103700037,
      "grad_norm": 0.5140116214752197,
      "learning_rate": 3.7291176834550176e-05,
      "loss": 10.2358,
      "step": 2055
    },
    {
      "epoch": 0.2549189456750402,
      "grad_norm": 0.631797194480896,
      "learning_rate": 3.7260240069298356e-05,
      "loss": 10.2293,
      "step": 2060
    },
    {
      "epoch": 0.25553768098007673,
      "grad_norm": 0.5149805545806885,
      "learning_rate": 3.722930330404653e-05,
      "loss": 10.2256,
      "step": 2065
    },
    {
      "epoch": 0.2561564162851132,
      "grad_norm": 0.5248982906341553,
      "learning_rate": 3.71983665387947e-05,
      "loss": 10.2201,
      "step": 2070
    },
    {
      "epoch": 0.25677515159014974,
      "grad_norm": 0.5114428400993347,
      "learning_rate": 3.7167429773542877e-05,
      "loss": 10.2296,
      "step": 2075
    },
    {
      "epoch": 0.2573938868951862,
      "grad_norm": 0.5204735398292542,
      "learning_rate": 3.713649300829106e-05,
      "loss": 10.2268,
      "step": 2080
    },
    {
      "epoch": 0.25801262220022275,
      "grad_norm": 0.516322672367096,
      "learning_rate": 3.710555624303923e-05,
      "loss": 10.2213,
      "step": 2085
    },
    {
      "epoch": 0.25863135750525923,
      "grad_norm": 0.4949057996273041,
      "learning_rate": 3.7074619477787404e-05,
      "loss": 10.2286,
      "step": 2090
    },
    {
      "epoch": 0.25925009281029576,
      "grad_norm": 0.5238949656486511,
      "learning_rate": 3.704368271253558e-05,
      "loss": 10.2254,
      "step": 2095
    },
    {
      "epoch": 0.25986882811533224,
      "grad_norm": 0.5150573253631592,
      "learning_rate": 3.701274594728376e-05,
      "loss": 10.2225,
      "step": 2100
    },
    {
      "epoch": 0.2604875634203688,
      "grad_norm": 0.5392067432403564,
      "learning_rate": 3.698180918203193e-05,
      "loss": 10.2095,
      "step": 2105
    },
    {
      "epoch": 0.26110629872540525,
      "grad_norm": 0.4992729425430298,
      "learning_rate": 3.6950872416780104e-05,
      "loss": 10.2259,
      "step": 2110
    },
    {
      "epoch": 0.2617250340304418,
      "grad_norm": 0.5438627600669861,
      "learning_rate": 3.691993565152828e-05,
      "loss": 10.215,
      "step": 2115
    },
    {
      "epoch": 0.26234376933547826,
      "grad_norm": 0.524542510509491,
      "learning_rate": 3.688899888627646e-05,
      "loss": 10.2206,
      "step": 2120
    },
    {
      "epoch": 0.2629625046405148,
      "grad_norm": 0.5336219668388367,
      "learning_rate": 3.6858062121024625e-05,
      "loss": 10.221,
      "step": 2125
    },
    {
      "epoch": 0.26358123994555127,
      "grad_norm": 0.5361073017120361,
      "learning_rate": 3.68271253557728e-05,
      "loss": 10.2007,
      "step": 2130
    },
    {
      "epoch": 0.2641999752505878,
      "grad_norm": 0.5253500938415527,
      "learning_rate": 3.679618859052097e-05,
      "loss": 10.2072,
      "step": 2135
    },
    {
      "epoch": 0.2648187105556243,
      "grad_norm": 0.5538864731788635,
      "learning_rate": 3.676525182526915e-05,
      "loss": 10.2004,
      "step": 2140
    },
    {
      "epoch": 0.2654374458606608,
      "grad_norm": 0.5351826548576355,
      "learning_rate": 3.6734315060017325e-05,
      "loss": 10.2119,
      "step": 2145
    },
    {
      "epoch": 0.2660561811656973,
      "grad_norm": 0.5122423768043518,
      "learning_rate": 3.67033782947655e-05,
      "loss": 10.2178,
      "step": 2150
    },
    {
      "epoch": 0.2666749164707338,
      "grad_norm": 0.521680474281311,
      "learning_rate": 3.667244152951367e-05,
      "loss": 10.2154,
      "step": 2155
    },
    {
      "epoch": 0.2672936517757703,
      "grad_norm": 0.5362449884414673,
      "learning_rate": 3.664150476426185e-05,
      "loss": 10.2005,
      "step": 2160
    },
    {
      "epoch": 0.26791238708080684,
      "grad_norm": 0.528174102306366,
      "learning_rate": 3.6610567999010026e-05,
      "loss": 10.2116,
      "step": 2165
    },
    {
      "epoch": 0.2685311223858433,
      "grad_norm": 0.527911365032196,
      "learning_rate": 3.65796312337582e-05,
      "loss": 10.2035,
      "step": 2170
    },
    {
      "epoch": 0.26914985769087985,
      "grad_norm": 0.5170944333076477,
      "learning_rate": 3.654869446850637e-05,
      "loss": 10.2163,
      "step": 2175
    },
    {
      "epoch": 0.2697685929959163,
      "grad_norm": 0.5491999387741089,
      "learning_rate": 3.651775770325455e-05,
      "loss": 10.1953,
      "step": 2180
    },
    {
      "epoch": 0.27038732830095286,
      "grad_norm": 0.5403200387954712,
      "learning_rate": 3.6486820938002726e-05,
      "loss": 10.1961,
      "step": 2185
    },
    {
      "epoch": 0.27100606360598933,
      "grad_norm": 0.514692485332489,
      "learning_rate": 3.64558841727509e-05,
      "loss": 10.2182,
      "step": 2190
    },
    {
      "epoch": 0.27162479891102587,
      "grad_norm": 0.5412871837615967,
      "learning_rate": 3.642494740749907e-05,
      "loss": 10.2054,
      "step": 2195
    },
    {
      "epoch": 0.27224353421606234,
      "grad_norm": 0.5196695923805237,
      "learning_rate": 3.6394010642247253e-05,
      "loss": 10.1941,
      "step": 2200
    },
    {
      "epoch": 0.2728622695210989,
      "grad_norm": 0.5117648243904114,
      "learning_rate": 3.636307387699543e-05,
      "loss": 10.1975,
      "step": 2205
    },
    {
      "epoch": 0.27348100482613535,
      "grad_norm": 0.5565699934959412,
      "learning_rate": 3.63321371117436e-05,
      "loss": 10.2104,
      "step": 2210
    },
    {
      "epoch": 0.2740997401311719,
      "grad_norm": 0.5243865847587585,
      "learning_rate": 3.6301200346491774e-05,
      "loss": 10.1998,
      "step": 2215
    },
    {
      "epoch": 0.27471847543620836,
      "grad_norm": 0.5134570002555847,
      "learning_rate": 3.627026358123995e-05,
      "loss": 10.1976,
      "step": 2220
    },
    {
      "epoch": 0.2753372107412449,
      "grad_norm": 0.5243655443191528,
      "learning_rate": 3.623932681598812e-05,
      "loss": 10.2113,
      "step": 2225
    },
    {
      "epoch": 0.2759559460462814,
      "grad_norm": 0.5210060477256775,
      "learning_rate": 3.6208390050736294e-05,
      "loss": 10.1968,
      "step": 2230
    },
    {
      "epoch": 0.2765746813513179,
      "grad_norm": 0.5466389060020447,
      "learning_rate": 3.617745328548447e-05,
      "loss": 10.1913,
      "step": 2235
    },
    {
      "epoch": 0.2771934166563544,
      "grad_norm": 0.5364136695861816,
      "learning_rate": 3.614651652023265e-05,
      "loss": 10.189,
      "step": 2240
    },
    {
      "epoch": 0.2778121519613909,
      "grad_norm": 0.5455203652381897,
      "learning_rate": 3.611557975498082e-05,
      "loss": 10.1936,
      "step": 2245
    },
    {
      "epoch": 0.2784308872664274,
      "grad_norm": 0.5356203317642212,
      "learning_rate": 3.6084642989728995e-05,
      "loss": 10.1955,
      "step": 2250
    },
    {
      "epoch": 0.27904962257146393,
      "grad_norm": 0.5339217185974121,
      "learning_rate": 3.605370622447717e-05,
      "loss": 10.1928,
      "step": 2255
    },
    {
      "epoch": 0.2796683578765004,
      "grad_norm": 0.5512320399284363,
      "learning_rate": 3.602276945922535e-05,
      "loss": 10.1853,
      "step": 2260
    },
    {
      "epoch": 0.28028709318153694,
      "grad_norm": 0.5355886220932007,
      "learning_rate": 3.599183269397352e-05,
      "loss": 10.189,
      "step": 2265
    },
    {
      "epoch": 0.2809058284865734,
      "grad_norm": 0.562099814414978,
      "learning_rate": 3.5960895928721695e-05,
      "loss": 10.1903,
      "step": 2270
    },
    {
      "epoch": 0.28152456379160995,
      "grad_norm": 0.5276872515678406,
      "learning_rate": 3.592995916346987e-05,
      "loss": 10.179,
      "step": 2275
    },
    {
      "epoch": 0.2821432990966464,
      "grad_norm": 0.5428574085235596,
      "learning_rate": 3.589902239821805e-05,
      "loss": 10.1947,
      "step": 2280
    },
    {
      "epoch": 0.28276203440168296,
      "grad_norm": 0.5484470725059509,
      "learning_rate": 3.586808563296622e-05,
      "loss": 10.1841,
      "step": 2285
    },
    {
      "epoch": 0.2833807697067195,
      "grad_norm": 0.5397262573242188,
      "learning_rate": 3.5837148867714396e-05,
      "loss": 10.1832,
      "step": 2290
    },
    {
      "epoch": 0.28399950501175597,
      "grad_norm": 0.5278793573379517,
      "learning_rate": 3.580621210246257e-05,
      "loss": 10.1839,
      "step": 2295
    },
    {
      "epoch": 0.2846182403167925,
      "grad_norm": 0.5704829096794128,
      "learning_rate": 3.577527533721074e-05,
      "loss": 10.1697,
      "step": 2300
    },
    {
      "epoch": 0.285236975621829,
      "grad_norm": 0.5560265779495239,
      "learning_rate": 3.5744338571958916e-05,
      "loss": 10.1715,
      "step": 2305
    },
    {
      "epoch": 0.2858557109268655,
      "grad_norm": 0.5564472675323486,
      "learning_rate": 3.571340180670709e-05,
      "loss": 10.186,
      "step": 2310
    },
    {
      "epoch": 0.286474446231902,
      "grad_norm": 0.564301073551178,
      "learning_rate": 3.568246504145526e-05,
      "loss": 10.1738,
      "step": 2315
    },
    {
      "epoch": 0.2870931815369385,
      "grad_norm": 0.5527178645133972,
      "learning_rate": 3.565152827620344e-05,
      "loss": 10.1725,
      "step": 2320
    },
    {
      "epoch": 0.287711916841975,
      "grad_norm": 0.5559393763542175,
      "learning_rate": 3.562059151095162e-05,
      "loss": 10.1674,
      "step": 2325
    },
    {
      "epoch": 0.28833065214701153,
      "grad_norm": 0.5433224439620972,
      "learning_rate": 3.558965474569979e-05,
      "loss": 10.1791,
      "step": 2330
    },
    {
      "epoch": 0.288949387452048,
      "grad_norm": 0.5532644987106323,
      "learning_rate": 3.5558717980447964e-05,
      "loss": 10.1729,
      "step": 2335
    },
    {
      "epoch": 0.28956812275708455,
      "grad_norm": 0.542407214641571,
      "learning_rate": 3.5527781215196144e-05,
      "loss": 10.168,
      "step": 2340
    },
    {
      "epoch": 0.290186858062121,
      "grad_norm": 0.5410732626914978,
      "learning_rate": 3.549684444994432e-05,
      "loss": 10.1723,
      "step": 2345
    },
    {
      "epoch": 0.29080559336715756,
      "grad_norm": 0.5538107752799988,
      "learning_rate": 3.546590768469249e-05,
      "loss": 10.1679,
      "step": 2350
    },
    {
      "epoch": 0.29142432867219403,
      "grad_norm": 0.5552197694778442,
      "learning_rate": 3.5434970919440664e-05,
      "loss": 10.1657,
      "step": 2355
    },
    {
      "epoch": 0.29204306397723057,
      "grad_norm": 0.5775472521781921,
      "learning_rate": 3.5404034154188844e-05,
      "loss": 10.1649,
      "step": 2360
    },
    {
      "epoch": 0.29266179928226704,
      "grad_norm": 0.536899983882904,
      "learning_rate": 3.537309738893702e-05,
      "loss": 10.1707,
      "step": 2365
    },
    {
      "epoch": 0.2932805345873036,
      "grad_norm": 0.6408485770225525,
      "learning_rate": 3.534216062368519e-05,
      "loss": 10.1701,
      "step": 2370
    },
    {
      "epoch": 0.29389926989234005,
      "grad_norm": 0.5615842342376709,
      "learning_rate": 3.5311223858433365e-05,
      "loss": 10.1627,
      "step": 2375
    },
    {
      "epoch": 0.2945180051973766,
      "grad_norm": 0.5555476546287537,
      "learning_rate": 3.528028709318154e-05,
      "loss": 10.164,
      "step": 2380
    },
    {
      "epoch": 0.29513674050241306,
      "grad_norm": 0.5477513074874878,
      "learning_rate": 3.524935032792971e-05,
      "loss": 10.1723,
      "step": 2385
    },
    {
      "epoch": 0.2957554758074496,
      "grad_norm": 0.5422446727752686,
      "learning_rate": 3.5218413562677885e-05,
      "loss": 10.1608,
      "step": 2390
    },
    {
      "epoch": 0.2963742111124861,
      "grad_norm": 0.5608952045440674,
      "learning_rate": 3.518747679742606e-05,
      "loss": 10.172,
      "step": 2395
    },
    {
      "epoch": 0.2969929464175226,
      "grad_norm": 0.5634661316871643,
      "learning_rate": 3.515654003217424e-05,
      "loss": 10.155,
      "step": 2400
    },
    {
      "epoch": 0.2976116817225591,
      "grad_norm": 0.5451548099517822,
      "learning_rate": 3.512560326692241e-05,
      "loss": 10.1548,
      "step": 2405
    },
    {
      "epoch": 0.2982304170275956,
      "grad_norm": 0.5389381647109985,
      "learning_rate": 3.5094666501670586e-05,
      "loss": 10.1523,
      "step": 2410
    },
    {
      "epoch": 0.2988491523326321,
      "grad_norm": 0.6239843368530273,
      "learning_rate": 3.506372973641876e-05,
      "loss": 10.1705,
      "step": 2415
    },
    {
      "epoch": 0.29946788763766863,
      "grad_norm": 0.56588214635849,
      "learning_rate": 3.503279297116694e-05,
      "loss": 10.1524,
      "step": 2420
    },
    {
      "epoch": 0.3000866229427051,
      "grad_norm": 0.5271834135055542,
      "learning_rate": 3.500185620591511e-05,
      "loss": 10.1621,
      "step": 2425
    },
    {
      "epoch": 0.30070535824774164,
      "grad_norm": 0.551410973072052,
      "learning_rate": 3.4970919440663286e-05,
      "loss": 10.1583,
      "step": 2430
    },
    {
      "epoch": 0.3013240935527781,
      "grad_norm": 0.5519779324531555,
      "learning_rate": 3.493998267541146e-05,
      "loss": 10.1557,
      "step": 2435
    },
    {
      "epoch": 0.30194282885781465,
      "grad_norm": 0.576538622379303,
      "learning_rate": 3.490904591015964e-05,
      "loss": 10.1617,
      "step": 2440
    },
    {
      "epoch": 0.3025615641628511,
      "grad_norm": 0.567689061164856,
      "learning_rate": 3.4878109144907813e-05,
      "loss": 10.1448,
      "step": 2445
    },
    {
      "epoch": 0.30318029946788766,
      "grad_norm": 0.5644587278366089,
      "learning_rate": 3.484717237965599e-05,
      "loss": 10.1499,
      "step": 2450
    },
    {
      "epoch": 0.30379903477292414,
      "grad_norm": 0.5614405870437622,
      "learning_rate": 3.481623561440416e-05,
      "loss": 10.1535,
      "step": 2455
    },
    {
      "epoch": 0.30441777007796067,
      "grad_norm": 0.5810660719871521,
      "learning_rate": 3.4785298849152334e-05,
      "loss": 10.1434,
      "step": 2460
    },
    {
      "epoch": 0.30503650538299715,
      "grad_norm": 0.5703725814819336,
      "learning_rate": 3.475436208390051e-05,
      "loss": 10.1488,
      "step": 2465
    },
    {
      "epoch": 0.3056552406880337,
      "grad_norm": 0.590947687625885,
      "learning_rate": 3.472342531864868e-05,
      "loss": 10.1324,
      "step": 2470
    },
    {
      "epoch": 0.30627397599307016,
      "grad_norm": 0.5714672803878784,
      "learning_rate": 3.4692488553396854e-05,
      "loss": 10.1426,
      "step": 2475
    },
    {
      "epoch": 0.3068927112981067,
      "grad_norm": 0.5565294027328491,
      "learning_rate": 3.4661551788145034e-05,
      "loss": 10.1411,
      "step": 2480
    },
    {
      "epoch": 0.30751144660314317,
      "grad_norm": 0.5577239990234375,
      "learning_rate": 3.463061502289321e-05,
      "loss": 10.161,
      "step": 2485
    },
    {
      "epoch": 0.3081301819081797,
      "grad_norm": 0.5801849961280823,
      "learning_rate": 3.459967825764138e-05,
      "loss": 10.1395,
      "step": 2490
    },
    {
      "epoch": 0.3087489172132162,
      "grad_norm": 0.5624817609786987,
      "learning_rate": 3.4568741492389555e-05,
      "loss": 10.1467,
      "step": 2495
    },
    {
      "epoch": 0.3093676525182527,
      "grad_norm": 0.5533233284950256,
      "learning_rate": 3.4537804727137735e-05,
      "loss": 10.1504,
      "step": 2500
    },
    {
      "epoch": 0.3099863878232892,
      "grad_norm": 0.5712581872940063,
      "learning_rate": 3.450686796188591e-05,
      "loss": 10.1374,
      "step": 2505
    },
    {
      "epoch": 0.3106051231283257,
      "grad_norm": 0.5561612844467163,
      "learning_rate": 3.447593119663408e-05,
      "loss": 10.1315,
      "step": 2510
    },
    {
      "epoch": 0.3112238584333622,
      "grad_norm": 0.5466171503067017,
      "learning_rate": 3.4444994431382255e-05,
      "loss": 10.1466,
      "step": 2515
    },
    {
      "epoch": 0.31184259373839873,
      "grad_norm": 0.577054500579834,
      "learning_rate": 3.4414057666130436e-05,
      "loss": 10.1334,
      "step": 2520
    },
    {
      "epoch": 0.3124613290434352,
      "grad_norm": 0.5567629337310791,
      "learning_rate": 3.438312090087861e-05,
      "loss": 10.1333,
      "step": 2525
    },
    {
      "epoch": 0.31308006434847174,
      "grad_norm": 0.5648949146270752,
      "learning_rate": 3.435218413562678e-05,
      "loss": 10.139,
      "step": 2530
    },
    {
      "epoch": 0.3136987996535082,
      "grad_norm": 0.556597113609314,
      "learning_rate": 3.4321247370374956e-05,
      "loss": 10.1327,
      "step": 2535
    },
    {
      "epoch": 0.31431753495854475,
      "grad_norm": 0.5550557374954224,
      "learning_rate": 3.429031060512313e-05,
      "loss": 10.1312,
      "step": 2540
    },
    {
      "epoch": 0.31493627026358123,
      "grad_norm": 0.5863808989524841,
      "learning_rate": 3.42593738398713e-05,
      "loss": 10.125,
      "step": 2545
    },
    {
      "epoch": 0.31555500556861776,
      "grad_norm": 0.5684635043144226,
      "learning_rate": 3.4228437074619476e-05,
      "loss": 10.1337,
      "step": 2550
    },
    {
      "epoch": 0.31617374087365424,
      "grad_norm": 0.5591942071914673,
      "learning_rate": 3.419750030936765e-05,
      "loss": 10.1393,
      "step": 2555
    },
    {
      "epoch": 0.3167924761786908,
      "grad_norm": 0.5475903749465942,
      "learning_rate": 3.416656354411583e-05,
      "loss": 10.118,
      "step": 2560
    },
    {
      "epoch": 0.31741121148372725,
      "grad_norm": 0.5462509989738464,
      "learning_rate": 3.4135626778864e-05,
      "loss": 10.1366,
      "step": 2565
    },
    {
      "epoch": 0.3180299467887638,
      "grad_norm": 0.5633292198181152,
      "learning_rate": 3.410469001361218e-05,
      "loss": 10.126,
      "step": 2570
    },
    {
      "epoch": 0.31864868209380026,
      "grad_norm": 0.5854493975639343,
      "learning_rate": 3.407375324836035e-05,
      "loss": 10.1205,
      "step": 2575
    },
    {
      "epoch": 0.3192674173988368,
      "grad_norm": 0.5803735852241516,
      "learning_rate": 3.404281648310853e-05,
      "loss": 10.1274,
      "step": 2580
    },
    {
      "epoch": 0.31988615270387327,
      "grad_norm": 0.562687873840332,
      "learning_rate": 3.4011879717856704e-05,
      "loss": 10.1207,
      "step": 2585
    },
    {
      "epoch": 0.3205048880089098,
      "grad_norm": 0.5858197212219238,
      "learning_rate": 3.398094295260488e-05,
      "loss": 10.1064,
      "step": 2590
    },
    {
      "epoch": 0.3211236233139463,
      "grad_norm": 0.5673084855079651,
      "learning_rate": 3.395000618735305e-05,
      "loss": 10.1131,
      "step": 2595
    },
    {
      "epoch": 0.3217423586189828,
      "grad_norm": 1.0853283405303955,
      "learning_rate": 3.3919069422101224e-05,
      "loss": 10.1235,
      "step": 2600
    },
    {
      "epoch": 0.3223610939240193,
      "grad_norm": 0.5707475543022156,
      "learning_rate": 3.3888132656849405e-05,
      "loss": 10.1112,
      "step": 2605
    },
    {
      "epoch": 0.3229798292290558,
      "grad_norm": 0.5774579644203186,
      "learning_rate": 3.385719589159758e-05,
      "loss": 10.131,
      "step": 2610
    },
    {
      "epoch": 0.3235985645340923,
      "grad_norm": 0.582822322845459,
      "learning_rate": 3.382625912634575e-05,
      "loss": 10.1165,
      "step": 2615
    },
    {
      "epoch": 0.32421729983912884,
      "grad_norm": 0.5913942456245422,
      "learning_rate": 3.3795322361093925e-05,
      "loss": 10.119,
      "step": 2620
    },
    {
      "epoch": 0.3248360351441653,
      "grad_norm": 0.5938135385513306,
      "learning_rate": 3.3764385595842105e-05,
      "loss": 10.1137,
      "step": 2625
    },
    {
      "epoch": 0.32545477044920185,
      "grad_norm": 0.5662577152252197,
      "learning_rate": 3.373344883059028e-05,
      "loss": 10.1278,
      "step": 2630
    },
    {
      "epoch": 0.3260735057542383,
      "grad_norm": 0.5665355324745178,
      "learning_rate": 3.3702512065338445e-05,
      "loss": 10.1352,
      "step": 2635
    },
    {
      "epoch": 0.32669224105927486,
      "grad_norm": 0.5804416537284851,
      "learning_rate": 3.367157530008662e-05,
      "loss": 10.1128,
      "step": 2640
    },
    {
      "epoch": 0.32731097636431133,
      "grad_norm": 0.5784264802932739,
      "learning_rate": 3.36406385348348e-05,
      "loss": 10.108,
      "step": 2645
    },
    {
      "epoch": 0.32792971166934787,
      "grad_norm": 0.5787414312362671,
      "learning_rate": 3.360970176958297e-05,
      "loss": 10.1035,
      "step": 2650
    },
    {
      "epoch": 0.32854844697438434,
      "grad_norm": 0.5738536715507507,
      "learning_rate": 3.3578765004331146e-05,
      "loss": 10.1039,
      "step": 2655
    },
    {
      "epoch": 0.3291671822794209,
      "grad_norm": 0.5652865171432495,
      "learning_rate": 3.354782823907932e-05,
      "loss": 10.1124,
      "step": 2660
    },
    {
      "epoch": 0.32978591758445736,
      "grad_norm": 0.5688542127609253,
      "learning_rate": 3.35168914738275e-05,
      "loss": 10.1122,
      "step": 2665
    },
    {
      "epoch": 0.3304046528894939,
      "grad_norm": 0.5809018015861511,
      "learning_rate": 3.348595470857567e-05,
      "loss": 10.1058,
      "step": 2670
    },
    {
      "epoch": 0.33102338819453037,
      "grad_norm": 0.5836099982261658,
      "learning_rate": 3.3455017943323846e-05,
      "loss": 10.1063,
      "step": 2675
    },
    {
      "epoch": 0.3316421234995669,
      "grad_norm": 0.5877246856689453,
      "learning_rate": 3.342408117807202e-05,
      "loss": 10.114,
      "step": 2680
    },
    {
      "epoch": 0.3322608588046034,
      "grad_norm": 0.567120373249054,
      "learning_rate": 3.33931444128202e-05,
      "loss": 10.1121,
      "step": 2685
    },
    {
      "epoch": 0.3328795941096399,
      "grad_norm": 0.571016252040863,
      "learning_rate": 3.3362207647568373e-05,
      "loss": 10.1093,
      "step": 2690
    },
    {
      "epoch": 0.3334983294146764,
      "grad_norm": 0.9175690412521362,
      "learning_rate": 3.333127088231655e-05,
      "loss": 10.0987,
      "step": 2695
    },
    {
      "epoch": 0.3341170647197129,
      "grad_norm": 0.5782616138458252,
      "learning_rate": 3.330033411706472e-05,
      "loss": 10.1154,
      "step": 2700
    },
    {
      "epoch": 0.3347358000247494,
      "grad_norm": 0.5640605092048645,
      "learning_rate": 3.32693973518129e-05,
      "loss": 10.1098,
      "step": 2705
    },
    {
      "epoch": 0.33535453532978593,
      "grad_norm": 0.5989370942115784,
      "learning_rate": 3.3238460586561074e-05,
      "loss": 10.0961,
      "step": 2710
    },
    {
      "epoch": 0.3359732706348224,
      "grad_norm": 0.5838376879692078,
      "learning_rate": 3.320752382130925e-05,
      "loss": 10.0998,
      "step": 2715
    },
    {
      "epoch": 0.33659200593985894,
      "grad_norm": 0.5906234979629517,
      "learning_rate": 3.317658705605742e-05,
      "loss": 10.0957,
      "step": 2720
    },
    {
      "epoch": 0.3372107412448954,
      "grad_norm": 0.5839958786964417,
      "learning_rate": 3.3145650290805594e-05,
      "loss": 10.1077,
      "step": 2725
    },
    {
      "epoch": 0.33782947654993195,
      "grad_norm": 0.5829567909240723,
      "learning_rate": 3.311471352555377e-05,
      "loss": 10.1,
      "step": 2730
    },
    {
      "epoch": 0.33844821185496843,
      "grad_norm": 0.5803480744361877,
      "learning_rate": 3.308377676030194e-05,
      "loss": 10.0995,
      "step": 2735
    },
    {
      "epoch": 0.33906694716000496,
      "grad_norm": 0.5877890586853027,
      "learning_rate": 3.3052839995050115e-05,
      "loss": 10.0904,
      "step": 2740
    },
    {
      "epoch": 0.33968568246504144,
      "grad_norm": 0.5949726700782776,
      "learning_rate": 3.3021903229798295e-05,
      "loss": 10.097,
      "step": 2745
    },
    {
      "epoch": 0.34030441777007797,
      "grad_norm": 0.6160765290260315,
      "learning_rate": 3.299096646454647e-05,
      "loss": 10.0862,
      "step": 2750
    },
    {
      "epoch": 0.34092315307511445,
      "grad_norm": 0.5806241035461426,
      "learning_rate": 3.296002969929464e-05,
      "loss": 10.0991,
      "step": 2755
    },
    {
      "epoch": 0.341541888380151,
      "grad_norm": 0.6004524827003479,
      "learning_rate": 3.2929092934042815e-05,
      "loss": 10.0784,
      "step": 2760
    },
    {
      "epoch": 0.34216062368518746,
      "grad_norm": 0.5474337935447693,
      "learning_rate": 3.2898156168790996e-05,
      "loss": 10.095,
      "step": 2765
    },
    {
      "epoch": 0.342779358990224,
      "grad_norm": 0.5872565507888794,
      "learning_rate": 3.286721940353917e-05,
      "loss": 10.0965,
      "step": 2770
    },
    {
      "epoch": 0.34339809429526047,
      "grad_norm": 0.6236754655838013,
      "learning_rate": 3.283628263828734e-05,
      "loss": 10.0936,
      "step": 2775
    },
    {
      "epoch": 0.344016829600297,
      "grad_norm": 0.6031074523925781,
      "learning_rate": 3.2805345873035516e-05,
      "loss": 10.084,
      "step": 2780
    },
    {
      "epoch": 0.3446355649053335,
      "grad_norm": 0.5818508267402649,
      "learning_rate": 3.2774409107783696e-05,
      "loss": 10.0918,
      "step": 2785
    },
    {
      "epoch": 0.34525430021037,
      "grad_norm": 0.6092684268951416,
      "learning_rate": 3.274347234253187e-05,
      "loss": 10.0895,
      "step": 2790
    },
    {
      "epoch": 0.3458730355154065,
      "grad_norm": 0.5809220671653748,
      "learning_rate": 3.271253557728004e-05,
      "loss": 10.0979,
      "step": 2795
    },
    {
      "epoch": 0.346491770820443,
      "grad_norm": 0.6162949800491333,
      "learning_rate": 3.2681598812028216e-05,
      "loss": 10.077,
      "step": 2800
    },
    {
      "epoch": 0.3471105061254795,
      "grad_norm": 0.5996999740600586,
      "learning_rate": 3.265066204677639e-05,
      "loss": 10.081,
      "step": 2805
    },
    {
      "epoch": 0.34772924143051603,
      "grad_norm": 0.6066058278083801,
      "learning_rate": 3.2619725281524563e-05,
      "loss": 10.0723,
      "step": 2810
    },
    {
      "epoch": 0.3483479767355525,
      "grad_norm": 0.6053802967071533,
      "learning_rate": 3.258878851627274e-05,
      "loss": 10.0757,
      "step": 2815
    },
    {
      "epoch": 0.34896671204058904,
      "grad_norm": 0.5812010765075684,
      "learning_rate": 3.255785175102091e-05,
      "loss": 10.0896,
      "step": 2820
    },
    {
      "epoch": 0.3495854473456255,
      "grad_norm": 0.6021271347999573,
      "learning_rate": 3.252691498576909e-05,
      "loss": 10.0771,
      "step": 2825
    },
    {
      "epoch": 0.35020418265066205,
      "grad_norm": 0.5963929295539856,
      "learning_rate": 3.2495978220517264e-05,
      "loss": 10.0774,
      "step": 2830
    },
    {
      "epoch": 0.35082291795569853,
      "grad_norm": 0.5904510617256165,
      "learning_rate": 3.246504145526544e-05,
      "loss": 10.0755,
      "step": 2835
    },
    {
      "epoch": 0.35144165326073507,
      "grad_norm": 0.614916980266571,
      "learning_rate": 3.243410469001361e-05,
      "loss": 10.0746,
      "step": 2840
    },
    {
      "epoch": 0.35206038856577154,
      "grad_norm": 0.6029673218727112,
      "learning_rate": 3.240316792476179e-05,
      "loss": 10.0742,
      "step": 2845
    },
    {
      "epoch": 0.3526791238708081,
      "grad_norm": 0.590315043926239,
      "learning_rate": 3.2372231159509965e-05,
      "loss": 10.0745,
      "step": 2850
    },
    {
      "epoch": 0.35329785917584455,
      "grad_norm": 0.5704329609870911,
      "learning_rate": 3.234129439425814e-05,
      "loss": 10.0843,
      "step": 2855
    },
    {
      "epoch": 0.3539165944808811,
      "grad_norm": 0.6092314720153809,
      "learning_rate": 3.231035762900631e-05,
      "loss": 10.0679,
      "step": 2860
    },
    {
      "epoch": 0.35453532978591756,
      "grad_norm": 0.6031217575073242,
      "learning_rate": 3.227942086375449e-05,
      "loss": 10.0822,
      "step": 2865
    },
    {
      "epoch": 0.3551540650909541,
      "grad_norm": 0.5985864996910095,
      "learning_rate": 3.2248484098502665e-05,
      "loss": 10.071,
      "step": 2870
    },
    {
      "epoch": 0.3557728003959906,
      "grad_norm": 0.589424192905426,
      "learning_rate": 3.221754733325084e-05,
      "loss": 10.0676,
      "step": 2875
    },
    {
      "epoch": 0.3563915357010271,
      "grad_norm": 0.6117711067199707,
      "learning_rate": 3.218661056799901e-05,
      "loss": 10.0682,
      "step": 2880
    },
    {
      "epoch": 0.3570102710060636,
      "grad_norm": 0.5962153673171997,
      "learning_rate": 3.2155673802747185e-05,
      "loss": 10.0859,
      "step": 2885
    },
    {
      "epoch": 0.3576290063111001,
      "grad_norm": 0.5924883484840393,
      "learning_rate": 3.212473703749536e-05,
      "loss": 10.0639,
      "step": 2890
    },
    {
      "epoch": 0.3582477416161366,
      "grad_norm": 0.5883165597915649,
      "learning_rate": 3.209380027224353e-05,
      "loss": 10.0724,
      "step": 2895
    },
    {
      "epoch": 0.3588664769211731,
      "grad_norm": 0.6132479906082153,
      "learning_rate": 3.2062863506991706e-05,
      "loss": 10.07,
      "step": 2900
    },
    {
      "epoch": 0.3594852122262096,
      "grad_norm": 0.5924638509750366,
      "learning_rate": 3.2031926741739886e-05,
      "loss": 10.0673,
      "step": 2905
    },
    {
      "epoch": 0.36010394753124614,
      "grad_norm": 0.6466853022575378,
      "learning_rate": 3.200098997648806e-05,
      "loss": 10.0699,
      "step": 2910
    },
    {
      "epoch": 0.3607226828362826,
      "grad_norm": 0.6042618155479431,
      "learning_rate": 3.197005321123623e-05,
      "loss": 10.0587,
      "step": 2915
    },
    {
      "epoch": 0.36134141814131915,
      "grad_norm": 0.6131114959716797,
      "learning_rate": 3.1939116445984406e-05,
      "loss": 10.0639,
      "step": 2920
    },
    {
      "epoch": 0.3619601534463556,
      "grad_norm": 0.6039714217185974,
      "learning_rate": 3.190817968073259e-05,
      "loss": 10.0728,
      "step": 2925
    },
    {
      "epoch": 0.36257888875139216,
      "grad_norm": 0.6092924475669861,
      "learning_rate": 3.187724291548076e-05,
      "loss": 10.0489,
      "step": 2930
    },
    {
      "epoch": 0.36319762405642864,
      "grad_norm": 0.6210100054740906,
      "learning_rate": 3.1846306150228934e-05,
      "loss": 10.0702,
      "step": 2935
    },
    {
      "epoch": 0.36381635936146517,
      "grad_norm": 0.6217057108879089,
      "learning_rate": 3.181536938497711e-05,
      "loss": 10.057,
      "step": 2940
    },
    {
      "epoch": 0.36443509466650165,
      "grad_norm": 0.5919103622436523,
      "learning_rate": 3.178443261972529e-05,
      "loss": 10.0606,
      "step": 2945
    },
    {
      "epoch": 0.3650538299715382,
      "grad_norm": 0.6099032759666443,
      "learning_rate": 3.175349585447346e-05,
      "loss": 10.0608,
      "step": 2950
    },
    {
      "epoch": 0.36567256527657466,
      "grad_norm": 0.5981515049934387,
      "learning_rate": 3.1722559089221634e-05,
      "loss": 10.0489,
      "step": 2955
    },
    {
      "epoch": 0.3662913005816112,
      "grad_norm": 0.606441080570221,
      "learning_rate": 3.169162232396981e-05,
      "loss": 10.0551,
      "step": 2960
    },
    {
      "epoch": 0.36691003588664767,
      "grad_norm": 0.6148017644882202,
      "learning_rate": 3.166068555871798e-05,
      "loss": 10.0532,
      "step": 2965
    },
    {
      "epoch": 0.3675287711916842,
      "grad_norm": 0.5998250246047974,
      "learning_rate": 3.1629748793466154e-05,
      "loss": 10.0496,
      "step": 2970
    },
    {
      "epoch": 0.3681475064967207,
      "grad_norm": 0.6314706206321716,
      "learning_rate": 3.159881202821433e-05,
      "loss": 10.0554,
      "step": 2975
    },
    {
      "epoch": 0.3687662418017572,
      "grad_norm": 0.614288330078125,
      "learning_rate": 3.15678752629625e-05,
      "loss": 10.0534,
      "step": 2980
    },
    {
      "epoch": 0.3693849771067937,
      "grad_norm": 0.6156412959098816,
      "learning_rate": 3.153693849771068e-05,
      "loss": 10.0379,
      "step": 2985
    },
    {
      "epoch": 0.3700037124118302,
      "grad_norm": 0.6273133158683777,
      "learning_rate": 3.1506001732458855e-05,
      "loss": 10.0417,
      "step": 2990
    },
    {
      "epoch": 0.3706224477168667,
      "grad_norm": 0.5896394848823547,
      "learning_rate": 3.147506496720703e-05,
      "loss": 10.0572,
      "step": 2995
    },
    {
      "epoch": 0.37124118302190323,
      "grad_norm": 0.608906090259552,
      "learning_rate": 3.14441282019552e-05,
      "loss": 10.0416,
      "step": 3000
    },
    {
      "epoch": 0.3718599183269397,
      "grad_norm": 0.6128961443901062,
      "learning_rate": 3.141319143670338e-05,
      "loss": 10.0524,
      "step": 3005
    },
    {
      "epoch": 0.37247865363197624,
      "grad_norm": 0.6167327761650085,
      "learning_rate": 3.1382254671451556e-05,
      "loss": 10.057,
      "step": 3010
    },
    {
      "epoch": 0.3730973889370127,
      "grad_norm": 0.6249868869781494,
      "learning_rate": 3.135131790619973e-05,
      "loss": 10.0369,
      "step": 3015
    },
    {
      "epoch": 0.37371612424204925,
      "grad_norm": 0.6130456924438477,
      "learning_rate": 3.13203811409479e-05,
      "loss": 10.0411,
      "step": 3020
    },
    {
      "epoch": 0.37433485954708573,
      "grad_norm": 0.6188594698905945,
      "learning_rate": 3.128944437569608e-05,
      "loss": 10.0403,
      "step": 3025
    },
    {
      "epoch": 0.37495359485212226,
      "grad_norm": 0.6378096342086792,
      "learning_rate": 3.1258507610444256e-05,
      "loss": 10.0294,
      "step": 3030
    },
    {
      "epoch": 0.37557233015715874,
      "grad_norm": 0.6289215683937073,
      "learning_rate": 3.122757084519243e-05,
      "loss": 10.0386,
      "step": 3035
    },
    {
      "epoch": 0.3761910654621953,
      "grad_norm": 0.6193420886993408,
      "learning_rate": 3.11966340799406e-05,
      "loss": 10.0342,
      "step": 3040
    },
    {
      "epoch": 0.37680980076723175,
      "grad_norm": 0.624709963798523,
      "learning_rate": 3.1165697314688777e-05,
      "loss": 10.0268,
      "step": 3045
    },
    {
      "epoch": 0.3774285360722683,
      "grad_norm": 0.6072433590888977,
      "learning_rate": 3.113476054943695e-05,
      "loss": 10.0262,
      "step": 3050
    },
    {
      "epoch": 0.3780472713773048,
      "grad_norm": 0.6172875761985779,
      "learning_rate": 3.1103823784185123e-05,
      "loss": 10.0449,
      "step": 3055
    },
    {
      "epoch": 0.3786660066823413,
      "grad_norm": 0.6180920600891113,
      "learning_rate": 3.10728870189333e-05,
      "loss": 10.0409,
      "step": 3060
    },
    {
      "epoch": 0.3792847419873778,
      "grad_norm": 0.6114706993103027,
      "learning_rate": 3.104195025368148e-05,
      "loss": 10.0454,
      "step": 3065
    },
    {
      "epoch": 0.3799034772924143,
      "grad_norm": 0.6364714503288269,
      "learning_rate": 3.101101348842965e-05,
      "loss": 10.024,
      "step": 3070
    },
    {
      "epoch": 0.38052221259745084,
      "grad_norm": 0.6162533760070801,
      "learning_rate": 3.0980076723177824e-05,
      "loss": 10.0346,
      "step": 3075
    },
    {
      "epoch": 0.3811409479024873,
      "grad_norm": 0.6092120409011841,
      "learning_rate": 3.0949139957926e-05,
      "loss": 10.0321,
      "step": 3080
    },
    {
      "epoch": 0.38175968320752385,
      "grad_norm": 0.6307060718536377,
      "learning_rate": 3.091820319267418e-05,
      "loss": 10.0261,
      "step": 3085
    },
    {
      "epoch": 0.3823784185125603,
      "grad_norm": 0.6243841648101807,
      "learning_rate": 3.088726642742235e-05,
      "loss": 10.0259,
      "step": 3090
    },
    {
      "epoch": 0.38299715381759686,
      "grad_norm": 0.6141773462295532,
      "learning_rate": 3.0856329662170525e-05,
      "loss": 10.0237,
      "step": 3095
    },
    {
      "epoch": 0.38361588912263334,
      "grad_norm": 0.6035345196723938,
      "learning_rate": 3.08253928969187e-05,
      "loss": 10.0345,
      "step": 3100
    },
    {
      "epoch": 0.38423462442766987,
      "grad_norm": 0.6252906322479248,
      "learning_rate": 3.079445613166688e-05,
      "loss": 10.0265,
      "step": 3105
    },
    {
      "epoch": 0.38485335973270635,
      "grad_norm": 0.6160735487937927,
      "learning_rate": 3.076351936641505e-05,
      "loss": 10.0234,
      "step": 3110
    },
    {
      "epoch": 0.3854720950377429,
      "grad_norm": 0.6521394848823547,
      "learning_rate": 3.0732582601163225e-05,
      "loss": 10.02,
      "step": 3115
    },
    {
      "epoch": 0.38609083034277936,
      "grad_norm": 0.6235247254371643,
      "learning_rate": 3.07016458359114e-05,
      "loss": 10.0339,
      "step": 3120
    },
    {
      "epoch": 0.3867095656478159,
      "grad_norm": 0.6203896999359131,
      "learning_rate": 3.067070907065958e-05,
      "loss": 10.0362,
      "step": 3125
    },
    {
      "epoch": 0.38732830095285237,
      "grad_norm": 0.6056758761405945,
      "learning_rate": 3.063977230540775e-05,
      "loss": 10.0297,
      "step": 3130
    },
    {
      "epoch": 0.3879470362578889,
      "grad_norm": 0.6412345170974731,
      "learning_rate": 3.0608835540155926e-05,
      "loss": 10.0222,
      "step": 3135
    },
    {
      "epoch": 0.3885657715629254,
      "grad_norm": 0.627006471157074,
      "learning_rate": 3.057789877490409e-05,
      "loss": 10.0192,
      "step": 3140
    },
    {
      "epoch": 0.3891845068679619,
      "grad_norm": 0.6388340592384338,
      "learning_rate": 3.054696200965227e-05,
      "loss": 10.0135,
      "step": 3145
    },
    {
      "epoch": 0.3898032421729984,
      "grad_norm": 0.6049745082855225,
      "learning_rate": 3.051602524440045e-05,
      "loss": 10.0263,
      "step": 3150
    },
    {
      "epoch": 0.3904219774780349,
      "grad_norm": 0.6268421411514282,
      "learning_rate": 3.0485088479148623e-05,
      "loss": 10.0199,
      "step": 3155
    },
    {
      "epoch": 0.3910407127830714,
      "grad_norm": 0.880540132522583,
      "learning_rate": 3.0454151713896796e-05,
      "loss": 10.0128,
      "step": 3160
    },
    {
      "epoch": 0.39165944808810793,
      "grad_norm": 0.6382395625114441,
      "learning_rate": 3.0423214948644973e-05,
      "loss": 10.0038,
      "step": 3165
    },
    {
      "epoch": 0.3922781833931444,
      "grad_norm": 0.6599711775779724,
      "learning_rate": 3.0392278183393147e-05,
      "loss": 10.0105,
      "step": 3170
    },
    {
      "epoch": 0.39289691869818094,
      "grad_norm": 0.6291724443435669,
      "learning_rate": 3.036134141814132e-05,
      "loss": 10.0058,
      "step": 3175
    },
    {
      "epoch": 0.3935156540032174,
      "grad_norm": 0.6206067204475403,
      "learning_rate": 3.0330404652889494e-05,
      "loss": 10.0207,
      "step": 3180
    },
    {
      "epoch": 0.39413438930825395,
      "grad_norm": 0.6241675615310669,
      "learning_rate": 3.0299467887637674e-05,
      "loss": 10.0078,
      "step": 3185
    },
    {
      "epoch": 0.39475312461329043,
      "grad_norm": 0.6514164209365845,
      "learning_rate": 3.0268531122385847e-05,
      "loss": 10.0242,
      "step": 3190
    },
    {
      "epoch": 0.39537185991832696,
      "grad_norm": 0.6376248002052307,
      "learning_rate": 3.023759435713402e-05,
      "loss": 10.0069,
      "step": 3195
    },
    {
      "epoch": 0.39599059522336344,
      "grad_norm": 0.6348063945770264,
      "learning_rate": 3.0206657591882194e-05,
      "loss": 10.0044,
      "step": 3200
    },
    {
      "epoch": 0.3966093305284,
      "grad_norm": 0.6451852917671204,
      "learning_rate": 3.017572082663037e-05,
      "loss": 10.0031,
      "step": 3205
    },
    {
      "epoch": 0.39722806583343645,
      "grad_norm": 0.6209341287612915,
      "learning_rate": 3.0144784061378544e-05,
      "loss": 10.017,
      "step": 3210
    },
    {
      "epoch": 0.397846801138473,
      "grad_norm": 0.616782546043396,
      "learning_rate": 3.0113847296126718e-05,
      "loss": 10.0059,
      "step": 3215
    },
    {
      "epoch": 0.39846553644350946,
      "grad_norm": 0.6388587355613708,
      "learning_rate": 3.008291053087489e-05,
      "loss": 10.0014,
      "step": 3220
    },
    {
      "epoch": 0.399084271748546,
      "grad_norm": 0.7735037207603455,
      "learning_rate": 3.005197376562307e-05,
      "loss": 10.0023,
      "step": 3225
    },
    {
      "epoch": 0.39970300705358247,
      "grad_norm": 0.6500544548034668,
      "learning_rate": 3.0021037000371245e-05,
      "loss": 10.0098,
      "step": 3230
    },
    {
      "epoch": 0.400321742358619,
      "grad_norm": 0.6311899423599243,
      "learning_rate": 2.999010023511942e-05,
      "loss": 9.9997,
      "step": 3235
    },
    {
      "epoch": 0.4009404776636555,
      "grad_norm": 0.6268082857131958,
      "learning_rate": 2.9959163469867592e-05,
      "loss": 9.9992,
      "step": 3240
    },
    {
      "epoch": 0.401559212968692,
      "grad_norm": 0.6419433355331421,
      "learning_rate": 2.992822670461577e-05,
      "loss": 9.9914,
      "step": 3245
    },
    {
      "epoch": 0.4021779482737285,
      "grad_norm": 0.6331334710121155,
      "learning_rate": 2.9897289939363942e-05,
      "loss": 10.002,
      "step": 3250
    },
    {
      "epoch": 0.402796683578765,
      "grad_norm": 0.6373754739761353,
      "learning_rate": 2.9866353174112116e-05,
      "loss": 9.9941,
      "step": 3255
    },
    {
      "epoch": 0.4034154188838015,
      "grad_norm": 0.6541115045547485,
      "learning_rate": 2.983541640886029e-05,
      "loss": 10.0024,
      "step": 3260
    },
    {
      "epoch": 0.40403415418883804,
      "grad_norm": 0.6446836590766907,
      "learning_rate": 2.980447964360847e-05,
      "loss": 9.9941,
      "step": 3265
    },
    {
      "epoch": 0.4046528894938745,
      "grad_norm": 0.6502975225448608,
      "learning_rate": 2.9773542878356643e-05,
      "loss": 9.9949,
      "step": 3270
    },
    {
      "epoch": 0.40527162479891105,
      "grad_norm": 0.6611225008964539,
      "learning_rate": 2.9742606113104816e-05,
      "loss": 9.991,
      "step": 3275
    },
    {
      "epoch": 0.4058903601039475,
      "grad_norm": 0.6302065849304199,
      "learning_rate": 2.971166934785299e-05,
      "loss": 9.9976,
      "step": 3280
    },
    {
      "epoch": 0.40650909540898406,
      "grad_norm": 0.6629530191421509,
      "learning_rate": 2.9680732582601166e-05,
      "loss": 9.991,
      "step": 3285
    },
    {
      "epoch": 0.40712783071402053,
      "grad_norm": 0.6216121912002563,
      "learning_rate": 2.964979581734934e-05,
      "loss": 10.0023,
      "step": 3290
    },
    {
      "epoch": 0.40774656601905707,
      "grad_norm": 0.665413498878479,
      "learning_rate": 2.9618859052097513e-05,
      "loss": 9.9921,
      "step": 3295
    },
    {
      "epoch": 0.40836530132409354,
      "grad_norm": 0.6451711654663086,
      "learning_rate": 2.9587922286845687e-05,
      "loss": 9.9946,
      "step": 3300
    },
    {
      "epoch": 0.4089840366291301,
      "grad_norm": 0.6311196684837341,
      "learning_rate": 2.9556985521593867e-05,
      "loss": 9.9884,
      "step": 3305
    },
    {
      "epoch": 0.40960277193416655,
      "grad_norm": 0.639893651008606,
      "learning_rate": 2.952604875634204e-05,
      "loss": 9.9833,
      "step": 3310
    },
    {
      "epoch": 0.4102215072392031,
      "grad_norm": 0.6288967132568359,
      "learning_rate": 2.9495111991090214e-05,
      "loss": 9.9911,
      "step": 3315
    },
    {
      "epoch": 0.41084024254423956,
      "grad_norm": 0.6633947491645813,
      "learning_rate": 2.9464175225838387e-05,
      "loss": 9.9922,
      "step": 3320
    },
    {
      "epoch": 0.4114589778492761,
      "grad_norm": 0.6744672060012817,
      "learning_rate": 2.9433238460586564e-05,
      "loss": 9.9876,
      "step": 3325
    },
    {
      "epoch": 0.4120777131543126,
      "grad_norm": 0.6573571562767029,
      "learning_rate": 2.9402301695334738e-05,
      "loss": 9.9881,
      "step": 3330
    },
    {
      "epoch": 0.4126964484593491,
      "grad_norm": 0.6318007111549377,
      "learning_rate": 2.937136493008291e-05,
      "loss": 10.0021,
      "step": 3335
    },
    {
      "epoch": 0.4133151837643856,
      "grad_norm": 0.6257010698318481,
      "learning_rate": 2.9340428164831085e-05,
      "loss": 9.9925,
      "step": 3340
    },
    {
      "epoch": 0.4139339190694221,
      "grad_norm": 0.6432370543479919,
      "learning_rate": 2.9309491399579265e-05,
      "loss": 9.9741,
      "step": 3345
    },
    {
      "epoch": 0.4145526543744586,
      "grad_norm": 0.6362488269805908,
      "learning_rate": 2.9278554634327438e-05,
      "loss": 9.9864,
      "step": 3350
    },
    {
      "epoch": 0.41517138967949513,
      "grad_norm": 0.6484906077384949,
      "learning_rate": 2.9247617869075612e-05,
      "loss": 9.9783,
      "step": 3355
    },
    {
      "epoch": 0.4157901249845316,
      "grad_norm": 0.6598631739616394,
      "learning_rate": 2.9216681103823785e-05,
      "loss": 9.9725,
      "step": 3360
    },
    {
      "epoch": 0.41640886028956814,
      "grad_norm": 0.6619321703910828,
      "learning_rate": 2.918574433857196e-05,
      "loss": 9.9816,
      "step": 3365
    },
    {
      "epoch": 0.4170275955946046,
      "grad_norm": 0.6570667624473572,
      "learning_rate": 2.9154807573320135e-05,
      "loss": 9.9754,
      "step": 3370
    },
    {
      "epoch": 0.41764633089964115,
      "grad_norm": 0.641963541507721,
      "learning_rate": 2.912387080806831e-05,
      "loss": 9.977,
      "step": 3375
    },
    {
      "epoch": 0.4182650662046776,
      "grad_norm": 0.6660823225975037,
      "learning_rate": 2.9092934042816482e-05,
      "loss": 9.9687,
      "step": 3380
    },
    {
      "epoch": 0.41888380150971416,
      "grad_norm": 0.6471954584121704,
      "learning_rate": 2.9061997277564656e-05,
      "loss": 9.9746,
      "step": 3385
    },
    {
      "epoch": 0.41950253681475064,
      "grad_norm": 0.6370503306388855,
      "learning_rate": 2.9031060512312836e-05,
      "loss": 9.9921,
      "step": 3390
    },
    {
      "epoch": 0.42012127211978717,
      "grad_norm": 0.6684100031852722,
      "learning_rate": 2.900012374706101e-05,
      "loss": 9.993,
      "step": 3395
    },
    {
      "epoch": 0.42074000742482365,
      "grad_norm": 0.6350740790367126,
      "learning_rate": 2.8969186981809183e-05,
      "loss": 9.9722,
      "step": 3400
    },
    {
      "epoch": 0.4213587427298602,
      "grad_norm": 0.6791630387306213,
      "learning_rate": 2.8938250216557356e-05,
      "loss": 9.9832,
      "step": 3405
    },
    {
      "epoch": 0.42197747803489666,
      "grad_norm": 0.6373003721237183,
      "learning_rate": 2.8907313451305533e-05,
      "loss": 9.9819,
      "step": 3410
    },
    {
      "epoch": 0.4225962133399332,
      "grad_norm": 0.6371186971664429,
      "learning_rate": 2.8876376686053707e-05,
      "loss": 9.9665,
      "step": 3415
    },
    {
      "epoch": 0.42321494864496967,
      "grad_norm": 0.6496630907058716,
      "learning_rate": 2.884543992080188e-05,
      "loss": 9.9725,
      "step": 3420
    },
    {
      "epoch": 0.4238336839500062,
      "grad_norm": 0.6582610607147217,
      "learning_rate": 2.8814503155550054e-05,
      "loss": 9.9602,
      "step": 3425
    },
    {
      "epoch": 0.4244524192550427,
      "grad_norm": 0.6774140000343323,
      "learning_rate": 2.8783566390298234e-05,
      "loss": 9.9665,
      "step": 3430
    },
    {
      "epoch": 0.4250711545600792,
      "grad_norm": 0.65788334608078,
      "learning_rate": 2.8752629625046407e-05,
      "loss": 9.969,
      "step": 3435
    },
    {
      "epoch": 0.4256898898651157,
      "grad_norm": 0.6790542602539062,
      "learning_rate": 2.872169285979458e-05,
      "loss": 9.9709,
      "step": 3440
    },
    {
      "epoch": 0.4263086251701522,
      "grad_norm": 0.6584944128990173,
      "learning_rate": 2.8690756094542754e-05,
      "loss": 9.9733,
      "step": 3445
    },
    {
      "epoch": 0.4269273604751887,
      "grad_norm": 0.6394343972206116,
      "learning_rate": 2.865981932929093e-05,
      "loss": 9.9608,
      "step": 3450
    },
    {
      "epoch": 0.42754609578022523,
      "grad_norm": 0.6327645778656006,
      "learning_rate": 2.8628882564039104e-05,
      "loss": 9.9622,
      "step": 3455
    },
    {
      "epoch": 0.4281648310852617,
      "grad_norm": 0.668998122215271,
      "learning_rate": 2.8597945798787278e-05,
      "loss": 9.9594,
      "step": 3460
    },
    {
      "epoch": 0.42878356639029824,
      "grad_norm": 0.6544957160949707,
      "learning_rate": 2.856700903353545e-05,
      "loss": 9.9664,
      "step": 3465
    },
    {
      "epoch": 0.4294023016953347,
      "grad_norm": 0.6687750220298767,
      "learning_rate": 2.853607226828363e-05,
      "loss": 9.962,
      "step": 3470
    },
    {
      "epoch": 0.43002103700037125,
      "grad_norm": 0.8728616237640381,
      "learning_rate": 2.8505135503031805e-05,
      "loss": 9.9698,
      "step": 3475
    },
    {
      "epoch": 0.43063977230540773,
      "grad_norm": 1.6627506017684937,
      "learning_rate": 2.847419873777998e-05,
      "loss": 9.9656,
      "step": 3480
    },
    {
      "epoch": 0.43125850761044426,
      "grad_norm": 0.6562338471412659,
      "learning_rate": 2.8443261972528152e-05,
      "loss": 9.9545,
      "step": 3485
    },
    {
      "epoch": 0.43187724291548074,
      "grad_norm": 0.6707453727722168,
      "learning_rate": 2.841232520727633e-05,
      "loss": 9.954,
      "step": 3490
    },
    {
      "epoch": 0.4324959782205173,
      "grad_norm": 0.6403122544288635,
      "learning_rate": 2.8381388442024502e-05,
      "loss": 9.9631,
      "step": 3495
    },
    {
      "epoch": 0.43311471352555375,
      "grad_norm": 0.6835969686508179,
      "learning_rate": 2.8350451676772676e-05,
      "loss": 9.956,
      "step": 3500
    },
    {
      "epoch": 0.4337334488305903,
      "grad_norm": 0.6739803552627563,
      "learning_rate": 2.831951491152085e-05,
      "loss": 9.956,
      "step": 3505
    },
    {
      "epoch": 0.43435218413562676,
      "grad_norm": 0.644069254398346,
      "learning_rate": 2.828857814626903e-05,
      "loss": 9.959,
      "step": 3510
    },
    {
      "epoch": 0.4349709194406633,
      "grad_norm": 0.6575890779495239,
      "learning_rate": 2.8257641381017203e-05,
      "loss": 9.961,
      "step": 3515
    },
    {
      "epoch": 0.4355896547456998,
      "grad_norm": 0.6655083298683167,
      "learning_rate": 2.8226704615765376e-05,
      "loss": 9.9692,
      "step": 3520
    },
    {
      "epoch": 0.4362083900507363,
      "grad_norm": 0.8069283366203308,
      "learning_rate": 2.819576785051355e-05,
      "loss": 9.96,
      "step": 3525
    },
    {
      "epoch": 0.4368271253557728,
      "grad_norm": 0.659879207611084,
      "learning_rate": 2.8164831085261727e-05,
      "loss": 9.9511,
      "step": 3530
    },
    {
      "epoch": 0.4374458606608093,
      "grad_norm": 0.6628430485725403,
      "learning_rate": 2.81338943200099e-05,
      "loss": 9.9554,
      "step": 3535
    },
    {
      "epoch": 0.4380645959658458,
      "grad_norm": 0.6513035297393799,
      "learning_rate": 2.8102957554758073e-05,
      "loss": 9.9622,
      "step": 3540
    },
    {
      "epoch": 0.4386833312708823,
      "grad_norm": 0.6515928506851196,
      "learning_rate": 2.8072020789506247e-05,
      "loss": 9.9498,
      "step": 3545
    },
    {
      "epoch": 0.4393020665759188,
      "grad_norm": 0.6557636260986328,
      "learning_rate": 2.8041084024254427e-05,
      "loss": 9.939,
      "step": 3550
    },
    {
      "epoch": 0.43992080188095534,
      "grad_norm": 0.6520994901657104,
      "learning_rate": 2.80101472590026e-05,
      "loss": 9.9354,
      "step": 3555
    },
    {
      "epoch": 0.4405395371859918,
      "grad_norm": 0.6429317593574524,
      "learning_rate": 2.7979210493750774e-05,
      "loss": 9.9463,
      "step": 3560
    },
    {
      "epoch": 0.44115827249102835,
      "grad_norm": 0.960143506526947,
      "learning_rate": 2.7948273728498947e-05,
      "loss": 9.9363,
      "step": 3565
    },
    {
      "epoch": 0.4417770077960648,
      "grad_norm": 0.6564841270446777,
      "learning_rate": 2.7917336963247128e-05,
      "loss": 9.9464,
      "step": 3570
    },
    {
      "epoch": 0.44239574310110136,
      "grad_norm": 0.6802830100059509,
      "learning_rate": 2.7886400197995298e-05,
      "loss": 9.9451,
      "step": 3575
    },
    {
      "epoch": 0.44301447840613783,
      "grad_norm": 0.7377637028694153,
      "learning_rate": 2.785546343274347e-05,
      "loss": 9.9368,
      "step": 3580
    },
    {
      "epoch": 0.44363321371117437,
      "grad_norm": 0.6738294959068298,
      "learning_rate": 2.7824526667491645e-05,
      "loss": 9.9417,
      "step": 3585
    },
    {
      "epoch": 0.44425194901621085,
      "grad_norm": 0.68399578332901,
      "learning_rate": 2.7793589902239825e-05,
      "loss": 9.9336,
      "step": 3590
    },
    {
      "epoch": 0.4448706843212474,
      "grad_norm": 0.662196159362793,
      "learning_rate": 2.7762653136987998e-05,
      "loss": 9.9369,
      "step": 3595
    },
    {
      "epoch": 0.44548941962628386,
      "grad_norm": 0.6617550849914551,
      "learning_rate": 2.7731716371736172e-05,
      "loss": 9.9363,
      "step": 3600
    },
    {
      "epoch": 0.4461081549313204,
      "grad_norm": 0.6784976124763489,
      "learning_rate": 2.7700779606484345e-05,
      "loss": 9.9317,
      "step": 3605
    },
    {
      "epoch": 0.44672689023635687,
      "grad_norm": 0.6583097577095032,
      "learning_rate": 2.7669842841232525e-05,
      "loss": 9.9417,
      "step": 3610
    },
    {
      "epoch": 0.4473456255413934,
      "grad_norm": 0.6868885159492493,
      "learning_rate": 2.76389060759807e-05,
      "loss": 9.9499,
      "step": 3615
    },
    {
      "epoch": 0.4479643608464299,
      "grad_norm": 0.67156583070755,
      "learning_rate": 2.760796931072887e-05,
      "loss": 9.9301,
      "step": 3620
    },
    {
      "epoch": 0.4485830961514664,
      "grad_norm": 0.6738335490226746,
      "learning_rate": 2.7577032545477042e-05,
      "loss": 9.9427,
      "step": 3625
    },
    {
      "epoch": 0.4492018314565029,
      "grad_norm": 0.6897249221801758,
      "learning_rate": 2.7546095780225223e-05,
      "loss": 9.9298,
      "step": 3630
    },
    {
      "epoch": 0.4498205667615394,
      "grad_norm": 0.6853035092353821,
      "learning_rate": 2.7515159014973396e-05,
      "loss": 9.9351,
      "step": 3635
    },
    {
      "epoch": 0.4504393020665759,
      "grad_norm": 0.6573383212089539,
      "learning_rate": 2.748422224972157e-05,
      "loss": 9.9347,
      "step": 3640
    },
    {
      "epoch": 0.45105803737161243,
      "grad_norm": 0.6808295249938965,
      "learning_rate": 2.7453285484469743e-05,
      "loss": 9.9313,
      "step": 3645
    },
    {
      "epoch": 0.4516767726766489,
      "grad_norm": 0.6618577241897583,
      "learning_rate": 2.7422348719217923e-05,
      "loss": 9.9222,
      "step": 3650
    },
    {
      "epoch": 0.45229550798168544,
      "grad_norm": 0.6686483025550842,
      "learning_rate": 2.7391411953966097e-05,
      "loss": 9.9281,
      "step": 3655
    },
    {
      "epoch": 0.4529142432867219,
      "grad_norm": 0.6559252738952637,
      "learning_rate": 2.736047518871427e-05,
      "loss": 9.9281,
      "step": 3660
    },
    {
      "epoch": 0.45353297859175845,
      "grad_norm": 0.6977870464324951,
      "learning_rate": 2.7329538423462444e-05,
      "loss": 9.9228,
      "step": 3665
    },
    {
      "epoch": 0.45415171389679493,
      "grad_norm": 0.6697185635566711,
      "learning_rate": 2.729860165821062e-05,
      "loss": 9.9245,
      "step": 3670
    },
    {
      "epoch": 0.45477044920183146,
      "grad_norm": 0.6664271950721741,
      "learning_rate": 2.7267664892958794e-05,
      "loss": 9.9181,
      "step": 3675
    },
    {
      "epoch": 0.45538918450686794,
      "grad_norm": 0.6755523681640625,
      "learning_rate": 2.7236728127706967e-05,
      "loss": 9.9274,
      "step": 3680
    },
    {
      "epoch": 0.45600791981190447,
      "grad_norm": 0.6702771186828613,
      "learning_rate": 2.720579136245514e-05,
      "loss": 9.9379,
      "step": 3685
    },
    {
      "epoch": 0.45662665511694095,
      "grad_norm": 0.7076529264450073,
      "learning_rate": 2.717485459720332e-05,
      "loss": 9.929,
      "step": 3690
    },
    {
      "epoch": 0.4572453904219775,
      "grad_norm": 0.6989125609397888,
      "learning_rate": 2.7143917831951494e-05,
      "loss": 9.92,
      "step": 3695
    },
    {
      "epoch": 0.45786412572701396,
      "grad_norm": 0.6804359555244446,
      "learning_rate": 2.7112981066699668e-05,
      "loss": 9.9218,
      "step": 3700
    },
    {
      "epoch": 0.4584828610320505,
      "grad_norm": 0.684853732585907,
      "learning_rate": 2.708204430144784e-05,
      "loss": 9.9222,
      "step": 3705
    },
    {
      "epoch": 0.45910159633708697,
      "grad_norm": 0.6437480449676514,
      "learning_rate": 2.7051107536196018e-05,
      "loss": 9.9256,
      "step": 3710
    },
    {
      "epoch": 0.4597203316421235,
      "grad_norm": 0.654154896736145,
      "learning_rate": 2.702017077094419e-05,
      "loss": 9.9254,
      "step": 3715
    },
    {
      "epoch": 0.46033906694716,
      "grad_norm": 0.6483764052391052,
      "learning_rate": 2.6989234005692365e-05,
      "loss": 9.922,
      "step": 3720
    },
    {
      "epoch": 0.4609578022521965,
      "grad_norm": 0.6960828900337219,
      "learning_rate": 2.695829724044054e-05,
      "loss": 9.9207,
      "step": 3725
    },
    {
      "epoch": 0.461576537557233,
      "grad_norm": 0.680539071559906,
      "learning_rate": 2.692736047518872e-05,
      "loss": 9.9297,
      "step": 3730
    },
    {
      "epoch": 0.4621952728622695,
      "grad_norm": 0.6649044156074524,
      "learning_rate": 2.6896423709936892e-05,
      "loss": 9.9319,
      "step": 3735
    },
    {
      "epoch": 0.462814008167306,
      "grad_norm": 0.6719271540641785,
      "learning_rate": 2.6865486944685066e-05,
      "loss": 9.9254,
      "step": 3740
    },
    {
      "epoch": 0.46343274347234253,
      "grad_norm": 0.6866425275802612,
      "learning_rate": 2.683455017943324e-05,
      "loss": 9.9099,
      "step": 3745
    },
    {
      "epoch": 0.464051478777379,
      "grad_norm": 0.6738466620445251,
      "learning_rate": 2.6803613414181416e-05,
      "loss": 9.9194,
      "step": 3750
    },
    {
      "epoch": 0.46467021408241554,
      "grad_norm": 0.6652119159698486,
      "learning_rate": 2.677267664892959e-05,
      "loss": 9.9283,
      "step": 3755
    },
    {
      "epoch": 0.465288949387452,
      "grad_norm": 0.6842427849769592,
      "learning_rate": 2.6741739883677763e-05,
      "loss": 9.9168,
      "step": 3760
    },
    {
      "epoch": 0.46590768469248856,
      "grad_norm": 0.6886729001998901,
      "learning_rate": 2.6710803118425936e-05,
      "loss": 9.8982,
      "step": 3765
    },
    {
      "epoch": 0.46652641999752503,
      "grad_norm": 0.6708517670631409,
      "learning_rate": 2.6679866353174116e-05,
      "loss": 9.9015,
      "step": 3770
    },
    {
      "epoch": 0.46714515530256157,
      "grad_norm": 0.7130897641181946,
      "learning_rate": 2.664892958792229e-05,
      "loss": 9.9074,
      "step": 3775
    },
    {
      "epoch": 0.46776389060759804,
      "grad_norm": 0.6982098817825317,
      "learning_rate": 2.6617992822670463e-05,
      "loss": 9.9039,
      "step": 3780
    },
    {
      "epoch": 0.4683826259126346,
      "grad_norm": 0.6681919693946838,
      "learning_rate": 2.6587056057418637e-05,
      "loss": 9.9258,
      "step": 3785
    },
    {
      "epoch": 0.46900136121767105,
      "grad_norm": 1.391956090927124,
      "learning_rate": 2.6556119292166814e-05,
      "loss": 9.92,
      "step": 3790
    },
    {
      "epoch": 0.4696200965227076,
      "grad_norm": 0.6694711446762085,
      "learning_rate": 2.6525182526914987e-05,
      "loss": 9.9131,
      "step": 3795
    },
    {
      "epoch": 0.47023883182774406,
      "grad_norm": 0.680496096611023,
      "learning_rate": 2.649424576166316e-05,
      "loss": 9.9017,
      "step": 3800
    },
    {
      "epoch": 0.4708575671327806,
      "grad_norm": 0.6855018734931946,
      "learning_rate": 2.6463308996411334e-05,
      "loss": 9.9084,
      "step": 3805
    },
    {
      "epoch": 0.4714763024378171,
      "grad_norm": 0.6959882378578186,
      "learning_rate": 2.6432372231159514e-05,
      "loss": 9.9093,
      "step": 3810
    },
    {
      "epoch": 0.4720950377428536,
      "grad_norm": 0.6895874738693237,
      "learning_rate": 2.6401435465907688e-05,
      "loss": 9.8863,
      "step": 3815
    },
    {
      "epoch": 0.47271377304789014,
      "grad_norm": 0.6811323165893555,
      "learning_rate": 2.637049870065586e-05,
      "loss": 9.9036,
      "step": 3820
    },
    {
      "epoch": 0.4733325083529266,
      "grad_norm": 0.6853577494621277,
      "learning_rate": 2.6339561935404035e-05,
      "loss": 9.8983,
      "step": 3825
    },
    {
      "epoch": 0.47395124365796315,
      "grad_norm": 0.7002264261245728,
      "learning_rate": 2.630862517015221e-05,
      "loss": 9.8959,
      "step": 3830
    },
    {
      "epoch": 0.47456997896299963,
      "grad_norm": 0.6891324520111084,
      "learning_rate": 2.6277688404900385e-05,
      "loss": 9.9114,
      "step": 3835
    },
    {
      "epoch": 0.47518871426803616,
      "grad_norm": 0.6960985660552979,
      "learning_rate": 2.624675163964856e-05,
      "loss": 9.8897,
      "step": 3840
    },
    {
      "epoch": 0.47580744957307264,
      "grad_norm": 0.6821807622909546,
      "learning_rate": 2.6215814874396732e-05,
      "loss": 9.9054,
      "step": 3845
    },
    {
      "epoch": 0.47642618487810917,
      "grad_norm": 0.6971168518066406,
      "learning_rate": 2.6184878109144912e-05,
      "loss": 9.8825,
      "step": 3850
    },
    {
      "epoch": 0.47704492018314565,
      "grad_norm": 0.7086196541786194,
      "learning_rate": 2.6153941343893085e-05,
      "loss": 9.8975,
      "step": 3855
    },
    {
      "epoch": 0.4776636554881822,
      "grad_norm": 0.6648578643798828,
      "learning_rate": 2.612300457864126e-05,
      "loss": 9.9057,
      "step": 3860
    },
    {
      "epoch": 0.47828239079321866,
      "grad_norm": 0.706583559513092,
      "learning_rate": 2.6092067813389432e-05,
      "loss": 9.8902,
      "step": 3865
    },
    {
      "epoch": 0.4789011260982552,
      "grad_norm": 0.6930044889450073,
      "learning_rate": 2.606113104813761e-05,
      "loss": 9.892,
      "step": 3870
    },
    {
      "epoch": 0.47951986140329167,
      "grad_norm": 0.6976168155670166,
      "learning_rate": 2.6030194282885783e-05,
      "loss": 9.8992,
      "step": 3875
    },
    {
      "epoch": 0.4801385967083282,
      "grad_norm": 0.7021157145500183,
      "learning_rate": 2.5999257517633956e-05,
      "loss": 9.8938,
      "step": 3880
    },
    {
      "epoch": 0.4807573320133647,
      "grad_norm": 0.6784963607788086,
      "learning_rate": 2.596832075238213e-05,
      "loss": 9.8948,
      "step": 3885
    },
    {
      "epoch": 0.4813760673184012,
      "grad_norm": 0.66207355260849,
      "learning_rate": 2.593738398713031e-05,
      "loss": 9.8998,
      "step": 3890
    },
    {
      "epoch": 0.4819948026234377,
      "grad_norm": 0.6790847778320312,
      "learning_rate": 2.5906447221878483e-05,
      "loss": 9.8941,
      "step": 3895
    },
    {
      "epoch": 0.4826135379284742,
      "grad_norm": 0.6779101490974426,
      "learning_rate": 2.5875510456626657e-05,
      "loss": 9.9056,
      "step": 3900
    },
    {
      "epoch": 0.4832322732335107,
      "grad_norm": 0.700685977935791,
      "learning_rate": 2.584457369137483e-05,
      "loss": 9.8932,
      "step": 3905
    },
    {
      "epoch": 0.48385100853854723,
      "grad_norm": 0.6908859014511108,
      "learning_rate": 2.5813636926123007e-05,
      "loss": 9.8848,
      "step": 3910
    },
    {
      "epoch": 0.4844697438435837,
      "grad_norm": 0.7037233114242554,
      "learning_rate": 2.578270016087118e-05,
      "loss": 9.8838,
      "step": 3915
    },
    {
      "epoch": 0.48508847914862024,
      "grad_norm": 0.6773281097412109,
      "learning_rate": 2.5751763395619354e-05,
      "loss": 9.8949,
      "step": 3920
    },
    {
      "epoch": 0.4857072144536567,
      "grad_norm": 0.7033843994140625,
      "learning_rate": 2.5720826630367527e-05,
      "loss": 9.8874,
      "step": 3925
    },
    {
      "epoch": 0.48632594975869325,
      "grad_norm": 0.7064381837844849,
      "learning_rate": 2.5689889865115708e-05,
      "loss": 9.8801,
      "step": 3930
    },
    {
      "epoch": 0.48694468506372973,
      "grad_norm": 0.7002949714660645,
      "learning_rate": 2.565895309986388e-05,
      "loss": 9.878,
      "step": 3935
    },
    {
      "epoch": 0.48756342036876626,
      "grad_norm": 2.450899839401245,
      "learning_rate": 2.5628016334612054e-05,
      "loss": 9.8951,
      "step": 3940
    },
    {
      "epoch": 0.48818215567380274,
      "grad_norm": 0.7054110169410706,
      "learning_rate": 2.5597079569360228e-05,
      "loss": 9.8877,
      "step": 3945
    },
    {
      "epoch": 0.4888008909788393,
      "grad_norm": 0.6914075016975403,
      "learning_rate": 2.5566142804108405e-05,
      "loss": 9.8894,
      "step": 3950
    },
    {
      "epoch": 0.48941962628387575,
      "grad_norm": 0.6885808706283569,
      "learning_rate": 2.5535206038856578e-05,
      "loss": 9.88,
      "step": 3955
    },
    {
      "epoch": 0.4900383615889123,
      "grad_norm": 0.7083455920219421,
      "learning_rate": 2.550426927360475e-05,
      "loss": 9.9113,
      "step": 3960
    },
    {
      "epoch": 0.49065709689394876,
      "grad_norm": 0.6692916750907898,
      "learning_rate": 2.5473332508352925e-05,
      "loss": 9.8987,
      "step": 3965
    },
    {
      "epoch": 0.4912758321989853,
      "grad_norm": 0.7178680896759033,
      "learning_rate": 2.5442395743101105e-05,
      "loss": 9.8975,
      "step": 3970
    },
    {
      "epoch": 0.4918945675040218,
      "grad_norm": 0.6926959753036499,
      "learning_rate": 2.541145897784928e-05,
      "loss": 9.8791,
      "step": 3975
    },
    {
      "epoch": 0.4925133028090583,
      "grad_norm": 0.7036349773406982,
      "learning_rate": 2.5380522212597452e-05,
      "loss": 9.8752,
      "step": 3980
    },
    {
      "epoch": 0.4931320381140948,
      "grad_norm": 0.6984120011329651,
      "learning_rate": 2.5349585447345626e-05,
      "loss": 9.869,
      "step": 3985
    },
    {
      "epoch": 0.4937507734191313,
      "grad_norm": 0.702412486076355,
      "learning_rate": 2.5318648682093802e-05,
      "loss": 9.8799,
      "step": 3990
    },
    {
      "epoch": 0.4943695087241678,
      "grad_norm": 0.6928656697273254,
      "learning_rate": 2.5287711916841976e-05,
      "loss": 9.8772,
      "step": 3995
    },
    {
      "epoch": 0.4949882440292043,
      "grad_norm": 0.6940387487411499,
      "learning_rate": 2.525677515159015e-05,
      "loss": 9.8831,
      "step": 4000
    },
    {
      "epoch": 0.4956069793342408,
      "grad_norm": 0.6989834904670715,
      "learning_rate": 2.5225838386338323e-05,
      "loss": 9.8873,
      "step": 4005
    },
    {
      "epoch": 0.49622571463927734,
      "grad_norm": 0.7127276659011841,
      "learning_rate": 2.5194901621086503e-05,
      "loss": 9.8691,
      "step": 4010
    },
    {
      "epoch": 0.4968444499443138,
      "grad_norm": 0.704845666885376,
      "learning_rate": 2.5163964855834676e-05,
      "loss": 9.8831,
      "step": 4015
    },
    {
      "epoch": 0.49746318524935035,
      "grad_norm": 0.7470578551292419,
      "learning_rate": 2.513302809058285e-05,
      "loss": 9.8904,
      "step": 4020
    },
    {
      "epoch": 0.4980819205543868,
      "grad_norm": 0.7226484417915344,
      "learning_rate": 2.5102091325331023e-05,
      "loss": 9.868,
      "step": 4025
    },
    {
      "epoch": 0.49870065585942336,
      "grad_norm": 0.7022948861122131,
      "learning_rate": 2.5071154560079204e-05,
      "loss": 9.8744,
      "step": 4030
    },
    {
      "epoch": 0.49931939116445984,
      "grad_norm": 0.7102589011192322,
      "learning_rate": 2.5040217794827374e-05,
      "loss": 9.864,
      "step": 4035
    },
    {
      "epoch": 0.49993812646949637,
      "grad_norm": 0.7115795612335205,
      "learning_rate": 2.5009281029575547e-05,
      "loss": 9.8696,
      "step": 4040
    },
    {
      "epoch": 0.5005568617745328,
      "grad_norm": 0.7343975901603699,
      "learning_rate": 2.4978344264323724e-05,
      "loss": 9.8576,
      "step": 4045
    },
    {
      "epoch": 0.5011755970795694,
      "grad_norm": 0.7011085152626038,
      "learning_rate": 2.4947407499071897e-05,
      "loss": 9.8689,
      "step": 4050
    },
    {
      "epoch": 0.5017943323846059,
      "grad_norm": 0.6964554190635681,
      "learning_rate": 2.4916470733820074e-05,
      "loss": 9.8664,
      "step": 4055
    },
    {
      "epoch": 0.5024130676896423,
      "grad_norm": 0.7001059055328369,
      "learning_rate": 2.4885533968568248e-05,
      "loss": 9.8757,
      "step": 4060
    },
    {
      "epoch": 0.5030318029946789,
      "grad_norm": 0.7300817370414734,
      "learning_rate": 2.4854597203316425e-05,
      "loss": 9.8615,
      "step": 4065
    },
    {
      "epoch": 0.5036505382997154,
      "grad_norm": 0.7038024663925171,
      "learning_rate": 2.4823660438064598e-05,
      "loss": 9.8604,
      "step": 4070
    },
    {
      "epoch": 0.5042692736047519,
      "grad_norm": 0.7174104452133179,
      "learning_rate": 2.4792723672812775e-05,
      "loss": 9.8652,
      "step": 4075
    },
    {
      "epoch": 0.5048880089097884,
      "grad_norm": 0.6982371211051941,
      "learning_rate": 2.4761786907560945e-05,
      "loss": 9.8719,
      "step": 4080
    },
    {
      "epoch": 0.5055067442148249,
      "grad_norm": 0.7086823582649231,
      "learning_rate": 2.4730850142309122e-05,
      "loss": 9.8589,
      "step": 4085
    },
    {
      "epoch": 0.5061254795198614,
      "grad_norm": 0.6762822866439819,
      "learning_rate": 2.4699913377057295e-05,
      "loss": 9.8769,
      "step": 4090
    },
    {
      "epoch": 0.506744214824898,
      "grad_norm": 0.6983185410499573,
      "learning_rate": 2.4668976611805472e-05,
      "loss": 9.8578,
      "step": 4095
    },
    {
      "epoch": 0.5073629501299344,
      "grad_norm": 1.2803332805633545,
      "learning_rate": 2.4638039846553645e-05,
      "loss": 9.8649,
      "step": 4100
    },
    {
      "epoch": 0.5079816854349709,
      "grad_norm": 0.7410219311714172,
      "learning_rate": 2.4607103081301822e-05,
      "loss": 9.8576,
      "step": 4105
    },
    {
      "epoch": 0.5086004207400074,
      "grad_norm": 0.7122843861579895,
      "learning_rate": 2.4576166316049996e-05,
      "loss": 9.8784,
      "step": 4110
    },
    {
      "epoch": 0.509219156045044,
      "grad_norm": 0.7233502268791199,
      "learning_rate": 2.4545229550798173e-05,
      "loss": 9.8583,
      "step": 4115
    },
    {
      "epoch": 0.5098378913500804,
      "grad_norm": 0.711049497127533,
      "learning_rate": 2.4514292785546346e-05,
      "loss": 9.8553,
      "step": 4120
    },
    {
      "epoch": 0.5104566266551169,
      "grad_norm": 0.7127248644828796,
      "learning_rate": 2.448335602029452e-05,
      "loss": 9.8628,
      "step": 4125
    },
    {
      "epoch": 0.5110753619601535,
      "grad_norm": 0.7064740061759949,
      "learning_rate": 2.4452419255042693e-05,
      "loss": 9.8515,
      "step": 4130
    },
    {
      "epoch": 0.51169409726519,
      "grad_norm": 0.6883571743965149,
      "learning_rate": 2.4421482489790866e-05,
      "loss": 9.8797,
      "step": 4135
    },
    {
      "epoch": 0.5123128325702264,
      "grad_norm": 0.688309371471405,
      "learning_rate": 2.4390545724539043e-05,
      "loss": 9.8676,
      "step": 4140
    },
    {
      "epoch": 0.512931567875263,
      "grad_norm": 0.7004178762435913,
      "learning_rate": 2.4359608959287217e-05,
      "loss": 9.8522,
      "step": 4145
    },
    {
      "epoch": 0.5135503031802995,
      "grad_norm": 0.6998472213745117,
      "learning_rate": 2.4328672194035394e-05,
      "loss": 9.8546,
      "step": 4150
    },
    {
      "epoch": 0.514169038485336,
      "grad_norm": 0.7199821472167969,
      "learning_rate": 2.4297735428783567e-05,
      "loss": 9.8515,
      "step": 4155
    },
    {
      "epoch": 0.5147877737903724,
      "grad_norm": 0.7262988090515137,
      "learning_rate": 2.4266798663531744e-05,
      "loss": 9.8569,
      "step": 4160
    },
    {
      "epoch": 0.515406509095409,
      "grad_norm": 0.7360613346099854,
      "learning_rate": 2.4235861898279917e-05,
      "loss": 9.8497,
      "step": 4165
    },
    {
      "epoch": 0.5160252444004455,
      "grad_norm": 0.6951295733451843,
      "learning_rate": 2.420492513302809e-05,
      "loss": 9.8531,
      "step": 4170
    },
    {
      "epoch": 0.516643979705482,
      "grad_norm": 0.7378495931625366,
      "learning_rate": 2.4173988367776264e-05,
      "loss": 9.8466,
      "step": 4175
    },
    {
      "epoch": 0.5172627150105185,
      "grad_norm": 0.6959829926490784,
      "learning_rate": 2.414305160252444e-05,
      "loss": 9.8598,
      "step": 4180
    },
    {
      "epoch": 0.517881450315555,
      "grad_norm": 0.7174587845802307,
      "learning_rate": 2.4112114837272614e-05,
      "loss": 9.8542,
      "step": 4185
    },
    {
      "epoch": 0.5185001856205915,
      "grad_norm": 0.6967417597770691,
      "learning_rate": 2.408117807202079e-05,
      "loss": 9.8432,
      "step": 4190
    },
    {
      "epoch": 0.5191189209256281,
      "grad_norm": 0.7171748876571655,
      "learning_rate": 2.4050241306768965e-05,
      "loss": 9.8508,
      "step": 4195
    },
    {
      "epoch": 0.5197376562306645,
      "grad_norm": 0.7423428893089294,
      "learning_rate": 2.401930454151714e-05,
      "loss": 9.8408,
      "step": 4200
    },
    {
      "epoch": 0.520356391535701,
      "grad_norm": 0.7052573561668396,
      "learning_rate": 2.3988367776265315e-05,
      "loss": 9.8293,
      "step": 4205
    },
    {
      "epoch": 0.5209751268407375,
      "grad_norm": 0.7183065414428711,
      "learning_rate": 2.395743101101349e-05,
      "loss": 9.8328,
      "step": 4210
    },
    {
      "epoch": 0.5215938621457741,
      "grad_norm": 0.7204165458679199,
      "learning_rate": 2.3926494245761662e-05,
      "loss": 9.8316,
      "step": 4215
    },
    {
      "epoch": 0.5222125974508105,
      "grad_norm": 0.7336531281471252,
      "learning_rate": 2.389555748050984e-05,
      "loss": 9.8377,
      "step": 4220
    },
    {
      "epoch": 0.522831332755847,
      "grad_norm": 1.4958330392837524,
      "learning_rate": 2.3864620715258012e-05,
      "loss": 9.8553,
      "step": 4225
    },
    {
      "epoch": 0.5234500680608836,
      "grad_norm": 0.7121859788894653,
      "learning_rate": 2.383368395000619e-05,
      "loss": 9.8352,
      "step": 4230
    },
    {
      "epoch": 0.5240688033659201,
      "grad_norm": 0.7074877023696899,
      "learning_rate": 2.3802747184754362e-05,
      "loss": 9.841,
      "step": 4235
    },
    {
      "epoch": 0.5246875386709565,
      "grad_norm": 0.6994908452033997,
      "learning_rate": 2.377181041950254e-05,
      "loss": 9.8441,
      "step": 4240
    },
    {
      "epoch": 0.525306273975993,
      "grad_norm": 0.7496887445449829,
      "learning_rate": 2.3740873654250713e-05,
      "loss": 9.8322,
      "step": 4245
    },
    {
      "epoch": 0.5259250092810296,
      "grad_norm": 0.7165661454200745,
      "learning_rate": 2.3709936888998886e-05,
      "loss": 9.847,
      "step": 4250
    },
    {
      "epoch": 0.5265437445860661,
      "grad_norm": 2.695955753326416,
      "learning_rate": 2.367900012374706e-05,
      "loss": 9.8501,
      "step": 4255
    },
    {
      "epoch": 0.5271624798911025,
      "grad_norm": 0.7323252558708191,
      "learning_rate": 2.3648063358495237e-05,
      "loss": 9.8248,
      "step": 4260
    },
    {
      "epoch": 0.5277812151961391,
      "grad_norm": 0.7309625744819641,
      "learning_rate": 2.361712659324341e-05,
      "loss": 9.8259,
      "step": 4265
    },
    {
      "epoch": 0.5283999505011756,
      "grad_norm": 0.7206649780273438,
      "learning_rate": 2.3586189827991587e-05,
      "loss": 9.8251,
      "step": 4270
    },
    {
      "epoch": 0.5290186858062121,
      "grad_norm": 0.7309756278991699,
      "learning_rate": 2.355525306273976e-05,
      "loss": 9.8268,
      "step": 4275
    },
    {
      "epoch": 0.5296374211112486,
      "grad_norm": 0.7199364304542542,
      "learning_rate": 2.3524316297487937e-05,
      "loss": 9.8411,
      "step": 4280
    },
    {
      "epoch": 0.5302561564162851,
      "grad_norm": 0.7252469062805176,
      "learning_rate": 2.349337953223611e-05,
      "loss": 9.83,
      "step": 4285
    },
    {
      "epoch": 0.5308748917213216,
      "grad_norm": 0.7222177982330322,
      "learning_rate": 2.3462442766984284e-05,
      "loss": 9.8295,
      "step": 4290
    },
    {
      "epoch": 0.5314936270263582,
      "grad_norm": 0.7039614915847778,
      "learning_rate": 2.3431506001732457e-05,
      "loss": 9.8374,
      "step": 4295
    },
    {
      "epoch": 0.5321123623313946,
      "grad_norm": 0.7327678799629211,
      "learning_rate": 2.3400569236480634e-05,
      "loss": 9.8295,
      "step": 4300
    },
    {
      "epoch": 0.5327310976364311,
      "grad_norm": 0.7188330292701721,
      "learning_rate": 2.3369632471228808e-05,
      "loss": 9.8414,
      "step": 4305
    },
    {
      "epoch": 0.5333498329414676,
      "grad_norm": 0.723609983921051,
      "learning_rate": 2.3338695705976985e-05,
      "loss": 9.8399,
      "step": 4310
    },
    {
      "epoch": 0.5339685682465042,
      "grad_norm": 0.7087708711624146,
      "learning_rate": 2.3307758940725158e-05,
      "loss": 9.8111,
      "step": 4315
    },
    {
      "epoch": 0.5345873035515406,
      "grad_norm": 0.7187711000442505,
      "learning_rate": 2.3276822175473335e-05,
      "loss": 9.8285,
      "step": 4320
    },
    {
      "epoch": 0.5352060388565771,
      "grad_norm": 0.7298442125320435,
      "learning_rate": 2.3245885410221508e-05,
      "loss": 9.8246,
      "step": 4325
    },
    {
      "epoch": 0.5358247741616137,
      "grad_norm": 0.7655134797096252,
      "learning_rate": 2.3214948644969685e-05,
      "loss": 9.8251,
      "step": 4330
    },
    {
      "epoch": 0.5364435094666502,
      "grad_norm": 0.739786684513092,
      "learning_rate": 2.3184011879717855e-05,
      "loss": 9.8132,
      "step": 4335
    },
    {
      "epoch": 0.5370622447716866,
      "grad_norm": 0.7405941486358643,
      "learning_rate": 2.3153075114466032e-05,
      "loss": 9.8249,
      "step": 4340
    },
    {
      "epoch": 0.5376809800767232,
      "grad_norm": 0.7129958271980286,
      "learning_rate": 2.3122138349214205e-05,
      "loss": 9.8315,
      "step": 4345
    },
    {
      "epoch": 0.5382997153817597,
      "grad_norm": 0.8844628930091858,
      "learning_rate": 2.3091201583962382e-05,
      "loss": 9.8431,
      "step": 4350
    },
    {
      "epoch": 0.5389184506867962,
      "grad_norm": 0.7367368340492249,
      "learning_rate": 2.3060264818710556e-05,
      "loss": 9.8128,
      "step": 4355
    },
    {
      "epoch": 0.5395371859918326,
      "grad_norm": 0.7289445996284485,
      "learning_rate": 2.3029328053458733e-05,
      "loss": 9.8171,
      "step": 4360
    },
    {
      "epoch": 0.5401559212968692,
      "grad_norm": 0.7110461592674255,
      "learning_rate": 2.2998391288206906e-05,
      "loss": 9.8284,
      "step": 4365
    },
    {
      "epoch": 0.5407746566019057,
      "grad_norm": 0.7572498321533203,
      "learning_rate": 2.2967454522955083e-05,
      "loss": 9.8234,
      "step": 4370
    },
    {
      "epoch": 0.5413933919069422,
      "grad_norm": 0.7507062554359436,
      "learning_rate": 2.2936517757703256e-05,
      "loss": 9.8122,
      "step": 4375
    },
    {
      "epoch": 0.5420121272119787,
      "grad_norm": 0.7334984540939331,
      "learning_rate": 2.290558099245143e-05,
      "loss": 9.8133,
      "step": 4380
    },
    {
      "epoch": 0.5426308625170152,
      "grad_norm": 0.7361506819725037,
      "learning_rate": 2.2874644227199603e-05,
      "loss": 9.8173,
      "step": 4385
    },
    {
      "epoch": 0.5432495978220517,
      "grad_norm": 0.7283931374549866,
      "learning_rate": 2.284370746194778e-05,
      "loss": 9.8073,
      "step": 4390
    },
    {
      "epoch": 0.5438683331270883,
      "grad_norm": 0.7375301718711853,
      "learning_rate": 2.2812770696695954e-05,
      "loss": 9.8129,
      "step": 4395
    },
    {
      "epoch": 0.5444870684321247,
      "grad_norm": 0.7459957003593445,
      "learning_rate": 2.278183393144413e-05,
      "loss": 9.8073,
      "step": 4400
    },
    {
      "epoch": 0.5451058037371612,
      "grad_norm": 0.7392629981040955,
      "learning_rate": 2.2750897166192304e-05,
      "loss": 9.8186,
      "step": 4405
    },
    {
      "epoch": 0.5457245390421978,
      "grad_norm": 0.7410823702812195,
      "learning_rate": 2.271996040094048e-05,
      "loss": 9.8059,
      "step": 4410
    },
    {
      "epoch": 0.5463432743472343,
      "grad_norm": 0.7217216491699219,
      "learning_rate": 2.2689023635688654e-05,
      "loss": 9.812,
      "step": 4415
    },
    {
      "epoch": 0.5469620096522707,
      "grad_norm": 0.7227736711502075,
      "learning_rate": 2.2658086870436828e-05,
      "loss": 9.8209,
      "step": 4420
    },
    {
      "epoch": 0.5475807449573072,
      "grad_norm": 0.7564347386360168,
      "learning_rate": 2.2627150105185e-05,
      "loss": 9.8248,
      "step": 4425
    },
    {
      "epoch": 0.5481994802623438,
      "grad_norm": 0.737159252166748,
      "learning_rate": 2.2596213339933178e-05,
      "loss": 9.8213,
      "step": 4430
    },
    {
      "epoch": 0.5488182155673803,
      "grad_norm": 0.7465341091156006,
      "learning_rate": 2.256527657468135e-05,
      "loss": 9.8098,
      "step": 4435
    },
    {
      "epoch": 0.5494369508724167,
      "grad_norm": 0.7387682199478149,
      "learning_rate": 2.2534339809429528e-05,
      "loss": 9.8031,
      "step": 4440
    },
    {
      "epoch": 0.5500556861774533,
      "grad_norm": 0.7524792551994324,
      "learning_rate": 2.25034030441777e-05,
      "loss": 9.8041,
      "step": 4445
    },
    {
      "epoch": 0.5506744214824898,
      "grad_norm": 0.7380833625793457,
      "learning_rate": 2.247246627892588e-05,
      "loss": 9.8069,
      "step": 4450
    },
    {
      "epoch": 0.5512931567875263,
      "grad_norm": 0.7275206446647644,
      "learning_rate": 2.2441529513674052e-05,
      "loss": 9.8101,
      "step": 4455
    },
    {
      "epoch": 0.5519118920925628,
      "grad_norm": 0.7236595153808594,
      "learning_rate": 2.2410592748422225e-05,
      "loss": 9.8151,
      "step": 4460
    },
    {
      "epoch": 0.5525306273975993,
      "grad_norm": 0.7200035452842712,
      "learning_rate": 2.23796559831704e-05,
      "loss": 9.8143,
      "step": 4465
    },
    {
      "epoch": 0.5531493627026358,
      "grad_norm": 0.7257844805717468,
      "learning_rate": 2.2348719217918576e-05,
      "loss": 9.8053,
      "step": 4470
    },
    {
      "epoch": 0.5537680980076723,
      "grad_norm": 0.7370189428329468,
      "learning_rate": 2.231778245266675e-05,
      "loss": 9.8046,
      "step": 4475
    },
    {
      "epoch": 0.5543868333127088,
      "grad_norm": 0.7104122042655945,
      "learning_rate": 2.2286845687414926e-05,
      "loss": 9.8209,
      "step": 4480
    },
    {
      "epoch": 0.5550055686177453,
      "grad_norm": 0.7331891059875488,
      "learning_rate": 2.22559089221631e-05,
      "loss": 9.8031,
      "step": 4485
    },
    {
      "epoch": 0.5556243039227818,
      "grad_norm": 0.7531576156616211,
      "learning_rate": 2.2224972156911276e-05,
      "loss": 9.8026,
      "step": 4490
    },
    {
      "epoch": 0.5562430392278184,
      "grad_norm": 0.7652230858802795,
      "learning_rate": 2.219403539165945e-05,
      "loss": 9.797,
      "step": 4495
    },
    {
      "epoch": 0.5568617745328548,
      "grad_norm": 0.7512637376785278,
      "learning_rate": 2.2163098626407623e-05,
      "loss": 9.7964,
      "step": 4500
    },
    {
      "epoch": 0.5574805098378913,
      "grad_norm": 0.7477118372917175,
      "learning_rate": 2.2132161861155797e-05,
      "loss": 9.8008,
      "step": 4505
    },
    {
      "epoch": 0.5580992451429279,
      "grad_norm": 0.7423000931739807,
      "learning_rate": 2.2101225095903973e-05,
      "loss": 9.8008,
      "step": 4510
    },
    {
      "epoch": 0.5587179804479644,
      "grad_norm": 0.7344635725021362,
      "learning_rate": 2.2070288330652147e-05,
      "loss": 9.7896,
      "step": 4515
    },
    {
      "epoch": 0.5593367157530008,
      "grad_norm": 0.7353709936141968,
      "learning_rate": 2.2039351565400324e-05,
      "loss": 9.812,
      "step": 4520
    },
    {
      "epoch": 0.5599554510580373,
      "grad_norm": 0.7527976036071777,
      "learning_rate": 2.2008414800148497e-05,
      "loss": 9.7955,
      "step": 4525
    },
    {
      "epoch": 0.5605741863630739,
      "grad_norm": 0.760881781578064,
      "learning_rate": 2.1977478034896674e-05,
      "loss": 9.7867,
      "step": 4530
    },
    {
      "epoch": 0.5611929216681104,
      "grad_norm": 0.7487826943397522,
      "learning_rate": 2.1946541269644847e-05,
      "loss": 9.7812,
      "step": 4535
    },
    {
      "epoch": 0.5618116569731468,
      "grad_norm": 0.7203408479690552,
      "learning_rate": 2.191560450439302e-05,
      "loss": 9.82,
      "step": 4540
    },
    {
      "epoch": 0.5624303922781834,
      "grad_norm": 0.7299968004226685,
      "learning_rate": 2.1884667739141194e-05,
      "loss": 9.7949,
      "step": 4545
    },
    {
      "epoch": 0.5630491275832199,
      "grad_norm": 0.7424200773239136,
      "learning_rate": 2.185373097388937e-05,
      "loss": 9.7926,
      "step": 4550
    },
    {
      "epoch": 0.5636678628882564,
      "grad_norm": 0.7536547183990479,
      "learning_rate": 2.1822794208637545e-05,
      "loss": 9.7847,
      "step": 4555
    },
    {
      "epoch": 0.5642865981932929,
      "grad_norm": 0.7627848386764526,
      "learning_rate": 2.179185744338572e-05,
      "loss": 9.8,
      "step": 4560
    },
    {
      "epoch": 0.5649053334983294,
      "grad_norm": 0.7453803420066833,
      "learning_rate": 2.1760920678133895e-05,
      "loss": 9.7823,
      "step": 4565
    },
    {
      "epoch": 0.5655240688033659,
      "grad_norm": 0.7287955284118652,
      "learning_rate": 2.1729983912882072e-05,
      "loss": 9.7869,
      "step": 4570
    },
    {
      "epoch": 0.5661428041084025,
      "grad_norm": 0.7570687532424927,
      "learning_rate": 2.1699047147630245e-05,
      "loss": 9.784,
      "step": 4575
    },
    {
      "epoch": 0.566761539413439,
      "grad_norm": 0.7544355988502502,
      "learning_rate": 2.1668110382378422e-05,
      "loss": 9.7812,
      "step": 4580
    },
    {
      "epoch": 0.5673802747184754,
      "grad_norm": 0.7792356014251709,
      "learning_rate": 2.1637173617126592e-05,
      "loss": 9.7757,
      "step": 4585
    },
    {
      "epoch": 0.5679990100235119,
      "grad_norm": 0.732563316822052,
      "learning_rate": 2.160623685187477e-05,
      "loss": 9.7947,
      "step": 4590
    },
    {
      "epoch": 0.5686177453285485,
      "grad_norm": 0.755720317363739,
      "learning_rate": 2.1575300086622942e-05,
      "loss": 9.7788,
      "step": 4595
    },
    {
      "epoch": 0.569236480633585,
      "grad_norm": 0.7488192319869995,
      "learning_rate": 2.154436332137112e-05,
      "loss": 9.7784,
      "step": 4600
    },
    {
      "epoch": 0.5698552159386214,
      "grad_norm": 0.7482604384422302,
      "learning_rate": 2.1513426556119293e-05,
      "loss": 9.7889,
      "step": 4605
    },
    {
      "epoch": 0.570473951243658,
      "grad_norm": 0.7275089025497437,
      "learning_rate": 2.148248979086747e-05,
      "loss": 9.7907,
      "step": 4610
    },
    {
      "epoch": 0.5710926865486945,
      "grad_norm": 0.7335599660873413,
      "learning_rate": 2.1451553025615643e-05,
      "loss": 9.7783,
      "step": 4615
    },
    {
      "epoch": 0.571711421853731,
      "grad_norm": 0.7428667545318604,
      "learning_rate": 2.142061626036382e-05,
      "loss": 9.7779,
      "step": 4620
    },
    {
      "epoch": 0.5723301571587674,
      "grad_norm": 0.751731276512146,
      "learning_rate": 2.1389679495111993e-05,
      "loss": 9.7888,
      "step": 4625
    },
    {
      "epoch": 0.572948892463804,
      "grad_norm": 0.7551005482673645,
      "learning_rate": 2.1358742729860167e-05,
      "loss": 9.7862,
      "step": 4630
    },
    {
      "epoch": 0.5735676277688405,
      "grad_norm": 0.7461105585098267,
      "learning_rate": 2.132780596460834e-05,
      "loss": 9.7749,
      "step": 4635
    },
    {
      "epoch": 0.574186363073877,
      "grad_norm": 0.7567535638809204,
      "learning_rate": 2.1296869199356517e-05,
      "loss": 9.7832,
      "step": 4640
    },
    {
      "epoch": 0.5748050983789135,
      "grad_norm": 0.7292292714118958,
      "learning_rate": 2.126593243410469e-05,
      "loss": 9.7801,
      "step": 4645
    },
    {
      "epoch": 0.57542383368395,
      "grad_norm": 0.7519697546958923,
      "learning_rate": 2.1234995668852867e-05,
      "loss": 9.7817,
      "step": 4650
    },
    {
      "epoch": 0.5760425689889865,
      "grad_norm": 0.7497820854187012,
      "learning_rate": 2.120405890360104e-05,
      "loss": 9.7709,
      "step": 4655
    },
    {
      "epoch": 0.5766613042940231,
      "grad_norm": 0.7376313805580139,
      "learning_rate": 2.1173122138349218e-05,
      "loss": 9.7809,
      "step": 4660
    },
    {
      "epoch": 0.5772800395990595,
      "grad_norm": 0.748196005821228,
      "learning_rate": 2.114218537309739e-05,
      "loss": 9.7789,
      "step": 4665
    },
    {
      "epoch": 0.577898774904096,
      "grad_norm": 0.7408873438835144,
      "learning_rate": 2.1111248607845564e-05,
      "loss": 9.799,
      "step": 4670
    },
    {
      "epoch": 0.5785175102091326,
      "grad_norm": 0.7598031759262085,
      "learning_rate": 2.1080311842593738e-05,
      "loss": 9.796,
      "step": 4675
    },
    {
      "epoch": 0.5791362455141691,
      "grad_norm": 0.7399067282676697,
      "learning_rate": 2.1049375077341915e-05,
      "loss": 9.7822,
      "step": 4680
    },
    {
      "epoch": 0.5797549808192055,
      "grad_norm": 0.7532928586006165,
      "learning_rate": 2.1018438312090088e-05,
      "loss": 9.7926,
      "step": 4685
    },
    {
      "epoch": 0.580373716124242,
      "grad_norm": 0.7673555612564087,
      "learning_rate": 2.0987501546838265e-05,
      "loss": 9.7725,
      "step": 4690
    },
    {
      "epoch": 0.5809924514292786,
      "grad_norm": 0.7399761080741882,
      "learning_rate": 2.095656478158644e-05,
      "loss": 9.7807,
      "step": 4695
    },
    {
      "epoch": 0.5816111867343151,
      "grad_norm": 0.7466430068016052,
      "learning_rate": 2.0925628016334615e-05,
      "loss": 9.7675,
      "step": 4700
    },
    {
      "epoch": 0.5822299220393515,
      "grad_norm": 0.7521030306816101,
      "learning_rate": 2.089469125108279e-05,
      "loss": 9.7655,
      "step": 4705
    },
    {
      "epoch": 0.5828486573443881,
      "grad_norm": 0.7518588900566101,
      "learning_rate": 2.0863754485830962e-05,
      "loss": 9.7733,
      "step": 4710
    },
    {
      "epoch": 0.5834673926494246,
      "grad_norm": 0.7285646200180054,
      "learning_rate": 2.0832817720579136e-05,
      "loss": 9.7814,
      "step": 4715
    },
    {
      "epoch": 0.5840861279544611,
      "grad_norm": 0.7464689016342163,
      "learning_rate": 2.0801880955327312e-05,
      "loss": 9.7792,
      "step": 4720
    },
    {
      "epoch": 0.5847048632594976,
      "grad_norm": 0.7454480528831482,
      "learning_rate": 2.0770944190075486e-05,
      "loss": 9.7721,
      "step": 4725
    },
    {
      "epoch": 0.5853235985645341,
      "grad_norm": 0.7746266722679138,
      "learning_rate": 2.0740007424823663e-05,
      "loss": 9.7557,
      "step": 4730
    },
    {
      "epoch": 0.5859423338695706,
      "grad_norm": 0.7443393468856812,
      "learning_rate": 2.0709070659571836e-05,
      "loss": 9.7687,
      "step": 4735
    },
    {
      "epoch": 0.5865610691746072,
      "grad_norm": 0.7396630644798279,
      "learning_rate": 2.0678133894320013e-05,
      "loss": 9.7742,
      "step": 4740
    },
    {
      "epoch": 0.5871798044796436,
      "grad_norm": 0.7568110227584839,
      "learning_rate": 2.0647197129068187e-05,
      "loss": 9.7634,
      "step": 4745
    },
    {
      "epoch": 0.5877985397846801,
      "grad_norm": 0.7500820755958557,
      "learning_rate": 2.061626036381636e-05,
      "loss": 9.7536,
      "step": 4750
    },
    {
      "epoch": 0.5884172750897166,
      "grad_norm": 0.7701388001441956,
      "learning_rate": 2.0585323598564533e-05,
      "loss": 9.7779,
      "step": 4755
    },
    {
      "epoch": 0.5890360103947532,
      "grad_norm": 0.775534987449646,
      "learning_rate": 2.055438683331271e-05,
      "loss": 9.7647,
      "step": 4760
    },
    {
      "epoch": 0.5896547456997896,
      "grad_norm": 0.7451505661010742,
      "learning_rate": 2.0523450068060884e-05,
      "loss": 9.7674,
      "step": 4765
    },
    {
      "epoch": 0.5902734810048261,
      "grad_norm": 0.7560603618621826,
      "learning_rate": 2.049251330280906e-05,
      "loss": 9.7601,
      "step": 4770
    },
    {
      "epoch": 0.5908922163098627,
      "grad_norm": 0.7587476372718811,
      "learning_rate": 2.0461576537557234e-05,
      "loss": 9.7632,
      "step": 4775
    },
    {
      "epoch": 0.5915109516148992,
      "grad_norm": 0.7622325420379639,
      "learning_rate": 2.043063977230541e-05,
      "loss": 9.7593,
      "step": 4780
    },
    {
      "epoch": 0.5921296869199356,
      "grad_norm": 0.7416059970855713,
      "learning_rate": 2.0399703007053584e-05,
      "loss": 9.773,
      "step": 4785
    },
    {
      "epoch": 0.5927484222249721,
      "grad_norm": 0.7665128707885742,
      "learning_rate": 2.0368766241801758e-05,
      "loss": 9.7418,
      "step": 4790
    },
    {
      "epoch": 0.5933671575300087,
      "grad_norm": 0.7481655478477478,
      "learning_rate": 2.033782947654993e-05,
      "loss": 9.7622,
      "step": 4795
    },
    {
      "epoch": 0.5939858928350452,
      "grad_norm": 0.7717500925064087,
      "learning_rate": 2.0306892711298108e-05,
      "loss": 9.7755,
      "step": 4800
    },
    {
      "epoch": 0.5946046281400816,
      "grad_norm": 0.7640740275382996,
      "learning_rate": 2.027595594604628e-05,
      "loss": 9.7439,
      "step": 4805
    },
    {
      "epoch": 0.5952233634451182,
      "grad_norm": 0.7508994340896606,
      "learning_rate": 2.0245019180794458e-05,
      "loss": 9.7701,
      "step": 4810
    },
    {
      "epoch": 0.5958420987501547,
      "grad_norm": 0.7404998540878296,
      "learning_rate": 2.0214082415542632e-05,
      "loss": 9.7682,
      "step": 4815
    },
    {
      "epoch": 0.5964608340551912,
      "grad_norm": 0.7845199704170227,
      "learning_rate": 2.018314565029081e-05,
      "loss": 9.7414,
      "step": 4820
    },
    {
      "epoch": 0.5970795693602277,
      "grad_norm": 0.7734289765357971,
      "learning_rate": 2.0152208885038982e-05,
      "loss": 9.7503,
      "step": 4825
    },
    {
      "epoch": 0.5976983046652642,
      "grad_norm": 0.7716442346572876,
      "learning_rate": 2.012127211978716e-05,
      "loss": 9.7552,
      "step": 4830
    },
    {
      "epoch": 0.5983170399703007,
      "grad_norm": 0.7489607334136963,
      "learning_rate": 2.0090335354535332e-05,
      "loss": 9.7688,
      "step": 4835
    },
    {
      "epoch": 0.5989357752753373,
      "grad_norm": 0.7496655583381653,
      "learning_rate": 2.0059398589283506e-05,
      "loss": 9.7658,
      "step": 4840
    },
    {
      "epoch": 0.5995545105803737,
      "grad_norm": 0.7631008625030518,
      "learning_rate": 2.002846182403168e-05,
      "loss": 9.75,
      "step": 4845
    },
    {
      "epoch": 0.6001732458854102,
      "grad_norm": 0.7692715525627136,
      "learning_rate": 1.9997525058779856e-05,
      "loss": 9.7556,
      "step": 4850
    },
    {
      "epoch": 0.6007919811904467,
      "grad_norm": 0.7683556079864502,
      "learning_rate": 1.996658829352803e-05,
      "loss": 9.7502,
      "step": 4855
    },
    {
      "epoch": 0.6014107164954833,
      "grad_norm": 0.7649844884872437,
      "learning_rate": 1.9935651528276206e-05,
      "loss": 9.7594,
      "step": 4860
    },
    {
      "epoch": 0.6020294518005197,
      "grad_norm": 0.7555107474327087,
      "learning_rate": 1.990471476302438e-05,
      "loss": 9.7552,
      "step": 4865
    },
    {
      "epoch": 0.6026481871055562,
      "grad_norm": 1.9986075162887573,
      "learning_rate": 1.9873777997772557e-05,
      "loss": 9.7491,
      "step": 4870
    },
    {
      "epoch": 0.6032669224105928,
      "grad_norm": 0.766299307346344,
      "learning_rate": 1.984284123252073e-05,
      "loss": 9.7608,
      "step": 4875
    },
    {
      "epoch": 0.6038856577156293,
      "grad_norm": 0.761920154094696,
      "learning_rate": 1.9811904467268904e-05,
      "loss": 9.7494,
      "step": 4880
    },
    {
      "epoch": 0.6045043930206657,
      "grad_norm": 0.7580006122589111,
      "learning_rate": 1.9780967702017077e-05,
      "loss": 9.7596,
      "step": 4885
    },
    {
      "epoch": 0.6051231283257023,
      "grad_norm": 0.7609337568283081,
      "learning_rate": 1.975003093676525e-05,
      "loss": 9.7401,
      "step": 4890
    },
    {
      "epoch": 0.6057418636307388,
      "grad_norm": 0.7536376714706421,
      "learning_rate": 1.9719094171513427e-05,
      "loss": 9.7617,
      "step": 4895
    },
    {
      "epoch": 0.6063605989357753,
      "grad_norm": 0.7470575571060181,
      "learning_rate": 1.96881574062616e-05,
      "loss": 9.7517,
      "step": 4900
    },
    {
      "epoch": 0.6069793342408117,
      "grad_norm": 0.7527756094932556,
      "learning_rate": 1.9657220641009778e-05,
      "loss": 9.7492,
      "step": 4905
    },
    {
      "epoch": 0.6075980695458483,
      "grad_norm": 0.7293347120285034,
      "learning_rate": 1.962628387575795e-05,
      "loss": 9.7549,
      "step": 4910
    },
    {
      "epoch": 0.6082168048508848,
      "grad_norm": 0.7690654993057251,
      "learning_rate": 1.9595347110506128e-05,
      "loss": 9.7495,
      "step": 4915
    },
    {
      "epoch": 0.6088355401559213,
      "grad_norm": 0.7656137347221375,
      "learning_rate": 1.95644103452543e-05,
      "loss": 9.7377,
      "step": 4920
    },
    {
      "epoch": 0.6094542754609578,
      "grad_norm": 0.7579247951507568,
      "learning_rate": 1.9533473580002475e-05,
      "loss": 9.7479,
      "step": 4925
    },
    {
      "epoch": 0.6100730107659943,
      "grad_norm": 0.7358015179634094,
      "learning_rate": 1.9502536814750648e-05,
      "loss": 9.7404,
      "step": 4930
    },
    {
      "epoch": 0.6106917460710308,
      "grad_norm": 0.7524685859680176,
      "learning_rate": 1.9471600049498825e-05,
      "loss": 9.7504,
      "step": 4935
    },
    {
      "epoch": 0.6113104813760674,
      "grad_norm": 0.7456150054931641,
      "learning_rate": 1.9440663284247e-05,
      "loss": 9.7559,
      "step": 4940
    },
    {
      "epoch": 0.6119292166811038,
      "grad_norm": 0.7816848158836365,
      "learning_rate": 1.9409726518995175e-05,
      "loss": 9.725,
      "step": 4945
    },
    {
      "epoch": 0.6125479519861403,
      "grad_norm": 0.7657338976860046,
      "learning_rate": 1.937878975374335e-05,
      "loss": 9.7516,
      "step": 4950
    },
    {
      "epoch": 0.6131666872911768,
      "grad_norm": 0.7557268142700195,
      "learning_rate": 1.9347852988491526e-05,
      "loss": 9.7537,
      "step": 4955
    },
    {
      "epoch": 0.6137854225962134,
      "grad_norm": 0.7811844348907471,
      "learning_rate": 1.93169162232397e-05,
      "loss": 9.7391,
      "step": 4960
    },
    {
      "epoch": 0.6144041579012498,
      "grad_norm": 0.7914026975631714,
      "learning_rate": 1.9285979457987873e-05,
      "loss": 9.741,
      "step": 4965
    },
    {
      "epoch": 0.6150228932062863,
      "grad_norm": 0.797376275062561,
      "learning_rate": 1.9255042692736046e-05,
      "loss": 9.7405,
      "step": 4970
    },
    {
      "epoch": 0.6156416285113229,
      "grad_norm": 0.7668954730033875,
      "learning_rate": 1.9224105927484223e-05,
      "loss": 9.7388,
      "step": 4975
    },
    {
      "epoch": 0.6162603638163594,
      "grad_norm": 0.7637534737586975,
      "learning_rate": 1.9193169162232396e-05,
      "loss": 9.7487,
      "step": 4980
    },
    {
      "epoch": 0.6168790991213958,
      "grad_norm": 0.7555226683616638,
      "learning_rate": 1.9162232396980573e-05,
      "loss": 9.7482,
      "step": 4985
    },
    {
      "epoch": 0.6174978344264324,
      "grad_norm": 0.7757394909858704,
      "learning_rate": 1.9131295631728747e-05,
      "loss": 9.7236,
      "step": 4990
    },
    {
      "epoch": 0.6181165697314689,
      "grad_norm": 0.746116578578949,
      "learning_rate": 1.9100358866476923e-05,
      "loss": 9.7508,
      "step": 4995
    },
    {
      "epoch": 0.6187353050365054,
      "grad_norm": 1.2912118434906006,
      "learning_rate": 1.9069422101225097e-05,
      "loss": 9.7349,
      "step": 5000
    },
    {
      "epoch": 0.6193540403415418,
      "grad_norm": 0.7409324645996094,
      "learning_rate": 1.903848533597327e-05,
      "loss": 9.7465,
      "step": 5005
    },
    {
      "epoch": 0.6199727756465784,
      "grad_norm": 0.7981904149055481,
      "learning_rate": 1.9007548570721444e-05,
      "loss": 9.7156,
      "step": 5010
    },
    {
      "epoch": 0.6205915109516149,
      "grad_norm": 0.7402826547622681,
      "learning_rate": 1.897661180546962e-05,
      "loss": 9.7383,
      "step": 5015
    },
    {
      "epoch": 0.6212102462566514,
      "grad_norm": 0.794200599193573,
      "learning_rate": 1.8945675040217794e-05,
      "loss": 9.7387,
      "step": 5020
    },
    {
      "epoch": 0.6218289815616879,
      "grad_norm": 0.774502158164978,
      "learning_rate": 1.891473827496597e-05,
      "loss": 9.7322,
      "step": 5025
    },
    {
      "epoch": 0.6224477168667244,
      "grad_norm": 0.7563994526863098,
      "learning_rate": 1.8883801509714144e-05,
      "loss": 9.7473,
      "step": 5030
    },
    {
      "epoch": 0.6230664521717609,
      "grad_norm": 0.7557821273803711,
      "learning_rate": 1.885286474446232e-05,
      "loss": 9.7404,
      "step": 5035
    },
    {
      "epoch": 0.6236851874767975,
      "grad_norm": 0.7805448770523071,
      "learning_rate": 1.8821927979210495e-05,
      "loss": 9.7416,
      "step": 5040
    },
    {
      "epoch": 0.6243039227818339,
      "grad_norm": 0.7772266864776611,
      "learning_rate": 1.8790991213958668e-05,
      "loss": 9.7386,
      "step": 5045
    },
    {
      "epoch": 0.6249226580868704,
      "grad_norm": 0.766532301902771,
      "learning_rate": 1.876005444870684e-05,
      "loss": 9.7302,
      "step": 5050
    },
    {
      "epoch": 0.625541393391907,
      "grad_norm": 0.7791079878807068,
      "learning_rate": 1.872911768345502e-05,
      "loss": 9.731,
      "step": 5055
    },
    {
      "epoch": 0.6261601286969435,
      "grad_norm": 0.7666425704956055,
      "learning_rate": 1.8698180918203192e-05,
      "loss": 9.7178,
      "step": 5060
    },
    {
      "epoch": 0.6267788640019799,
      "grad_norm": 0.7408289313316345,
      "learning_rate": 1.866724415295137e-05,
      "loss": 9.7424,
      "step": 5065
    },
    {
      "epoch": 0.6273975993070164,
      "grad_norm": 0.7843794226646423,
      "learning_rate": 1.8636307387699542e-05,
      "loss": 9.7235,
      "step": 5070
    },
    {
      "epoch": 0.628016334612053,
      "grad_norm": 0.7950379848480225,
      "learning_rate": 1.860537062244772e-05,
      "loss": 9.7318,
      "step": 5075
    },
    {
      "epoch": 0.6286350699170895,
      "grad_norm": 0.7904731035232544,
      "learning_rate": 1.8574433857195892e-05,
      "loss": 9.7302,
      "step": 5080
    },
    {
      "epoch": 0.6292538052221259,
      "grad_norm": 0.7414706349372864,
      "learning_rate": 1.854349709194407e-05,
      "loss": 9.735,
      "step": 5085
    },
    {
      "epoch": 0.6298725405271625,
      "grad_norm": 0.7711132168769836,
      "learning_rate": 1.8512560326692243e-05,
      "loss": 9.7262,
      "step": 5090
    },
    {
      "epoch": 0.630491275832199,
      "grad_norm": 0.7583500742912292,
      "learning_rate": 1.8481623561440416e-05,
      "loss": 9.7382,
      "step": 5095
    },
    {
      "epoch": 0.6311100111372355,
      "grad_norm": 0.7598022818565369,
      "learning_rate": 1.845068679618859e-05,
      "loss": 9.7454,
      "step": 5100
    },
    {
      "epoch": 0.631728746442272,
      "grad_norm": 0.7844098210334778,
      "learning_rate": 1.8419750030936766e-05,
      "loss": 9.7183,
      "step": 5105
    },
    {
      "epoch": 0.6323474817473085,
      "grad_norm": 0.7781710624694824,
      "learning_rate": 1.838881326568494e-05,
      "loss": 9.7217,
      "step": 5110
    },
    {
      "epoch": 0.632966217052345,
      "grad_norm": 1.4966404438018799,
      "learning_rate": 1.8357876500433117e-05,
      "loss": 9.7136,
      "step": 5115
    },
    {
      "epoch": 0.6335849523573815,
      "grad_norm": 1.4119170904159546,
      "learning_rate": 1.832693973518129e-05,
      "loss": 9.7238,
      "step": 5120
    },
    {
      "epoch": 0.634203687662418,
      "grad_norm": 0.7824867367744446,
      "learning_rate": 1.8296002969929467e-05,
      "loss": 9.725,
      "step": 5125
    },
    {
      "epoch": 0.6348224229674545,
      "grad_norm": 0.773253321647644,
      "learning_rate": 1.826506620467764e-05,
      "loss": 9.7129,
      "step": 5130
    },
    {
      "epoch": 0.635441158272491,
      "grad_norm": 0.7701585292816162,
      "learning_rate": 1.8234129439425814e-05,
      "loss": 9.733,
      "step": 5135
    },
    {
      "epoch": 0.6360598935775276,
      "grad_norm": 0.8019140362739563,
      "learning_rate": 1.8203192674173987e-05,
      "loss": 9.7219,
      "step": 5140
    },
    {
      "epoch": 0.636678628882564,
      "grad_norm": 0.8002726435661316,
      "learning_rate": 1.8172255908922164e-05,
      "loss": 9.7369,
      "step": 5145
    },
    {
      "epoch": 0.6372973641876005,
      "grad_norm": 0.7651234269142151,
      "learning_rate": 1.8141319143670338e-05,
      "loss": 9.7163,
      "step": 5150
    },
    {
      "epoch": 0.6379160994926371,
      "grad_norm": 0.798576831817627,
      "learning_rate": 1.8110382378418514e-05,
      "loss": 9.7184,
      "step": 5155
    },
    {
      "epoch": 0.6385348347976736,
      "grad_norm": 0.7666305303573608,
      "learning_rate": 1.8079445613166688e-05,
      "loss": 9.7163,
      "step": 5160
    },
    {
      "epoch": 0.63915357010271,
      "grad_norm": 0.761394202709198,
      "learning_rate": 1.8048508847914865e-05,
      "loss": 9.7291,
      "step": 5165
    },
    {
      "epoch": 0.6397723054077465,
      "grad_norm": 0.7727460861206055,
      "learning_rate": 1.8017572082663038e-05,
      "loss": 9.7193,
      "step": 5170
    },
    {
      "epoch": 0.6403910407127831,
      "grad_norm": 0.7753876447677612,
      "learning_rate": 1.798663531741121e-05,
      "loss": 9.7148,
      "step": 5175
    },
    {
      "epoch": 0.6410097760178196,
      "grad_norm": 0.7689822912216187,
      "learning_rate": 1.7955698552159385e-05,
      "loss": 9.7159,
      "step": 5180
    },
    {
      "epoch": 0.641628511322856,
      "grad_norm": 0.7684281468391418,
      "learning_rate": 1.7924761786907562e-05,
      "loss": 9.7242,
      "step": 5185
    },
    {
      "epoch": 0.6422472466278926,
      "grad_norm": 0.7778536677360535,
      "learning_rate": 1.7893825021655735e-05,
      "loss": 9.7038,
      "step": 5190
    },
    {
      "epoch": 0.6428659819329291,
      "grad_norm": 0.7612407207489014,
      "learning_rate": 1.7862888256403912e-05,
      "loss": 9.7286,
      "step": 5195
    },
    {
      "epoch": 0.6434847172379656,
      "grad_norm": 0.7996537089347839,
      "learning_rate": 1.7831951491152086e-05,
      "loss": 9.7037,
      "step": 5200
    },
    {
      "epoch": 0.644103452543002,
      "grad_norm": 0.7748554944992065,
      "learning_rate": 1.7801014725900262e-05,
      "loss": 9.7225,
      "step": 5205
    },
    {
      "epoch": 0.6447221878480386,
      "grad_norm": 0.7692780494689941,
      "learning_rate": 1.7770077960648436e-05,
      "loss": 9.7176,
      "step": 5210
    },
    {
      "epoch": 0.6453409231530751,
      "grad_norm": 0.7997418642044067,
      "learning_rate": 1.773914119539661e-05,
      "loss": 9.7074,
      "step": 5215
    },
    {
      "epoch": 0.6459596584581117,
      "grad_norm": 0.7803428173065186,
      "learning_rate": 1.7708204430144783e-05,
      "loss": 9.719,
      "step": 5220
    },
    {
      "epoch": 0.6465783937631481,
      "grad_norm": 0.7810871601104736,
      "learning_rate": 1.767726766489296e-05,
      "loss": 9.7073,
      "step": 5225
    },
    {
      "epoch": 0.6471971290681846,
      "grad_norm": 0.8012171983718872,
      "learning_rate": 1.7646330899641133e-05,
      "loss": 9.7252,
      "step": 5230
    },
    {
      "epoch": 0.6478158643732211,
      "grad_norm": 0.7794088125228882,
      "learning_rate": 1.761539413438931e-05,
      "loss": 9.6971,
      "step": 5235
    },
    {
      "epoch": 0.6484345996782577,
      "grad_norm": 0.7810291647911072,
      "learning_rate": 1.7584457369137483e-05,
      "loss": 9.7174,
      "step": 5240
    },
    {
      "epoch": 0.6490533349832941,
      "grad_norm": 0.7778422832489014,
      "learning_rate": 1.755352060388566e-05,
      "loss": 9.6955,
      "step": 5245
    },
    {
      "epoch": 0.6496720702883306,
      "grad_norm": 0.7642297148704529,
      "learning_rate": 1.7522583838633834e-05,
      "loss": 9.711,
      "step": 5250
    },
    {
      "epoch": 0.6502908055933672,
      "grad_norm": 0.7697567343711853,
      "learning_rate": 1.7491647073382007e-05,
      "loss": 9.7023,
      "step": 5255
    },
    {
      "epoch": 0.6509095408984037,
      "grad_norm": 0.7788180708885193,
      "learning_rate": 1.746071030813018e-05,
      "loss": 9.7178,
      "step": 5260
    },
    {
      "epoch": 0.6515282762034401,
      "grad_norm": 0.7823185920715332,
      "learning_rate": 1.7429773542878357e-05,
      "loss": 9.7218,
      "step": 5265
    },
    {
      "epoch": 0.6521470115084766,
      "grad_norm": 0.8089513778686523,
      "learning_rate": 1.739883677762653e-05,
      "loss": 9.7003,
      "step": 5270
    },
    {
      "epoch": 0.6527657468135132,
      "grad_norm": 0.7753899097442627,
      "learning_rate": 1.7367900012374708e-05,
      "loss": 9.7022,
      "step": 5275
    },
    {
      "epoch": 0.6533844821185497,
      "grad_norm": 0.760468065738678,
      "learning_rate": 1.733696324712288e-05,
      "loss": 9.712,
      "step": 5280
    },
    {
      "epoch": 0.6540032174235861,
      "grad_norm": 0.7937093377113342,
      "learning_rate": 1.7306026481871058e-05,
      "loss": 9.7057,
      "step": 5285
    },
    {
      "epoch": 0.6546219527286227,
      "grad_norm": 0.7803763747215271,
      "learning_rate": 1.727508971661923e-05,
      "loss": 9.6888,
      "step": 5290
    },
    {
      "epoch": 0.6552406880336592,
      "grad_norm": 0.7582731246948242,
      "learning_rate": 1.7244152951367408e-05,
      "loss": 9.7205,
      "step": 5295
    },
    {
      "epoch": 0.6558594233386957,
      "grad_norm": 0.7996813058853149,
      "learning_rate": 1.721321618611558e-05,
      "loss": 9.7043,
      "step": 5300
    },
    {
      "epoch": 0.6564781586437322,
      "grad_norm": 0.7913403511047363,
      "learning_rate": 1.7182279420863755e-05,
      "loss": 9.7184,
      "step": 5305
    },
    {
      "epoch": 0.6570968939487687,
      "grad_norm": 0.780754029750824,
      "learning_rate": 1.715134265561193e-05,
      "loss": 9.6998,
      "step": 5310
    },
    {
      "epoch": 0.6577156292538052,
      "grad_norm": 0.7708781361579895,
      "learning_rate": 1.7120405890360105e-05,
      "loss": 9.7173,
      "step": 5315
    },
    {
      "epoch": 0.6583343645588418,
      "grad_norm": 0.8114801645278931,
      "learning_rate": 1.708946912510828e-05,
      "loss": 9.6944,
      "step": 5320
    },
    {
      "epoch": 0.6589530998638782,
      "grad_norm": 0.762732207775116,
      "learning_rate": 1.7058532359856456e-05,
      "loss": 9.7149,
      "step": 5325
    },
    {
      "epoch": 0.6595718351689147,
      "grad_norm": 0.7876279950141907,
      "learning_rate": 1.702759559460463e-05,
      "loss": 9.7006,
      "step": 5330
    },
    {
      "epoch": 0.6601905704739512,
      "grad_norm": 0.7778507471084595,
      "learning_rate": 1.6996658829352806e-05,
      "loss": 9.7151,
      "step": 5335
    },
    {
      "epoch": 0.6608093057789878,
      "grad_norm": 0.7629062533378601,
      "learning_rate": 1.696572206410098e-05,
      "loss": 9.6995,
      "step": 5340
    },
    {
      "epoch": 0.6614280410840243,
      "grad_norm": 0.7933209538459778,
      "learning_rate": 1.6934785298849153e-05,
      "loss": 9.6839,
      "step": 5345
    },
    {
      "epoch": 0.6620467763890607,
      "grad_norm": 0.7873448729515076,
      "learning_rate": 1.6903848533597326e-05,
      "loss": 9.6954,
      "step": 5350
    },
    {
      "epoch": 0.6626655116940973,
      "grad_norm": 0.8097968697547913,
      "learning_rate": 1.6872911768345503e-05,
      "loss": 9.6858,
      "step": 5355
    },
    {
      "epoch": 0.6632842469991338,
      "grad_norm": 0.7773791551589966,
      "learning_rate": 1.6841975003093677e-05,
      "loss": 9.708,
      "step": 5360
    },
    {
      "epoch": 0.6639029823041703,
      "grad_norm": 0.7742453813552856,
      "learning_rate": 1.6811038237841854e-05,
      "loss": 9.6972,
      "step": 5365
    },
    {
      "epoch": 0.6645217176092068,
      "grad_norm": 0.8082020878791809,
      "learning_rate": 1.6780101472590027e-05,
      "loss": 9.6903,
      "step": 5370
    },
    {
      "epoch": 0.6651404529142433,
      "grad_norm": 0.7900300621986389,
      "learning_rate": 1.6749164707338204e-05,
      "loss": 9.6883,
      "step": 5375
    },
    {
      "epoch": 0.6657591882192798,
      "grad_norm": 0.8050157427787781,
      "learning_rate": 1.6718227942086377e-05,
      "loss": 9.6768,
      "step": 5380
    },
    {
      "epoch": 0.6663779235243164,
      "grad_norm": 0.7972925305366516,
      "learning_rate": 1.668729117683455e-05,
      "loss": 9.6886,
      "step": 5385
    },
    {
      "epoch": 0.6669966588293528,
      "grad_norm": 0.7848356366157532,
      "learning_rate": 1.6656354411582724e-05,
      "loss": 9.7004,
      "step": 5390
    },
    {
      "epoch": 0.6676153941343893,
      "grad_norm": 0.7638557553291321,
      "learning_rate": 1.66254176463309e-05,
      "loss": 9.6836,
      "step": 5395
    },
    {
      "epoch": 0.6682341294394258,
      "grad_norm": 0.7858190536499023,
      "learning_rate": 1.6594480881079074e-05,
      "loss": 9.6863,
      "step": 5400
    },
    {
      "epoch": 0.6688528647444624,
      "grad_norm": 0.7893261313438416,
      "learning_rate": 1.656354411582725e-05,
      "loss": 9.6856,
      "step": 5405
    },
    {
      "epoch": 0.6694716000494988,
      "grad_norm": 0.7908456325531006,
      "learning_rate": 1.6532607350575425e-05,
      "loss": 9.6895,
      "step": 5410
    },
    {
      "epoch": 0.6700903353545353,
      "grad_norm": 0.7917628884315491,
      "learning_rate": 1.65016705853236e-05,
      "loss": 9.693,
      "step": 5415
    },
    {
      "epoch": 0.6707090706595719,
      "grad_norm": 0.7965425848960876,
      "learning_rate": 1.6470733820071775e-05,
      "loss": 9.6864,
      "step": 5420
    },
    {
      "epoch": 0.6713278059646084,
      "grad_norm": 0.8005728721618652,
      "learning_rate": 1.643979705481995e-05,
      "loss": 9.6976,
      "step": 5425
    },
    {
      "epoch": 0.6719465412696448,
      "grad_norm": 0.8014860153198242,
      "learning_rate": 1.6408860289568122e-05,
      "loss": 9.6802,
      "step": 5430
    },
    {
      "epoch": 0.6725652765746813,
      "grad_norm": 0.7948771715164185,
      "learning_rate": 1.63779235243163e-05,
      "loss": 9.688,
      "step": 5435
    },
    {
      "epoch": 0.6731840118797179,
      "grad_norm": 0.8128145337104797,
      "learning_rate": 1.6346986759064472e-05,
      "loss": 9.672,
      "step": 5440
    },
    {
      "epoch": 0.6738027471847544,
      "grad_norm": 0.7796803712844849,
      "learning_rate": 1.631604999381265e-05,
      "loss": 9.7147,
      "step": 5445
    },
    {
      "epoch": 0.6744214824897908,
      "grad_norm": 0.7654549479484558,
      "learning_rate": 1.6285113228560822e-05,
      "loss": 9.701,
      "step": 5450
    },
    {
      "epoch": 0.6750402177948274,
      "grad_norm": 0.7600424289703369,
      "learning_rate": 1.6254176463309e-05,
      "loss": 9.6987,
      "step": 5455
    },
    {
      "epoch": 0.6756589530998639,
      "grad_norm": 0.7997243404388428,
      "learning_rate": 1.6223239698057173e-05,
      "loss": 9.6798,
      "step": 5460
    },
    {
      "epoch": 0.6762776884049004,
      "grad_norm": 0.8043048977851868,
      "learning_rate": 1.6192302932805346e-05,
      "loss": 9.6856,
      "step": 5465
    },
    {
      "epoch": 0.6768964237099369,
      "grad_norm": 0.7818135619163513,
      "learning_rate": 1.616136616755352e-05,
      "loss": 9.6937,
      "step": 5470
    },
    {
      "epoch": 0.6775151590149734,
      "grad_norm": 0.8117141127586365,
      "learning_rate": 1.6130429402301697e-05,
      "loss": 9.6716,
      "step": 5475
    },
    {
      "epoch": 0.6781338943200099,
      "grad_norm": 0.791955292224884,
      "learning_rate": 1.609949263704987e-05,
      "loss": 9.7009,
      "step": 5480
    },
    {
      "epoch": 0.6787526296250465,
      "grad_norm": 0.7808468341827393,
      "learning_rate": 1.6068555871798047e-05,
      "loss": 9.6759,
      "step": 5485
    },
    {
      "epoch": 0.6793713649300829,
      "grad_norm": 0.8136070370674133,
      "learning_rate": 1.603761910654622e-05,
      "loss": 9.6763,
      "step": 5490
    },
    {
      "epoch": 0.6799901002351194,
      "grad_norm": 0.7838420867919922,
      "learning_rate": 1.6006682341294397e-05,
      "loss": 9.6839,
      "step": 5495
    },
    {
      "epoch": 0.6806088355401559,
      "grad_norm": 0.804891049861908,
      "learning_rate": 1.597574557604257e-05,
      "loss": 9.673,
      "step": 5500
    },
    {
      "epoch": 0.6812275708451925,
      "grad_norm": 0.8013268709182739,
      "learning_rate": 1.5944808810790744e-05,
      "loss": 9.6904,
      "step": 5505
    },
    {
      "epoch": 0.6818463061502289,
      "grad_norm": 0.786186695098877,
      "learning_rate": 1.5913872045538917e-05,
      "loss": 9.6882,
      "step": 5510
    },
    {
      "epoch": 0.6824650414552654,
      "grad_norm": 0.7988574504852295,
      "learning_rate": 1.5882935280287094e-05,
      "loss": 9.6881,
      "step": 5515
    },
    {
      "epoch": 0.683083776760302,
      "grad_norm": 0.7662178874015808,
      "learning_rate": 1.5851998515035268e-05,
      "loss": 9.6763,
      "step": 5520
    },
    {
      "epoch": 0.6837025120653385,
      "grad_norm": 0.814612090587616,
      "learning_rate": 1.5821061749783445e-05,
      "loss": 9.6581,
      "step": 5525
    },
    {
      "epoch": 0.6843212473703749,
      "grad_norm": 0.7861854434013367,
      "learning_rate": 1.5790124984531618e-05,
      "loss": 9.6867,
      "step": 5530
    },
    {
      "epoch": 0.6849399826754115,
      "grad_norm": 0.7895804643630981,
      "learning_rate": 1.5759188219279795e-05,
      "loss": 9.6785,
      "step": 5535
    },
    {
      "epoch": 0.685558717980448,
      "grad_norm": 0.795143187046051,
      "learning_rate": 1.5728251454027968e-05,
      "loss": 9.6731,
      "step": 5540
    },
    {
      "epoch": 0.6861774532854845,
      "grad_norm": 0.8017016649246216,
      "learning_rate": 1.5697314688776145e-05,
      "loss": 9.6848,
      "step": 5545
    },
    {
      "epoch": 0.6867961885905209,
      "grad_norm": 0.7866496443748474,
      "learning_rate": 1.5666377923524315e-05,
      "loss": 9.686,
      "step": 5550
    },
    {
      "epoch": 0.6874149238955575,
      "grad_norm": 0.765164315700531,
      "learning_rate": 1.5635441158272492e-05,
      "loss": 9.6903,
      "step": 5555
    },
    {
      "epoch": 0.688033659200594,
      "grad_norm": 0.792915940284729,
      "learning_rate": 1.5604504393020665e-05,
      "loss": 9.6716,
      "step": 5560
    },
    {
      "epoch": 0.6886523945056305,
      "grad_norm": 0.81101393699646,
      "learning_rate": 1.5573567627768842e-05,
      "loss": 9.6691,
      "step": 5565
    },
    {
      "epoch": 0.689271129810667,
      "grad_norm": 0.8113877773284912,
      "learning_rate": 1.5542630862517016e-05,
      "loss": 9.6707,
      "step": 5570
    },
    {
      "epoch": 0.6898898651157035,
      "grad_norm": 0.7977849245071411,
      "learning_rate": 1.5511694097265193e-05,
      "loss": 9.673,
      "step": 5575
    },
    {
      "epoch": 0.69050860042074,
      "grad_norm": 0.7889214754104614,
      "learning_rate": 1.5480757332013366e-05,
      "loss": 9.662,
      "step": 5580
    },
    {
      "epoch": 0.6911273357257766,
      "grad_norm": 0.7843697667121887,
      "learning_rate": 1.5449820566761543e-05,
      "loss": 9.6782,
      "step": 5585
    },
    {
      "epoch": 0.691746071030813,
      "grad_norm": 0.7954530715942383,
      "learning_rate": 1.5418883801509716e-05,
      "loss": 9.6748,
      "step": 5590
    },
    {
      "epoch": 0.6923648063358495,
      "grad_norm": 0.8126107454299927,
      "learning_rate": 1.538794703625789e-05,
      "loss": 9.6694,
      "step": 5595
    },
    {
      "epoch": 0.692983541640886,
      "grad_norm": 0.8247904777526855,
      "learning_rate": 1.5357010271006063e-05,
      "loss": 9.6501,
      "step": 5600
    },
    {
      "epoch": 0.6936022769459226,
      "grad_norm": 0.8091068863868713,
      "learning_rate": 1.532607350575424e-05,
      "loss": 9.6678,
      "step": 5605
    },
    {
      "epoch": 0.694221012250959,
      "grad_norm": 0.8099085688591003,
      "learning_rate": 1.5295136740502414e-05,
      "loss": 9.6689,
      "step": 5610
    },
    {
      "epoch": 0.6948397475559955,
      "grad_norm": 0.7997695803642273,
      "learning_rate": 1.526419997525059e-05,
      "loss": 9.681,
      "step": 5615
    },
    {
      "epoch": 0.6954584828610321,
      "grad_norm": 0.8145144581794739,
      "learning_rate": 1.5233263209998764e-05,
      "loss": 9.669,
      "step": 5620
    },
    {
      "epoch": 0.6960772181660686,
      "grad_norm": 0.798393726348877,
      "learning_rate": 1.5202326444746939e-05,
      "loss": 9.67,
      "step": 5625
    },
    {
      "epoch": 0.696695953471105,
      "grad_norm": 0.8247685432434082,
      "learning_rate": 1.5171389679495112e-05,
      "loss": 9.6672,
      "step": 5630
    },
    {
      "epoch": 0.6973146887761416,
      "grad_norm": 0.8186852335929871,
      "learning_rate": 1.514045291424329e-05,
      "loss": 9.6565,
      "step": 5635
    },
    {
      "epoch": 0.6979334240811781,
      "grad_norm": 0.813069760799408,
      "learning_rate": 1.5109516148991463e-05,
      "loss": 9.6607,
      "step": 5640
    },
    {
      "epoch": 0.6985521593862146,
      "grad_norm": 0.8159972429275513,
      "learning_rate": 1.5078579383739636e-05,
      "loss": 9.6776,
      "step": 5645
    },
    {
      "epoch": 0.699170894691251,
      "grad_norm": 0.7956772446632385,
      "learning_rate": 1.5047642618487811e-05,
      "loss": 9.6792,
      "step": 5650
    },
    {
      "epoch": 0.6997896299962876,
      "grad_norm": 0.8034965991973877,
      "learning_rate": 1.5016705853235985e-05,
      "loss": 9.6535,
      "step": 5655
    },
    {
      "epoch": 0.7004083653013241,
      "grad_norm": 0.7716604471206665,
      "learning_rate": 1.4985769087984162e-05,
      "loss": 9.6956,
      "step": 5660
    },
    {
      "epoch": 0.7010271006063606,
      "grad_norm": 0.7932407259941101,
      "learning_rate": 1.4954832322732335e-05,
      "loss": 9.6693,
      "step": 5665
    },
    {
      "epoch": 0.7016458359113971,
      "grad_norm": 0.7900193929672241,
      "learning_rate": 1.492389555748051e-05,
      "loss": 9.6722,
      "step": 5670
    },
    {
      "epoch": 0.7022645712164336,
      "grad_norm": 0.8125334978103638,
      "learning_rate": 1.4892958792228684e-05,
      "loss": 9.6517,
      "step": 5675
    },
    {
      "epoch": 0.7028833065214701,
      "grad_norm": 0.7840325832366943,
      "learning_rate": 1.486202202697686e-05,
      "loss": 9.6784,
      "step": 5680
    },
    {
      "epoch": 0.7035020418265067,
      "grad_norm": 0.8141003847122192,
      "learning_rate": 1.4831085261725034e-05,
      "loss": 9.6532,
      "step": 5685
    },
    {
      "epoch": 0.7041207771315431,
      "grad_norm": 1.1582863330841064,
      "learning_rate": 1.4800148496473209e-05,
      "loss": 9.6603,
      "step": 5690
    },
    {
      "epoch": 0.7047395124365796,
      "grad_norm": 0.80693519115448,
      "learning_rate": 1.4769211731221383e-05,
      "loss": 9.6602,
      "step": 5695
    },
    {
      "epoch": 0.7053582477416162,
      "grad_norm": 0.7992913722991943,
      "learning_rate": 1.473827496596956e-05,
      "loss": 9.6718,
      "step": 5700
    },
    {
      "epoch": 0.7059769830466527,
      "grad_norm": 0.8015346527099609,
      "learning_rate": 1.4707338200717733e-05,
      "loss": 9.6715,
      "step": 5705
    },
    {
      "epoch": 0.7065957183516891,
      "grad_norm": 0.8166565299034119,
      "learning_rate": 1.4676401435465908e-05,
      "loss": 9.6519,
      "step": 5710
    },
    {
      "epoch": 0.7072144536567256,
      "grad_norm": 0.8056490421295166,
      "learning_rate": 1.4645464670214081e-05,
      "loss": 9.6709,
      "step": 5715
    },
    {
      "epoch": 0.7078331889617622,
      "grad_norm": 1.0628079175949097,
      "learning_rate": 1.4614527904962258e-05,
      "loss": 9.6624,
      "step": 5720
    },
    {
      "epoch": 0.7084519242667987,
      "grad_norm": 0.8048571944236755,
      "learning_rate": 1.4583591139710432e-05,
      "loss": 9.6602,
      "step": 5725
    },
    {
      "epoch": 0.7090706595718351,
      "grad_norm": 0.8249792456626892,
      "learning_rate": 1.4552654374458607e-05,
      "loss": 9.6452,
      "step": 5730
    },
    {
      "epoch": 0.7096893948768717,
      "grad_norm": 0.8007671236991882,
      "learning_rate": 1.452171760920678e-05,
      "loss": 9.6527,
      "step": 5735
    },
    {
      "epoch": 0.7103081301819082,
      "grad_norm": 0.8144829273223877,
      "learning_rate": 1.4490780843954957e-05,
      "loss": 9.6537,
      "step": 5740
    },
    {
      "epoch": 0.7109268654869447,
      "grad_norm": 0.8429927229881287,
      "learning_rate": 1.445984407870313e-05,
      "loss": 9.6417,
      "step": 5745
    },
    {
      "epoch": 0.7115456007919811,
      "grad_norm": 0.813316822052002,
      "learning_rate": 1.4428907313451306e-05,
      "loss": 9.6558,
      "step": 5750
    },
    {
      "epoch": 0.7121643360970177,
      "grad_norm": 0.7933462858200073,
      "learning_rate": 1.439797054819948e-05,
      "loss": 9.6516,
      "step": 5755
    },
    {
      "epoch": 0.7127830714020542,
      "grad_norm": 0.8276988863945007,
      "learning_rate": 1.4367033782947656e-05,
      "loss": 9.6619,
      "step": 5760
    },
    {
      "epoch": 0.7134018067070907,
      "grad_norm": 0.7795549631118774,
      "learning_rate": 1.433609701769583e-05,
      "loss": 9.6709,
      "step": 5765
    },
    {
      "epoch": 0.7140205420121272,
      "grad_norm": 0.7945383191108704,
      "learning_rate": 1.4305160252444005e-05,
      "loss": 9.656,
      "step": 5770
    },
    {
      "epoch": 0.7146392773171637,
      "grad_norm": 0.8014668822288513,
      "learning_rate": 1.4274223487192178e-05,
      "loss": 9.6711,
      "step": 5775
    },
    {
      "epoch": 0.7152580126222002,
      "grad_norm": 0.8247209191322327,
      "learning_rate": 1.4243286721940355e-05,
      "loss": 9.6639,
      "step": 5780
    },
    {
      "epoch": 0.7158767479272368,
      "grad_norm": 0.7978716492652893,
      "learning_rate": 1.4212349956688528e-05,
      "loss": 9.6447,
      "step": 5785
    },
    {
      "epoch": 0.7164954832322732,
      "grad_norm": 0.8142762780189514,
      "learning_rate": 1.4181413191436705e-05,
      "loss": 9.6629,
      "step": 5790
    },
    {
      "epoch": 0.7171142185373097,
      "grad_norm": 0.8282533288002014,
      "learning_rate": 1.4150476426184877e-05,
      "loss": 9.643,
      "step": 5795
    },
    {
      "epoch": 0.7177329538423463,
      "grad_norm": 0.8265928626060486,
      "learning_rate": 1.4119539660933054e-05,
      "loss": 9.6583,
      "step": 5800
    },
    {
      "epoch": 0.7183516891473828,
      "grad_norm": 0.8205691576004028,
      "learning_rate": 1.4088602895681227e-05,
      "loss": 9.6476,
      "step": 5805
    },
    {
      "epoch": 0.7189704244524192,
      "grad_norm": 0.81816166639328,
      "learning_rate": 1.4057666130429404e-05,
      "loss": 9.6474,
      "step": 5810
    },
    {
      "epoch": 0.7195891597574557,
      "grad_norm": 0.8004152178764343,
      "learning_rate": 1.4026729365177576e-05,
      "loss": 9.6535,
      "step": 5815
    },
    {
      "epoch": 0.7202078950624923,
      "grad_norm": 0.8221141695976257,
      "learning_rate": 1.3995792599925753e-05,
      "loss": 9.6407,
      "step": 5820
    },
    {
      "epoch": 0.7208266303675288,
      "grad_norm": 0.8140720725059509,
      "learning_rate": 1.3964855834673926e-05,
      "loss": 9.6504,
      "step": 5825
    },
    {
      "epoch": 0.7214453656725652,
      "grad_norm": 0.8244473934173584,
      "learning_rate": 1.3933919069422103e-05,
      "loss": 9.631,
      "step": 5830
    },
    {
      "epoch": 0.7220641009776018,
      "grad_norm": 0.8222771286964417,
      "learning_rate": 1.3902982304170276e-05,
      "loss": 9.6553,
      "step": 5835
    },
    {
      "epoch": 0.7226828362826383,
      "grad_norm": 0.7936749458312988,
      "learning_rate": 1.3872045538918452e-05,
      "loss": 9.657,
      "step": 5840
    },
    {
      "epoch": 0.7233015715876748,
      "grad_norm": 0.8086034655570984,
      "learning_rate": 1.3841108773666625e-05,
      "loss": 9.6428,
      "step": 5845
    },
    {
      "epoch": 0.7239203068927113,
      "grad_norm": 0.8205540180206299,
      "learning_rate": 1.3810172008414802e-05,
      "loss": 9.6465,
      "step": 5850
    },
    {
      "epoch": 0.7245390421977478,
      "grad_norm": 0.8154431581497192,
      "learning_rate": 1.3779235243162975e-05,
      "loss": 9.6383,
      "step": 5855
    },
    {
      "epoch": 0.7251577775027843,
      "grad_norm": 0.7897377610206604,
      "learning_rate": 1.374829847791115e-05,
      "loss": 9.6382,
      "step": 5860
    },
    {
      "epoch": 0.7257765128078209,
      "grad_norm": 0.8346249461174011,
      "learning_rate": 1.3717361712659324e-05,
      "loss": 9.6368,
      "step": 5865
    },
    {
      "epoch": 0.7263952481128573,
      "grad_norm": 0.8470889329910278,
      "learning_rate": 1.36864249474075e-05,
      "loss": 9.6413,
      "step": 5870
    },
    {
      "epoch": 0.7270139834178938,
      "grad_norm": 0.7905415296554565,
      "learning_rate": 1.3655488182155674e-05,
      "loss": 9.6503,
      "step": 5875
    },
    {
      "epoch": 0.7276327187229303,
      "grad_norm": 0.8297327160835266,
      "learning_rate": 1.362455141690385e-05,
      "loss": 9.6506,
      "step": 5880
    },
    {
      "epoch": 0.7282514540279669,
      "grad_norm": 0.7999584674835205,
      "learning_rate": 1.3593614651652023e-05,
      "loss": 9.6448,
      "step": 5885
    },
    {
      "epoch": 0.7288701893330033,
      "grad_norm": 0.7995285391807556,
      "learning_rate": 1.35626778864002e-05,
      "loss": 9.6495,
      "step": 5890
    },
    {
      "epoch": 0.7294889246380398,
      "grad_norm": 0.8038880228996277,
      "learning_rate": 1.3531741121148373e-05,
      "loss": 9.6573,
      "step": 5895
    },
    {
      "epoch": 0.7301076599430764,
      "grad_norm": 0.8197425603866577,
      "learning_rate": 1.3500804355896548e-05,
      "loss": 9.6546,
      "step": 5900
    },
    {
      "epoch": 0.7307263952481129,
      "grad_norm": 0.8447290658950806,
      "learning_rate": 1.3469867590644722e-05,
      "loss": 9.6563,
      "step": 5905
    },
    {
      "epoch": 0.7313451305531493,
      "grad_norm": 0.8296226263046265,
      "learning_rate": 1.3438930825392898e-05,
      "loss": 9.6369,
      "step": 5910
    },
    {
      "epoch": 0.7319638658581858,
      "grad_norm": 0.8194687366485596,
      "learning_rate": 1.3407994060141072e-05,
      "loss": 9.6272,
      "step": 5915
    },
    {
      "epoch": 0.7325826011632224,
      "grad_norm": 0.778822660446167,
      "learning_rate": 1.3377057294889247e-05,
      "loss": 9.6548,
      "step": 5920
    },
    {
      "epoch": 0.7332013364682589,
      "grad_norm": 0.8178225159645081,
      "learning_rate": 1.334612052963742e-05,
      "loss": 9.6465,
      "step": 5925
    },
    {
      "epoch": 0.7338200717732953,
      "grad_norm": 0.8008419871330261,
      "learning_rate": 1.3315183764385597e-05,
      "loss": 9.6566,
      "step": 5930
    },
    {
      "epoch": 0.7344388070783319,
      "grad_norm": 0.8322784900665283,
      "learning_rate": 1.328424699913377e-05,
      "loss": 9.639,
      "step": 5935
    },
    {
      "epoch": 0.7350575423833684,
      "grad_norm": 0.8115869760513306,
      "learning_rate": 1.3253310233881946e-05,
      "loss": 9.6334,
      "step": 5940
    },
    {
      "epoch": 0.7356762776884049,
      "grad_norm": 0.8373883366584778,
      "learning_rate": 1.322237346863012e-05,
      "loss": 9.6393,
      "step": 5945
    },
    {
      "epoch": 0.7362950129934414,
      "grad_norm": 0.8342370986938477,
      "learning_rate": 1.3191436703378296e-05,
      "loss": 9.6294,
      "step": 5950
    },
    {
      "epoch": 0.7369137482984779,
      "grad_norm": 0.8361262679100037,
      "learning_rate": 1.316049993812647e-05,
      "loss": 9.6281,
      "step": 5955
    },
    {
      "epoch": 0.7375324836035144,
      "grad_norm": 0.7924367189407349,
      "learning_rate": 1.3129563172874645e-05,
      "loss": 9.6552,
      "step": 5960
    },
    {
      "epoch": 0.738151218908551,
      "grad_norm": 0.7991692423820496,
      "learning_rate": 1.3098626407622818e-05,
      "loss": 9.6415,
      "step": 5965
    },
    {
      "epoch": 0.7387699542135874,
      "grad_norm": 0.8290521502494812,
      "learning_rate": 1.3067689642370995e-05,
      "loss": 9.6253,
      "step": 5970
    },
    {
      "epoch": 0.7393886895186239,
      "grad_norm": 0.8040965795516968,
      "learning_rate": 1.3036752877119169e-05,
      "loss": 9.6358,
      "step": 5975
    },
    {
      "epoch": 0.7400074248236604,
      "grad_norm": 0.8111066222190857,
      "learning_rate": 1.3005816111867344e-05,
      "loss": 9.6325,
      "step": 5980
    },
    {
      "epoch": 0.740626160128697,
      "grad_norm": 2.103257894515991,
      "learning_rate": 1.2974879346615517e-05,
      "loss": 9.6429,
      "step": 5985
    },
    {
      "epoch": 0.7412448954337334,
      "grad_norm": 0.8149127960205078,
      "learning_rate": 1.2943942581363694e-05,
      "loss": 9.6326,
      "step": 5990
    },
    {
      "epoch": 0.7418636307387699,
      "grad_norm": 0.8134175539016724,
      "learning_rate": 1.2913005816111867e-05,
      "loss": 9.6219,
      "step": 5995
    },
    {
      "epoch": 0.7424823660438065,
      "grad_norm": 0.8158199191093445,
      "learning_rate": 1.2882069050860043e-05,
      "loss": 9.653,
      "step": 6000
    },
    {
      "epoch": 0.743101101348843,
      "grad_norm": 0.8195940852165222,
      "learning_rate": 1.2851132285608216e-05,
      "loss": 9.6233,
      "step": 6005
    },
    {
      "epoch": 0.7437198366538794,
      "grad_norm": 0.8274351954460144,
      "learning_rate": 1.2820195520356393e-05,
      "loss": 9.6309,
      "step": 6010
    },
    {
      "epoch": 0.744338571958916,
      "grad_norm": 0.8204032778739929,
      "learning_rate": 1.2789258755104566e-05,
      "loss": 9.6245,
      "step": 6015
    },
    {
      "epoch": 0.7449573072639525,
      "grad_norm": 0.802147626876831,
      "learning_rate": 1.2758321989852741e-05,
      "loss": 9.6285,
      "step": 6020
    },
    {
      "epoch": 0.745576042568989,
      "grad_norm": 0.8125606179237366,
      "learning_rate": 1.2727385224600915e-05,
      "loss": 9.6259,
      "step": 6025
    },
    {
      "epoch": 0.7461947778740254,
      "grad_norm": 0.8376052975654602,
      "learning_rate": 1.2696448459349092e-05,
      "loss": 9.6177,
      "step": 6030
    },
    {
      "epoch": 0.746813513179062,
      "grad_norm": 1.6292932033538818,
      "learning_rate": 1.2665511694097265e-05,
      "loss": 9.6382,
      "step": 6035
    },
    {
      "epoch": 0.7474322484840985,
      "grad_norm": 0.8054199814796448,
      "learning_rate": 1.2634574928845442e-05,
      "loss": 9.6443,
      "step": 6040
    },
    {
      "epoch": 0.748050983789135,
      "grad_norm": 0.8292089700698853,
      "learning_rate": 1.2603638163593614e-05,
      "loss": 9.6195,
      "step": 6045
    },
    {
      "epoch": 0.7486697190941715,
      "grad_norm": 0.8123289942741394,
      "learning_rate": 1.257270139834179e-05,
      "loss": 9.6311,
      "step": 6050
    },
    {
      "epoch": 0.749288454399208,
      "grad_norm": 1.3729345798492432,
      "learning_rate": 1.2541764633089964e-05,
      "loss": 9.6371,
      "step": 6055
    },
    {
      "epoch": 0.7499071897042445,
      "grad_norm": 0.8161652684211731,
      "learning_rate": 1.2510827867838141e-05,
      "loss": 9.6329,
      "step": 6060
    },
    {
      "epoch": 0.7505259250092811,
      "grad_norm": 0.8230689764022827,
      "learning_rate": 1.2479891102586314e-05,
      "loss": 9.6238,
      "step": 6065
    },
    {
      "epoch": 0.7511446603143175,
      "grad_norm": 0.8558626174926758,
      "learning_rate": 1.2448954337334488e-05,
      "loss": 9.6274,
      "step": 6070
    },
    {
      "epoch": 0.751763395619354,
      "grad_norm": 0.8384385704994202,
      "learning_rate": 1.2418017572082663e-05,
      "loss": 9.6092,
      "step": 6075
    },
    {
      "epoch": 0.7523821309243905,
      "grad_norm": 0.8241512179374695,
      "learning_rate": 1.2387080806830838e-05,
      "loss": 9.6312,
      "step": 6080
    },
    {
      "epoch": 0.7530008662294271,
      "grad_norm": 0.8147686719894409,
      "learning_rate": 1.2356144041579013e-05,
      "loss": 9.6308,
      "step": 6085
    },
    {
      "epoch": 0.7536196015344635,
      "grad_norm": 0.8044889569282532,
      "learning_rate": 1.2325207276327187e-05,
      "loss": 9.6212,
      "step": 6090
    },
    {
      "epoch": 0.7542383368395,
      "grad_norm": 0.8636247515678406,
      "learning_rate": 1.2294270511075362e-05,
      "loss": 9.6026,
      "step": 6095
    },
    {
      "epoch": 0.7548570721445366,
      "grad_norm": 0.8201771378517151,
      "learning_rate": 1.2263333745823537e-05,
      "loss": 9.6216,
      "step": 6100
    },
    {
      "epoch": 0.7554758074495731,
      "grad_norm": 0.8289753198623657,
      "learning_rate": 1.2232396980571712e-05,
      "loss": 9.6208,
      "step": 6105
    },
    {
      "epoch": 0.7560945427546096,
      "grad_norm": 0.8184656500816345,
      "learning_rate": 1.2201460215319886e-05,
      "loss": 9.623,
      "step": 6110
    },
    {
      "epoch": 0.756713278059646,
      "grad_norm": 0.8511149287223816,
      "learning_rate": 1.217052345006806e-05,
      "loss": 9.6138,
      "step": 6115
    },
    {
      "epoch": 0.7573320133646826,
      "grad_norm": 0.8130069971084595,
      "learning_rate": 1.2139586684816236e-05,
      "loss": 9.6218,
      "step": 6120
    },
    {
      "epoch": 0.7579507486697191,
      "grad_norm": 0.7901248931884766,
      "learning_rate": 1.2108649919564411e-05,
      "loss": 9.6371,
      "step": 6125
    },
    {
      "epoch": 0.7585694839747557,
      "grad_norm": 0.8168195486068726,
      "learning_rate": 1.2077713154312584e-05,
      "loss": 9.6292,
      "step": 6130
    },
    {
      "epoch": 0.7591882192797921,
      "grad_norm": 0.8334643244743347,
      "learning_rate": 1.204677638906076e-05,
      "loss": 9.6108,
      "step": 6135
    },
    {
      "epoch": 0.7598069545848286,
      "grad_norm": 0.8189067244529724,
      "learning_rate": 1.2015839623808935e-05,
      "loss": 9.6218,
      "step": 6140
    },
    {
      "epoch": 0.7604256898898651,
      "grad_norm": 0.7892506718635559,
      "learning_rate": 1.198490285855711e-05,
      "loss": 9.6225,
      "step": 6145
    },
    {
      "epoch": 0.7610444251949017,
      "grad_norm": 0.8114447593688965,
      "learning_rate": 1.1953966093305283e-05,
      "loss": 9.6201,
      "step": 6150
    },
    {
      "epoch": 0.7616631604999381,
      "grad_norm": 0.8312785029411316,
      "learning_rate": 1.1923029328053458e-05,
      "loss": 9.6212,
      "step": 6155
    },
    {
      "epoch": 0.7622818958049746,
      "grad_norm": 0.8263265490531921,
      "learning_rate": 1.1892092562801634e-05,
      "loss": 9.6283,
      "step": 6160
    },
    {
      "epoch": 0.7629006311100112,
      "grad_norm": 0.8286614418029785,
      "learning_rate": 1.1861155797549809e-05,
      "loss": 9.611,
      "step": 6165
    },
    {
      "epoch": 0.7635193664150477,
      "grad_norm": 0.8241921663284302,
      "learning_rate": 1.1830219032297984e-05,
      "loss": 9.6123,
      "step": 6170
    },
    {
      "epoch": 0.7641381017200841,
      "grad_norm": 0.8205949664115906,
      "learning_rate": 1.1799282267046157e-05,
      "loss": 9.6237,
      "step": 6175
    },
    {
      "epoch": 0.7647568370251207,
      "grad_norm": 0.8363347053527832,
      "learning_rate": 1.1768345501794333e-05,
      "loss": 9.6128,
      "step": 6180
    },
    {
      "epoch": 0.7653755723301572,
      "grad_norm": 0.8260341286659241,
      "learning_rate": 1.1737408736542508e-05,
      "loss": 9.6133,
      "step": 6185
    },
    {
      "epoch": 0.7659943076351937,
      "grad_norm": 0.8183020353317261,
      "learning_rate": 1.1706471971290683e-05,
      "loss": 9.6193,
      "step": 6190
    },
    {
      "epoch": 0.7666130429402301,
      "grad_norm": 0.8382743000984192,
      "learning_rate": 1.1675535206038856e-05,
      "loss": 9.615,
      "step": 6195
    },
    {
      "epoch": 0.7672317782452667,
      "grad_norm": 0.8123548030853271,
      "learning_rate": 1.1644598440787031e-05,
      "loss": 9.6311,
      "step": 6200
    },
    {
      "epoch": 0.7678505135503032,
      "grad_norm": 0.8242918848991394,
      "learning_rate": 1.1613661675535207e-05,
      "loss": 9.621,
      "step": 6205
    },
    {
      "epoch": 0.7684692488553397,
      "grad_norm": 0.8341015577316284,
      "learning_rate": 1.1582724910283382e-05,
      "loss": 9.5936,
      "step": 6210
    },
    {
      "epoch": 0.7690879841603762,
      "grad_norm": 0.8286389708518982,
      "learning_rate": 1.1551788145031555e-05,
      "loss": 9.6242,
      "step": 6215
    },
    {
      "epoch": 0.7697067194654127,
      "grad_norm": 0.8197998404502869,
      "learning_rate": 1.152085137977973e-05,
      "loss": 9.6171,
      "step": 6220
    },
    {
      "epoch": 0.7703254547704492,
      "grad_norm": 0.8195252418518066,
      "learning_rate": 1.1489914614527905e-05,
      "loss": 9.6285,
      "step": 6225
    },
    {
      "epoch": 0.7709441900754858,
      "grad_norm": 0.8268277645111084,
      "learning_rate": 1.145897784927608e-05,
      "loss": 9.6206,
      "step": 6230
    },
    {
      "epoch": 0.7715629253805222,
      "grad_norm": 0.8538702130317688,
      "learning_rate": 1.1428041084024254e-05,
      "loss": 9.6205,
      "step": 6235
    },
    {
      "epoch": 0.7721816606855587,
      "grad_norm": 0.8047409653663635,
      "learning_rate": 1.1397104318772429e-05,
      "loss": 9.6364,
      "step": 6240
    },
    {
      "epoch": 0.7728003959905952,
      "grad_norm": 0.816850483417511,
      "learning_rate": 1.1366167553520604e-05,
      "loss": 9.6181,
      "step": 6245
    },
    {
      "epoch": 0.7734191312956318,
      "grad_norm": 0.7933956384658813,
      "learning_rate": 1.133523078826878e-05,
      "loss": 9.6216,
      "step": 6250
    },
    {
      "epoch": 0.7740378666006682,
      "grad_norm": 0.8171630501747131,
      "learning_rate": 1.1304294023016953e-05,
      "loss": 9.6255,
      "step": 6255
    },
    {
      "epoch": 0.7746566019057047,
      "grad_norm": 0.8147255182266235,
      "learning_rate": 1.1273357257765128e-05,
      "loss": 9.6246,
      "step": 6260
    },
    {
      "epoch": 0.7752753372107413,
      "grad_norm": 0.8280456066131592,
      "learning_rate": 1.1242420492513303e-05,
      "loss": 9.6088,
      "step": 6265
    },
    {
      "epoch": 0.7758940725157778,
      "grad_norm": 0.8243122696876526,
      "learning_rate": 1.1211483727261478e-05,
      "loss": 9.6159,
      "step": 6270
    },
    {
      "epoch": 0.7765128078208142,
      "grad_norm": 0.825667679309845,
      "learning_rate": 1.1180546962009652e-05,
      "loss": 9.6347,
      "step": 6275
    },
    {
      "epoch": 0.7771315431258508,
      "grad_norm": 0.8302186131477356,
      "learning_rate": 1.1149610196757827e-05,
      "loss": 9.6071,
      "step": 6280
    },
    {
      "epoch": 0.7777502784308873,
      "grad_norm": 0.8423643112182617,
      "learning_rate": 1.1118673431506002e-05,
      "loss": 9.6025,
      "step": 6285
    },
    {
      "epoch": 0.7783690137359238,
      "grad_norm": 0.8313718438148499,
      "learning_rate": 1.1087736666254177e-05,
      "loss": 9.6345,
      "step": 6290
    },
    {
      "epoch": 0.7789877490409602,
      "grad_norm": 0.8150802254676819,
      "learning_rate": 1.1056799901002352e-05,
      "loss": 9.6143,
      "step": 6295
    },
    {
      "epoch": 0.7796064843459968,
      "grad_norm": 0.8146603107452393,
      "learning_rate": 1.1025863135750526e-05,
      "loss": 9.6209,
      "step": 6300
    },
    {
      "epoch": 0.7802252196510333,
      "grad_norm": 0.812252402305603,
      "learning_rate": 1.0994926370498701e-05,
      "loss": 9.6242,
      "step": 6305
    },
    {
      "epoch": 0.7808439549560698,
      "grad_norm": 0.8304713368415833,
      "learning_rate": 1.0963989605246876e-05,
      "loss": 9.6101,
      "step": 6310
    },
    {
      "epoch": 0.7814626902611063,
      "grad_norm": 0.8346320986747742,
      "learning_rate": 1.0933052839995051e-05,
      "loss": 9.5999,
      "step": 6315
    },
    {
      "epoch": 0.7820814255661428,
      "grad_norm": 0.8363698124885559,
      "learning_rate": 1.0902116074743225e-05,
      "loss": 9.6096,
      "step": 6320
    },
    {
      "epoch": 0.7827001608711793,
      "grad_norm": 0.7804926633834839,
      "learning_rate": 1.08711793094914e-05,
      "loss": 9.6211,
      "step": 6325
    },
    {
      "epoch": 0.7833188961762159,
      "grad_norm": 0.8355554342269897,
      "learning_rate": 1.0840242544239575e-05,
      "loss": 9.6313,
      "step": 6330
    },
    {
      "epoch": 0.7839376314812523,
      "grad_norm": 0.8309280276298523,
      "learning_rate": 1.080930577898775e-05,
      "loss": 9.6061,
      "step": 6335
    },
    {
      "epoch": 0.7845563667862888,
      "grad_norm": 0.8317446112632751,
      "learning_rate": 1.0778369013735924e-05,
      "loss": 9.6192,
      "step": 6340
    },
    {
      "epoch": 0.7851751020913253,
      "grad_norm": 0.8058114051818848,
      "learning_rate": 1.0747432248484099e-05,
      "loss": 9.6087,
      "step": 6345
    },
    {
      "epoch": 0.7857938373963619,
      "grad_norm": 0.8264184594154358,
      "learning_rate": 1.0716495483232274e-05,
      "loss": 9.6014,
      "step": 6350
    },
    {
      "epoch": 0.7864125727013983,
      "grad_norm": 0.827539324760437,
      "learning_rate": 1.0685558717980449e-05,
      "loss": 9.6028,
      "step": 6355
    },
    {
      "epoch": 0.7870313080064348,
      "grad_norm": 0.8457391262054443,
      "learning_rate": 1.0654621952728622e-05,
      "loss": 9.5943,
      "step": 6360
    },
    {
      "epoch": 0.7876500433114714,
      "grad_norm": 0.8263046741485596,
      "learning_rate": 1.0623685187476798e-05,
      "loss": 9.601,
      "step": 6365
    },
    {
      "epoch": 0.7882687786165079,
      "grad_norm": 0.8045315742492676,
      "learning_rate": 1.0592748422224973e-05,
      "loss": 9.6064,
      "step": 6370
    },
    {
      "epoch": 0.7888875139215443,
      "grad_norm": 0.8199279308319092,
      "learning_rate": 1.0561811656973148e-05,
      "loss": 9.6085,
      "step": 6375
    },
    {
      "epoch": 0.7895062492265809,
      "grad_norm": 0.8137102723121643,
      "learning_rate": 1.0530874891721321e-05,
      "loss": 9.6173,
      "step": 6380
    },
    {
      "epoch": 0.7901249845316174,
      "grad_norm": 0.8144874572753906,
      "learning_rate": 1.0499938126469496e-05,
      "loss": 9.6021,
      "step": 6385
    },
    {
      "epoch": 0.7907437198366539,
      "grad_norm": 0.8318936824798584,
      "learning_rate": 1.0469001361217672e-05,
      "loss": 9.6168,
      "step": 6390
    },
    {
      "epoch": 0.7913624551416903,
      "grad_norm": 0.8031910061836243,
      "learning_rate": 1.0438064595965847e-05,
      "loss": 9.5996,
      "step": 6395
    },
    {
      "epoch": 0.7919811904467269,
      "grad_norm": 0.8271949887275696,
      "learning_rate": 1.040712783071402e-05,
      "loss": 9.5945,
      "step": 6400
    },
    {
      "epoch": 0.7925999257517634,
      "grad_norm": 0.8239979147911072,
      "learning_rate": 1.0376191065462195e-05,
      "loss": 9.6066,
      "step": 6405
    },
    {
      "epoch": 0.7932186610568,
      "grad_norm": 0.811976969242096,
      "learning_rate": 1.034525430021037e-05,
      "loss": 9.6119,
      "step": 6410
    },
    {
      "epoch": 0.7938373963618364,
      "grad_norm": 0.8199281692504883,
      "learning_rate": 1.0314317534958546e-05,
      "loss": 9.6173,
      "step": 6415
    },
    {
      "epoch": 0.7944561316668729,
      "grad_norm": 0.8165796399116516,
      "learning_rate": 1.028338076970672e-05,
      "loss": 9.593,
      "step": 6420
    },
    {
      "epoch": 0.7950748669719094,
      "grad_norm": 0.8111289143562317,
      "learning_rate": 1.0252444004454894e-05,
      "loss": 9.6009,
      "step": 6425
    },
    {
      "epoch": 0.795693602276946,
      "grad_norm": 0.8281963467597961,
      "learning_rate": 1.022150723920307e-05,
      "loss": 9.6008,
      "step": 6430
    },
    {
      "epoch": 0.7963123375819824,
      "grad_norm": 0.8201638460159302,
      "learning_rate": 1.0190570473951245e-05,
      "loss": 9.5968,
      "step": 6435
    },
    {
      "epoch": 0.7969310728870189,
      "grad_norm": 0.8006452322006226,
      "learning_rate": 1.015963370869942e-05,
      "loss": 9.6146,
      "step": 6440
    },
    {
      "epoch": 0.7975498081920555,
      "grad_norm": 0.8352183103561401,
      "learning_rate": 1.0128696943447593e-05,
      "loss": 9.5919,
      "step": 6445
    },
    {
      "epoch": 0.798168543497092,
      "grad_norm": 2.2156267166137695,
      "learning_rate": 1.0097760178195768e-05,
      "loss": 9.593,
      "step": 6450
    },
    {
      "epoch": 0.7987872788021284,
      "grad_norm": 0.8238091468811035,
      "learning_rate": 1.0066823412943943e-05,
      "loss": 9.5941,
      "step": 6455
    },
    {
      "epoch": 0.7994060141071649,
      "grad_norm": 0.8232970833778381,
      "learning_rate": 1.0035886647692119e-05,
      "loss": 9.5896,
      "step": 6460
    },
    {
      "epoch": 0.8000247494122015,
      "grad_norm": 0.8260982632637024,
      "learning_rate": 1.0004949882440292e-05,
      "loss": 9.6005,
      "step": 6465
    },
    {
      "epoch": 0.800643484717238,
      "grad_norm": 0.8525119423866272,
      "learning_rate": 9.974013117188467e-06,
      "loss": 9.6166,
      "step": 6470
    },
    {
      "epoch": 0.8012622200222744,
      "grad_norm": 0.8494219779968262,
      "learning_rate": 9.943076351936642e-06,
      "loss": 9.5999,
      "step": 6475
    },
    {
      "epoch": 0.801880955327311,
      "grad_norm": 0.826122522354126,
      "learning_rate": 9.912139586684817e-06,
      "loss": 9.6094,
      "step": 6480
    },
    {
      "epoch": 0.8024996906323475,
      "grad_norm": 0.8243784308433533,
      "learning_rate": 9.881202821432991e-06,
      "loss": 9.6127,
      "step": 6485
    },
    {
      "epoch": 0.803118425937384,
      "grad_norm": 0.8141985535621643,
      "learning_rate": 9.850266056181166e-06,
      "loss": 9.5993,
      "step": 6490
    },
    {
      "epoch": 0.8037371612424205,
      "grad_norm": 0.8172165155410767,
      "learning_rate": 9.819329290929341e-06,
      "loss": 9.6004,
      "step": 6495
    },
    {
      "epoch": 0.804355896547457,
      "grad_norm": 0.8259287476539612,
      "learning_rate": 9.788392525677516e-06,
      "loss": 9.6037,
      "step": 6500
    },
    {
      "epoch": 0.8049746318524935,
      "grad_norm": 0.8020278215408325,
      "learning_rate": 9.75745576042569e-06,
      "loss": 9.608,
      "step": 6505
    },
    {
      "epoch": 0.80559336715753,
      "grad_norm": 0.8151118159294128,
      "learning_rate": 9.726518995173865e-06,
      "loss": 9.6003,
      "step": 6510
    },
    {
      "epoch": 0.8062121024625665,
      "grad_norm": 0.9528477191925049,
      "learning_rate": 9.69558222992204e-06,
      "loss": 9.5647,
      "step": 6515
    },
    {
      "epoch": 0.806830837767603,
      "grad_norm": 0.8419329524040222,
      "learning_rate": 9.664645464670215e-06,
      "loss": 9.5938,
      "step": 6520
    },
    {
      "epoch": 0.8074495730726395,
      "grad_norm": 0.8640084862709045,
      "learning_rate": 9.63370869941839e-06,
      "loss": 9.6027,
      "step": 6525
    },
    {
      "epoch": 0.8080683083776761,
      "grad_norm": 0.8428340554237366,
      "learning_rate": 9.602771934166564e-06,
      "loss": 9.5979,
      "step": 6530
    },
    {
      "epoch": 0.8086870436827125,
      "grad_norm": 0.8379940390586853,
      "learning_rate": 9.571835168914739e-06,
      "loss": 9.5782,
      "step": 6535
    },
    {
      "epoch": 0.809305778987749,
      "grad_norm": 0.8137843608856201,
      "learning_rate": 9.540898403662914e-06,
      "loss": 9.6102,
      "step": 6540
    },
    {
      "epoch": 0.8099245142927856,
      "grad_norm": 0.8080351948738098,
      "learning_rate": 9.50996163841109e-06,
      "loss": 9.6111,
      "step": 6545
    },
    {
      "epoch": 0.8105432495978221,
      "grad_norm": 0.8478522300720215,
      "learning_rate": 9.479024873159263e-06,
      "loss": 9.5847,
      "step": 6550
    },
    {
      "epoch": 0.8111619849028585,
      "grad_norm": 0.8259920477867126,
      "learning_rate": 9.448088107907438e-06,
      "loss": 9.5964,
      "step": 6555
    },
    {
      "epoch": 0.811780720207895,
      "grad_norm": 0.8323947787284851,
      "learning_rate": 9.417151342655613e-06,
      "loss": 9.582,
      "step": 6560
    },
    {
      "epoch": 0.8123994555129316,
      "grad_norm": 0.8442746996879578,
      "learning_rate": 9.386214577403788e-06,
      "loss": 9.6021,
      "step": 6565
    },
    {
      "epoch": 0.8130181908179681,
      "grad_norm": 0.8361489176750183,
      "learning_rate": 9.355277812151962e-06,
      "loss": 9.5986,
      "step": 6570
    },
    {
      "epoch": 0.8136369261230045,
      "grad_norm": 0.8426061272621155,
      "learning_rate": 9.324341046900137e-06,
      "loss": 9.5782,
      "step": 6575
    },
    {
      "epoch": 0.8142556614280411,
      "grad_norm": 0.8546231985092163,
      "learning_rate": 9.293404281648312e-06,
      "loss": 9.5857,
      "step": 6580
    },
    {
      "epoch": 0.8148743967330776,
      "grad_norm": 0.8234230875968933,
      "learning_rate": 9.262467516396487e-06,
      "loss": 9.5984,
      "step": 6585
    },
    {
      "epoch": 0.8154931320381141,
      "grad_norm": 0.8583722114562988,
      "learning_rate": 9.23153075114466e-06,
      "loss": 9.6012,
      "step": 6590
    },
    {
      "epoch": 0.8161118673431506,
      "grad_norm": 0.8114727735519409,
      "learning_rate": 9.200593985892836e-06,
      "loss": 9.5977,
      "step": 6595
    },
    {
      "epoch": 0.8167306026481871,
      "grad_norm": 0.8446483016014099,
      "learning_rate": 9.16965722064101e-06,
      "loss": 9.6,
      "step": 6600
    },
    {
      "epoch": 0.8173493379532236,
      "grad_norm": 0.8565563559532166,
      "learning_rate": 9.138720455389186e-06,
      "loss": 9.585,
      "step": 6605
    },
    {
      "epoch": 0.8179680732582602,
      "grad_norm": 0.838373601436615,
      "learning_rate": 9.10778369013736e-06,
      "loss": 9.5864,
      "step": 6610
    },
    {
      "epoch": 0.8185868085632966,
      "grad_norm": 0.8409719467163086,
      "learning_rate": 9.076846924885534e-06,
      "loss": 9.5853,
      "step": 6615
    },
    {
      "epoch": 0.8192055438683331,
      "grad_norm": 0.8458887934684753,
      "learning_rate": 9.04591015963371e-06,
      "loss": 9.591,
      "step": 6620
    },
    {
      "epoch": 0.8198242791733696,
      "grad_norm": 0.8408207297325134,
      "learning_rate": 9.014973394381885e-06,
      "loss": 9.5783,
      "step": 6625
    },
    {
      "epoch": 0.8204430144784062,
      "grad_norm": 0.8228905200958252,
      "learning_rate": 8.984036629130058e-06,
      "loss": 9.5896,
      "step": 6630
    },
    {
      "epoch": 0.8210617497834426,
      "grad_norm": 0.8494939208030701,
      "learning_rate": 8.953099863878233e-06,
      "loss": 9.5936,
      "step": 6635
    },
    {
      "epoch": 0.8216804850884791,
      "grad_norm": 0.8296558856964111,
      "learning_rate": 8.922163098626408e-06,
      "loss": 9.5795,
      "step": 6640
    },
    {
      "epoch": 0.8222992203935157,
      "grad_norm": 0.8577395081520081,
      "learning_rate": 8.891226333374584e-06,
      "loss": 9.5871,
      "step": 6645
    },
    {
      "epoch": 0.8229179556985522,
      "grad_norm": 0.8023784756660461,
      "learning_rate": 8.860289568122759e-06,
      "loss": 9.613,
      "step": 6650
    },
    {
      "epoch": 0.8235366910035886,
      "grad_norm": 0.8634418249130249,
      "learning_rate": 8.829352802870932e-06,
      "loss": 9.6011,
      "step": 6655
    },
    {
      "epoch": 0.8241554263086251,
      "grad_norm": 0.8499588966369629,
      "learning_rate": 8.798416037619107e-06,
      "loss": 9.591,
      "step": 6660
    },
    {
      "epoch": 0.8247741616136617,
      "grad_norm": 0.8388671875,
      "learning_rate": 8.767479272367282e-06,
      "loss": 9.5935,
      "step": 6665
    },
    {
      "epoch": 0.8253928969186982,
      "grad_norm": 0.8531434535980225,
      "learning_rate": 8.736542507115458e-06,
      "loss": 9.5924,
      "step": 6670
    },
    {
      "epoch": 0.8260116322237346,
      "grad_norm": 0.841559648513794,
      "learning_rate": 8.705605741863631e-06,
      "loss": 9.5877,
      "step": 6675
    },
    {
      "epoch": 0.8266303675287712,
      "grad_norm": 0.8069517016410828,
      "learning_rate": 8.674668976611806e-06,
      "loss": 9.5971,
      "step": 6680
    },
    {
      "epoch": 0.8272491028338077,
      "grad_norm": 0.8102715611457825,
      "learning_rate": 8.643732211359981e-06,
      "loss": 9.611,
      "step": 6685
    },
    {
      "epoch": 0.8278678381388442,
      "grad_norm": 0.8605831861495972,
      "learning_rate": 8.612795446108157e-06,
      "loss": 9.576,
      "step": 6690
    },
    {
      "epoch": 0.8284865734438807,
      "grad_norm": 0.833431601524353,
      "learning_rate": 8.58185868085633e-06,
      "loss": 9.5771,
      "step": 6695
    },
    {
      "epoch": 0.8291053087489172,
      "grad_norm": 0.8394922614097595,
      "learning_rate": 8.550921915604505e-06,
      "loss": 9.5873,
      "step": 6700
    },
    {
      "epoch": 0.8297240440539537,
      "grad_norm": 0.8422120809555054,
      "learning_rate": 8.51998515035268e-06,
      "loss": 9.5861,
      "step": 6705
    },
    {
      "epoch": 0.8303427793589903,
      "grad_norm": 0.8754172325134277,
      "learning_rate": 8.489048385100855e-06,
      "loss": 9.5647,
      "step": 6710
    },
    {
      "epoch": 0.8309615146640267,
      "grad_norm": 0.8313160538673401,
      "learning_rate": 8.458111619849029e-06,
      "loss": 9.5929,
      "step": 6715
    },
    {
      "epoch": 0.8315802499690632,
      "grad_norm": 0.8665469884872437,
      "learning_rate": 8.427174854597204e-06,
      "loss": 9.5718,
      "step": 6720
    },
    {
      "epoch": 0.8321989852740997,
      "grad_norm": 0.8260572552680969,
      "learning_rate": 8.396238089345379e-06,
      "loss": 9.5945,
      "step": 6725
    },
    {
      "epoch": 0.8328177205791363,
      "grad_norm": 0.8581351041793823,
      "learning_rate": 8.365301324093554e-06,
      "loss": 9.5906,
      "step": 6730
    },
    {
      "epoch": 0.8334364558841727,
      "grad_norm": 0.9613292217254639,
      "learning_rate": 8.334364558841728e-06,
      "loss": 9.563,
      "step": 6735
    },
    {
      "epoch": 0.8340551911892092,
      "grad_norm": 0.859661340713501,
      "learning_rate": 8.303427793589903e-06,
      "loss": 9.5699,
      "step": 6740
    },
    {
      "epoch": 0.8346739264942458,
      "grad_norm": 0.8400396704673767,
      "learning_rate": 8.272491028338078e-06,
      "loss": 9.5969,
      "step": 6745
    },
    {
      "epoch": 0.8352926617992823,
      "grad_norm": 0.8328942060470581,
      "learning_rate": 8.241554263086253e-06,
      "loss": 9.5939,
      "step": 6750
    },
    {
      "epoch": 0.8359113971043187,
      "grad_norm": 0.8520841598510742,
      "learning_rate": 8.210617497834428e-06,
      "loss": 9.5799,
      "step": 6755
    },
    {
      "epoch": 0.8365301324093553,
      "grad_norm": 0.8364163041114807,
      "learning_rate": 8.179680732582602e-06,
      "loss": 9.5871,
      "step": 6760
    },
    {
      "epoch": 0.8371488677143918,
      "grad_norm": 0.8301446437835693,
      "learning_rate": 8.148743967330777e-06,
      "loss": 9.587,
      "step": 6765
    },
    {
      "epoch": 0.8377676030194283,
      "grad_norm": 0.8400984406471252,
      "learning_rate": 8.117807202078952e-06,
      "loss": 9.561,
      "step": 6770
    },
    {
      "epoch": 0.8383863383244647,
      "grad_norm": 0.8592439293861389,
      "learning_rate": 8.086870436827127e-06,
      "loss": 9.5817,
      "step": 6775
    },
    {
      "epoch": 0.8390050736295013,
      "grad_norm": 0.854843020439148,
      "learning_rate": 8.0559336715753e-06,
      "loss": 9.5856,
      "step": 6780
    },
    {
      "epoch": 0.8396238089345378,
      "grad_norm": 0.8520979285240173,
      "learning_rate": 8.024996906323476e-06,
      "loss": 9.5853,
      "step": 6785
    },
    {
      "epoch": 0.8402425442395743,
      "grad_norm": 0.8223438262939453,
      "learning_rate": 7.99406014107165e-06,
      "loss": 9.5881,
      "step": 6790
    },
    {
      "epoch": 0.8408612795446108,
      "grad_norm": 0.8293606042861938,
      "learning_rate": 7.963123375819824e-06,
      "loss": 9.6071,
      "step": 6795
    },
    {
      "epoch": 0.8414800148496473,
      "grad_norm": 0.8437837958335876,
      "learning_rate": 7.932186610568e-06,
      "loss": 9.5739,
      "step": 6800
    },
    {
      "epoch": 0.8420987501546838,
      "grad_norm": 0.8350732326507568,
      "learning_rate": 7.901249845316173e-06,
      "loss": 9.5888,
      "step": 6805
    },
    {
      "epoch": 0.8427174854597204,
      "grad_norm": 0.8599924445152283,
      "learning_rate": 7.870313080064348e-06,
      "loss": 9.5769,
      "step": 6810
    },
    {
      "epoch": 0.8433362207647568,
      "grad_norm": 0.8293936252593994,
      "learning_rate": 7.839376314812523e-06,
      "loss": 9.5755,
      "step": 6815
    },
    {
      "epoch": 0.8439549560697933,
      "grad_norm": 0.8577421307563782,
      "learning_rate": 7.808439549560698e-06,
      "loss": 9.5792,
      "step": 6820
    },
    {
      "epoch": 0.8445736913748298,
      "grad_norm": 0.8645420670509338,
      "learning_rate": 7.777502784308872e-06,
      "loss": 9.5772,
      "step": 6825
    },
    {
      "epoch": 0.8451924266798664,
      "grad_norm": 0.8517225384712219,
      "learning_rate": 7.746566019057047e-06,
      "loss": 9.5819,
      "step": 6830
    },
    {
      "epoch": 0.8458111619849028,
      "grad_norm": 0.8519184589385986,
      "learning_rate": 7.715629253805222e-06,
      "loss": 9.5625,
      "step": 6835
    },
    {
      "epoch": 0.8464298972899393,
      "grad_norm": 0.828804075717926,
      "learning_rate": 7.684692488553397e-06,
      "loss": 9.5777,
      "step": 6840
    },
    {
      "epoch": 0.8470486325949759,
      "grad_norm": 0.8666483163833618,
      "learning_rate": 7.65375572330157e-06,
      "loss": 9.5565,
      "step": 6845
    },
    {
      "epoch": 0.8476673679000124,
      "grad_norm": 0.8275617957115173,
      "learning_rate": 7.622818958049746e-06,
      "loss": 9.583,
      "step": 6850
    },
    {
      "epoch": 0.8482861032050488,
      "grad_norm": 0.8454660177230835,
      "learning_rate": 7.591882192797921e-06,
      "loss": 9.5805,
      "step": 6855
    },
    {
      "epoch": 0.8489048385100854,
      "grad_norm": 0.830983579158783,
      "learning_rate": 7.560945427546095e-06,
      "loss": 9.5654,
      "step": 6860
    },
    {
      "epoch": 0.8495235738151219,
      "grad_norm": 0.852228045463562,
      "learning_rate": 7.5300086622942705e-06,
      "loss": 9.5729,
      "step": 6865
    },
    {
      "epoch": 0.8501423091201584,
      "grad_norm": 0.8179441690444946,
      "learning_rate": 7.499071897042445e-06,
      "loss": 9.5927,
      "step": 6870
    },
    {
      "epoch": 0.850761044425195,
      "grad_norm": 0.8495848178863525,
      "learning_rate": 7.46813513179062e-06,
      "loss": 9.5789,
      "step": 6875
    },
    {
      "epoch": 0.8513797797302314,
      "grad_norm": 0.8305089473724365,
      "learning_rate": 7.437198366538794e-06,
      "loss": 9.5838,
      "step": 6880
    },
    {
      "epoch": 0.8519985150352679,
      "grad_norm": 0.8524693846702576,
      "learning_rate": 7.406261601286969e-06,
      "loss": 9.558,
      "step": 6885
    },
    {
      "epoch": 0.8526172503403044,
      "grad_norm": 0.8553917407989502,
      "learning_rate": 7.375324836035144e-06,
      "loss": 9.585,
      "step": 6890
    },
    {
      "epoch": 0.853235985645341,
      "grad_norm": 0.8366822004318237,
      "learning_rate": 7.344388070783319e-06,
      "loss": 9.588,
      "step": 6895
    },
    {
      "epoch": 0.8538547209503774,
      "grad_norm": 0.8370919823646545,
      "learning_rate": 7.313451305531493e-06,
      "loss": 9.5975,
      "step": 6900
    },
    {
      "epoch": 0.8544734562554139,
      "grad_norm": 0.8429886698722839,
      "learning_rate": 7.282514540279668e-06,
      "loss": 9.5591,
      "step": 6905
    },
    {
      "epoch": 0.8550921915604505,
      "grad_norm": 0.8044723272323608,
      "learning_rate": 7.251577775027843e-06,
      "loss": 9.5744,
      "step": 6910
    },
    {
      "epoch": 0.855710926865487,
      "grad_norm": 0.8471161723136902,
      "learning_rate": 7.220641009776018e-06,
      "loss": 9.5546,
      "step": 6915
    },
    {
      "epoch": 0.8563296621705234,
      "grad_norm": 0.8231103420257568,
      "learning_rate": 7.189704244524193e-06,
      "loss": 9.5778,
      "step": 6920
    },
    {
      "epoch": 0.85694839747556,
      "grad_norm": 0.8373333811759949,
      "learning_rate": 7.158767479272367e-06,
      "loss": 9.5815,
      "step": 6925
    },
    {
      "epoch": 0.8575671327805965,
      "grad_norm": 0.8460178375244141,
      "learning_rate": 7.127830714020542e-06,
      "loss": 9.5802,
      "step": 6930
    },
    {
      "epoch": 0.858185868085633,
      "grad_norm": 0.850398063659668,
      "learning_rate": 7.0968939487687165e-06,
      "loss": 9.5876,
      "step": 6935
    },
    {
      "epoch": 0.8588046033906694,
      "grad_norm": 0.8245312571525574,
      "learning_rate": 7.065957183516892e-06,
      "loss": 9.5629,
      "step": 6940
    },
    {
      "epoch": 0.859423338695706,
      "grad_norm": 0.8634379506111145,
      "learning_rate": 7.035020418265066e-06,
      "loss": 9.5675,
      "step": 6945
    },
    {
      "epoch": 0.8600420740007425,
      "grad_norm": 0.8654093742370605,
      "learning_rate": 7.004083653013241e-06,
      "loss": 9.5773,
      "step": 6950
    },
    {
      "epoch": 0.860660809305779,
      "grad_norm": 0.8423557877540588,
      "learning_rate": 6.973146887761415e-06,
      "loss": 9.5724,
      "step": 6955
    },
    {
      "epoch": 0.8612795446108155,
      "grad_norm": 0.8543217778205872,
      "learning_rate": 6.9422101225095906e-06,
      "loss": 9.5764,
      "step": 6960
    },
    {
      "epoch": 0.861898279915852,
      "grad_norm": 0.8308379054069519,
      "learning_rate": 6.911273357257765e-06,
      "loss": 9.578,
      "step": 6965
    },
    {
      "epoch": 0.8625170152208885,
      "grad_norm": 0.8409302830696106,
      "learning_rate": 6.88033659200594e-06,
      "loss": 9.5671,
      "step": 6970
    },
    {
      "epoch": 0.8631357505259251,
      "grad_norm": 0.8474629521369934,
      "learning_rate": 6.849399826754114e-06,
      "loss": 9.5666,
      "step": 6975
    },
    {
      "epoch": 0.8637544858309615,
      "grad_norm": 0.8611764311790466,
      "learning_rate": 6.8184630615022894e-06,
      "loss": 9.5628,
      "step": 6980
    },
    {
      "epoch": 0.864373221135998,
      "grad_norm": 0.8523878455162048,
      "learning_rate": 6.787526296250464e-06,
      "loss": 9.5829,
      "step": 6985
    },
    {
      "epoch": 0.8649919564410345,
      "grad_norm": 0.8785322308540344,
      "learning_rate": 6.756589530998639e-06,
      "loss": 9.551,
      "step": 6990
    },
    {
      "epoch": 0.8656106917460711,
      "grad_norm": 0.8812420964241028,
      "learning_rate": 6.725652765746813e-06,
      "loss": 9.5601,
      "step": 6995
    },
    {
      "epoch": 0.8662294270511075,
      "grad_norm": 0.8663480877876282,
      "learning_rate": 6.694716000494988e-06,
      "loss": 9.5879,
      "step": 7000
    },
    {
      "epoch": 0.866848162356144,
      "grad_norm": 0.8492878675460815,
      "learning_rate": 6.663779235243163e-06,
      "loss": 9.5639,
      "step": 7005
    },
    {
      "epoch": 0.8674668976611806,
      "grad_norm": 0.8382119536399841,
      "learning_rate": 6.632842469991338e-06,
      "loss": 9.5675,
      "step": 7010
    },
    {
      "epoch": 0.8680856329662171,
      "grad_norm": 0.8494365811347961,
      "learning_rate": 6.601905704739512e-06,
      "loss": 9.5871,
      "step": 7015
    },
    {
      "epoch": 0.8687043682712535,
      "grad_norm": 0.8438649773597717,
      "learning_rate": 6.570968939487687e-06,
      "loss": 9.5642,
      "step": 7020
    },
    {
      "epoch": 0.8693231035762901,
      "grad_norm": 0.8302252888679504,
      "learning_rate": 6.540032174235862e-06,
      "loss": 9.5566,
      "step": 7025
    },
    {
      "epoch": 0.8699418388813266,
      "grad_norm": 0.8718971610069275,
      "learning_rate": 6.509095408984037e-06,
      "loss": 9.5571,
      "step": 7030
    },
    {
      "epoch": 0.8705605741863631,
      "grad_norm": 0.8752239346504211,
      "learning_rate": 6.478158643732212e-06,
      "loss": 9.5569,
      "step": 7035
    },
    {
      "epoch": 0.8711793094913995,
      "grad_norm": 0.8456215858459473,
      "learning_rate": 6.447221878480386e-06,
      "loss": 9.5683,
      "step": 7040
    },
    {
      "epoch": 0.8717980447964361,
      "grad_norm": 0.8554917573928833,
      "learning_rate": 6.416285113228561e-06,
      "loss": 9.5631,
      "step": 7045
    },
    {
      "epoch": 0.8724167801014726,
      "grad_norm": 0.8789622783660889,
      "learning_rate": 6.3853483479767355e-06,
      "loss": 9.5583,
      "step": 7050
    },
    {
      "epoch": 0.8730355154065091,
      "grad_norm": 0.8442721366882324,
      "learning_rate": 6.354411582724911e-06,
      "loss": 9.5679,
      "step": 7055
    },
    {
      "epoch": 0.8736542507115456,
      "grad_norm": 0.8710828423500061,
      "learning_rate": 6.323474817473085e-06,
      "loss": 9.56,
      "step": 7060
    },
    {
      "epoch": 0.8742729860165821,
      "grad_norm": 0.8583917021751404,
      "learning_rate": 6.29253805222126e-06,
      "loss": 9.5869,
      "step": 7065
    },
    {
      "epoch": 0.8748917213216186,
      "grad_norm": 0.829927921295166,
      "learning_rate": 6.261601286969434e-06,
      "loss": 9.5808,
      "step": 7070
    },
    {
      "epoch": 0.8755104566266552,
      "grad_norm": 0.8495274186134338,
      "learning_rate": 6.2306645217176096e-06,
      "loss": 9.5741,
      "step": 7075
    },
    {
      "epoch": 0.8761291919316916,
      "grad_norm": 0.8540933132171631,
      "learning_rate": 6.199727756465784e-06,
      "loss": 9.5568,
      "step": 7080
    },
    {
      "epoch": 0.8767479272367281,
      "grad_norm": 0.8371572494506836,
      "learning_rate": 6.168790991213959e-06,
      "loss": 9.571,
      "step": 7085
    },
    {
      "epoch": 0.8773666625417647,
      "grad_norm": 0.8559521436691284,
      "learning_rate": 6.137854225962133e-06,
      "loss": 9.5554,
      "step": 7090
    },
    {
      "epoch": 0.8779853978468012,
      "grad_norm": 0.829595148563385,
      "learning_rate": 6.1069174607103084e-06,
      "loss": 9.5868,
      "step": 7095
    },
    {
      "epoch": 0.8786041331518376,
      "grad_norm": 1.964314341545105,
      "learning_rate": 6.075980695458483e-06,
      "loss": 9.5616,
      "step": 7100
    },
    {
      "epoch": 0.8792228684568741,
      "grad_norm": 0.849597156047821,
      "learning_rate": 6.045043930206658e-06,
      "loss": 9.5573,
      "step": 7105
    },
    {
      "epoch": 0.8798416037619107,
      "grad_norm": 0.8521008491516113,
      "learning_rate": 6.014107164954832e-06,
      "loss": 9.5651,
      "step": 7110
    },
    {
      "epoch": 0.8804603390669472,
      "grad_norm": 0.8304376602172852,
      "learning_rate": 5.983170399703007e-06,
      "loss": 9.5637,
      "step": 7115
    },
    {
      "epoch": 0.8810790743719836,
      "grad_norm": 0.8433154821395874,
      "learning_rate": 5.952233634451182e-06,
      "loss": 9.5893,
      "step": 7120
    },
    {
      "epoch": 0.8816978096770202,
      "grad_norm": 0.8636220693588257,
      "learning_rate": 5.921296869199357e-06,
      "loss": 9.5405,
      "step": 7125
    },
    {
      "epoch": 0.8823165449820567,
      "grad_norm": 0.8467676639556885,
      "learning_rate": 5.890360103947531e-06,
      "loss": 9.5627,
      "step": 7130
    },
    {
      "epoch": 0.8829352802870932,
      "grad_norm": 0.8438928127288818,
      "learning_rate": 5.859423338695706e-06,
      "loss": 9.5724,
      "step": 7135
    },
    {
      "epoch": 0.8835540155921296,
      "grad_norm": 0.8560394644737244,
      "learning_rate": 5.828486573443881e-06,
      "loss": 9.5684,
      "step": 7140
    },
    {
      "epoch": 0.8841727508971662,
      "grad_norm": 0.8583905100822449,
      "learning_rate": 5.797549808192056e-06,
      "loss": 9.5564,
      "step": 7145
    },
    {
      "epoch": 0.8847914862022027,
      "grad_norm": 0.8394703269004822,
      "learning_rate": 5.766613042940231e-06,
      "loss": 9.5835,
      "step": 7150
    },
    {
      "epoch": 0.8854102215072392,
      "grad_norm": 3.2359535694122314,
      "learning_rate": 5.735676277688405e-06,
      "loss": 9.5618,
      "step": 7155
    },
    {
      "epoch": 0.8860289568122757,
      "grad_norm": 0.8159907460212708,
      "learning_rate": 5.70473951243658e-06,
      "loss": 9.58,
      "step": 7160
    },
    {
      "epoch": 0.8866476921173122,
      "grad_norm": 0.8430298566818237,
      "learning_rate": 5.6738027471847545e-06,
      "loss": 9.5671,
      "step": 7165
    },
    {
      "epoch": 0.8872664274223487,
      "grad_norm": 0.8602701425552368,
      "learning_rate": 5.64286598193293e-06,
      "loss": 9.5607,
      "step": 7170
    },
    {
      "epoch": 0.8878851627273853,
      "grad_norm": 0.8127551674842834,
      "learning_rate": 5.611929216681104e-06,
      "loss": 9.5843,
      "step": 7175
    },
    {
      "epoch": 0.8885038980324217,
      "grad_norm": 1.0697908401489258,
      "learning_rate": 5.580992451429279e-06,
      "loss": 9.5677,
      "step": 7180
    },
    {
      "epoch": 0.8891226333374582,
      "grad_norm": 0.8306682705879211,
      "learning_rate": 5.550055686177453e-06,
      "loss": 9.5634,
      "step": 7185
    },
    {
      "epoch": 0.8897413686424948,
      "grad_norm": 0.8738499879837036,
      "learning_rate": 5.5191189209256285e-06,
      "loss": 9.5608,
      "step": 7190
    },
    {
      "epoch": 0.8903601039475313,
      "grad_norm": 0.8599546551704407,
      "learning_rate": 5.488182155673803e-06,
      "loss": 9.5493,
      "step": 7195
    },
    {
      "epoch": 0.8909788392525677,
      "grad_norm": 2.896808624267578,
      "learning_rate": 5.457245390421978e-06,
      "loss": 9.5724,
      "step": 7200
    },
    {
      "epoch": 0.8915975745576042,
      "grad_norm": 0.9000993967056274,
      "learning_rate": 5.426308625170152e-06,
      "loss": 9.5539,
      "step": 7205
    },
    {
      "epoch": 0.8922163098626408,
      "grad_norm": 0.8605513572692871,
      "learning_rate": 5.3953718599183274e-06,
      "loss": 9.5651,
      "step": 7210
    },
    {
      "epoch": 0.8928350451676773,
      "grad_norm": 0.854853630065918,
      "learning_rate": 5.364435094666502e-06,
      "loss": 9.5743,
      "step": 7215
    },
    {
      "epoch": 0.8934537804727137,
      "grad_norm": 0.8386319875717163,
      "learning_rate": 5.333498329414677e-06,
      "loss": 9.5654,
      "step": 7220
    },
    {
      "epoch": 0.8940725157777503,
      "grad_norm": 0.8412799835205078,
      "learning_rate": 5.302561564162851e-06,
      "loss": 9.5825,
      "step": 7225
    },
    {
      "epoch": 0.8946912510827868,
      "grad_norm": 0.8164218068122864,
      "learning_rate": 5.271624798911026e-06,
      "loss": 9.5771,
      "step": 7230
    },
    {
      "epoch": 0.8953099863878233,
      "grad_norm": 0.8449985980987549,
      "learning_rate": 5.240688033659201e-06,
      "loss": 9.5462,
      "step": 7235
    },
    {
      "epoch": 0.8959287216928598,
      "grad_norm": 0.8366987109184265,
      "learning_rate": 5.209751268407376e-06,
      "loss": 9.556,
      "step": 7240
    },
    {
      "epoch": 0.8965474569978963,
      "grad_norm": 0.841218888759613,
      "learning_rate": 5.17881450315555e-06,
      "loss": 9.5751,
      "step": 7245
    },
    {
      "epoch": 0.8971661923029328,
      "grad_norm": 0.8509640693664551,
      "learning_rate": 5.147877737903725e-06,
      "loss": 9.5593,
      "step": 7250
    },
    {
      "epoch": 0.8977849276079694,
      "grad_norm": 0.8588118553161621,
      "learning_rate": 5.1169409726519e-06,
      "loss": 9.5615,
      "step": 7255
    },
    {
      "epoch": 0.8984036629130058,
      "grad_norm": 0.8259632587432861,
      "learning_rate": 5.086004207400075e-06,
      "loss": 9.5779,
      "step": 7260
    },
    {
      "epoch": 0.8990223982180423,
      "grad_norm": 0.8384994864463806,
      "learning_rate": 5.05506744214825e-06,
      "loss": 9.5587,
      "step": 7265
    },
    {
      "epoch": 0.8996411335230788,
      "grad_norm": 0.871748149394989,
      "learning_rate": 5.024130676896424e-06,
      "loss": 9.5475,
      "step": 7270
    },
    {
      "epoch": 0.9002598688281154,
      "grad_norm": 0.8459409475326538,
      "learning_rate": 4.993193911644599e-06,
      "loss": 9.5534,
      "step": 7275
    },
    {
      "epoch": 0.9008786041331518,
      "grad_norm": 0.8214849233627319,
      "learning_rate": 4.9622571463927735e-06,
      "loss": 9.5723,
      "step": 7280
    },
    {
      "epoch": 0.9014973394381883,
      "grad_norm": 0.8607012629508972,
      "learning_rate": 4.931320381140949e-06,
      "loss": 9.5827,
      "step": 7285
    },
    {
      "epoch": 0.9021160747432249,
      "grad_norm": 0.839544951915741,
      "learning_rate": 4.900383615889123e-06,
      "loss": 9.5636,
      "step": 7290
    },
    {
      "epoch": 0.9027348100482614,
      "grad_norm": 0.8360326290130615,
      "learning_rate": 4.869446850637298e-06,
      "loss": 9.5624,
      "step": 7295
    },
    {
      "epoch": 0.9033535453532978,
      "grad_norm": 0.8650000095367432,
      "learning_rate": 4.838510085385472e-06,
      "loss": 9.5534,
      "step": 7300
    },
    {
      "epoch": 0.9039722806583343,
      "grad_norm": 0.8444043397903442,
      "learning_rate": 4.8075733201336475e-06,
      "loss": 9.565,
      "step": 7305
    },
    {
      "epoch": 0.9045910159633709,
      "grad_norm": 0.8249734044075012,
      "learning_rate": 4.776636554881822e-06,
      "loss": 9.572,
      "step": 7310
    },
    {
      "epoch": 0.9052097512684074,
      "grad_norm": 0.8596587777137756,
      "learning_rate": 4.745699789629997e-06,
      "loss": 9.5529,
      "step": 7315
    },
    {
      "epoch": 0.9058284865734438,
      "grad_norm": 0.8576188683509827,
      "learning_rate": 4.714763024378171e-06,
      "loss": 9.5536,
      "step": 7320
    },
    {
      "epoch": 0.9064472218784804,
      "grad_norm": 0.8789446353912354,
      "learning_rate": 4.683826259126346e-06,
      "loss": 9.5627,
      "step": 7325
    },
    {
      "epoch": 0.9070659571835169,
      "grad_norm": 0.8512715101242065,
      "learning_rate": 4.652889493874521e-06,
      "loss": 9.5442,
      "step": 7330
    },
    {
      "epoch": 0.9076846924885534,
      "grad_norm": 0.8577072620391846,
      "learning_rate": 4.621952728622696e-06,
      "loss": 9.5588,
      "step": 7335
    },
    {
      "epoch": 0.9083034277935899,
      "grad_norm": 0.8455462455749512,
      "learning_rate": 4.59101596337087e-06,
      "loss": 9.5492,
      "step": 7340
    },
    {
      "epoch": 0.9089221630986264,
      "grad_norm": 0.8655344843864441,
      "learning_rate": 4.560079198119045e-06,
      "loss": 9.5517,
      "step": 7345
    },
    {
      "epoch": 0.9095408984036629,
      "grad_norm": 0.8772990107536316,
      "learning_rate": 4.52914243286722e-06,
      "loss": 9.5638,
      "step": 7350
    },
    {
      "epoch": 0.9101596337086995,
      "grad_norm": 0.8801037073135376,
      "learning_rate": 4.498205667615395e-06,
      "loss": 9.5598,
      "step": 7355
    },
    {
      "epoch": 0.9107783690137359,
      "grad_norm": 0.8378186225891113,
      "learning_rate": 4.467268902363569e-06,
      "loss": 9.5842,
      "step": 7360
    },
    {
      "epoch": 0.9113971043187724,
      "grad_norm": 0.8396189212799072,
      "learning_rate": 4.436332137111743e-06,
      "loss": 9.5514,
      "step": 7365
    },
    {
      "epoch": 0.9120158396238089,
      "grad_norm": 0.8609519600868225,
      "learning_rate": 4.4053953718599185e-06,
      "loss": 9.5398,
      "step": 7370
    },
    {
      "epoch": 0.9126345749288455,
      "grad_norm": 0.8641504645347595,
      "learning_rate": 4.374458606608093e-06,
      "loss": 9.5761,
      "step": 7375
    },
    {
      "epoch": 0.9132533102338819,
      "grad_norm": 0.8252928853034973,
      "learning_rate": 4.343521841356268e-06,
      "loss": 9.5544,
      "step": 7380
    },
    {
      "epoch": 0.9138720455389184,
      "grad_norm": 0.847283661365509,
      "learning_rate": 4.312585076104442e-06,
      "loss": 9.5483,
      "step": 7385
    },
    {
      "epoch": 0.914490780843955,
      "grad_norm": 0.8685345649719238,
      "learning_rate": 4.281648310852617e-06,
      "loss": 9.5587,
      "step": 7390
    },
    {
      "epoch": 0.9151095161489915,
      "grad_norm": 0.8540006875991821,
      "learning_rate": 4.250711545600792e-06,
      "loss": 9.5639,
      "step": 7395
    },
    {
      "epoch": 0.9157282514540279,
      "grad_norm": 0.838589072227478,
      "learning_rate": 4.219774780348967e-06,
      "loss": 9.5599,
      "step": 7400
    },
    {
      "epoch": 0.9163469867590645,
      "grad_norm": 0.8509452939033508,
      "learning_rate": 4.188838015097141e-06,
      "loss": 9.5697,
      "step": 7405
    },
    {
      "epoch": 0.916965722064101,
      "grad_norm": 0.8626502156257629,
      "learning_rate": 4.157901249845316e-06,
      "loss": 9.546,
      "step": 7410
    },
    {
      "epoch": 0.9175844573691375,
      "grad_norm": 0.8504093885421753,
      "learning_rate": 4.1269644845934905e-06,
      "loss": 9.5583,
      "step": 7415
    },
    {
      "epoch": 0.9182031926741739,
      "grad_norm": 0.8614107966423035,
      "learning_rate": 4.096027719341666e-06,
      "loss": 9.5548,
      "step": 7420
    },
    {
      "epoch": 0.9188219279792105,
      "grad_norm": 0.8298864364624023,
      "learning_rate": 4.06509095408984e-06,
      "loss": 9.5739,
      "step": 7425
    },
    {
      "epoch": 0.919440663284247,
      "grad_norm": 0.8360140323638916,
      "learning_rate": 4.034154188838015e-06,
      "loss": 9.5576,
      "step": 7430
    },
    {
      "epoch": 0.9200593985892835,
      "grad_norm": 0.8546193838119507,
      "learning_rate": 4.003217423586189e-06,
      "loss": 9.5566,
      "step": 7435
    },
    {
      "epoch": 0.92067813389432,
      "grad_norm": 0.8590888381004333,
      "learning_rate": 3.9722806583343646e-06,
      "loss": 9.5581,
      "step": 7440
    },
    {
      "epoch": 0.9212968691993565,
      "grad_norm": 0.8739840388298035,
      "learning_rate": 3.94134389308254e-06,
      "loss": 9.5657,
      "step": 7445
    },
    {
      "epoch": 0.921915604504393,
      "grad_norm": 0.8539726734161377,
      "learning_rate": 3.910407127830714e-06,
      "loss": 9.5523,
      "step": 7450
    },
    {
      "epoch": 0.9225343398094296,
      "grad_norm": 0.8543306589126587,
      "learning_rate": 3.879470362578889e-06,
      "loss": 9.5665,
      "step": 7455
    },
    {
      "epoch": 0.923153075114466,
      "grad_norm": 0.8490535020828247,
      "learning_rate": 3.8485335973270634e-06,
      "loss": 9.55,
      "step": 7460
    },
    {
      "epoch": 0.9237718104195025,
      "grad_norm": 0.8416416645050049,
      "learning_rate": 3.817596832075239e-06,
      "loss": 9.549,
      "step": 7465
    },
    {
      "epoch": 0.924390545724539,
      "grad_norm": 0.8682522773742676,
      "learning_rate": 3.786660066823413e-06,
      "loss": 9.5537,
      "step": 7470
    },
    {
      "epoch": 0.9250092810295756,
      "grad_norm": 0.8527073860168457,
      "learning_rate": 3.7557233015715876e-06,
      "loss": 9.5564,
      "step": 7475
    },
    {
      "epoch": 0.925628016334612,
      "grad_norm": 0.849097728729248,
      "learning_rate": 3.7247865363197623e-06,
      "loss": 9.565,
      "step": 7480
    },
    {
      "epoch": 0.9262467516396485,
      "grad_norm": 0.8636399507522583,
      "learning_rate": 3.693849771067937e-06,
      "loss": 9.5472,
      "step": 7485
    },
    {
      "epoch": 0.9268654869446851,
      "grad_norm": 0.8764976859092712,
      "learning_rate": 3.6629130058161118e-06,
      "loss": 9.5465,
      "step": 7490
    },
    {
      "epoch": 0.9274842222497216,
      "grad_norm": 0.8520698547363281,
      "learning_rate": 3.6319762405642865e-06,
      "loss": 9.5585,
      "step": 7495
    },
    {
      "epoch": 0.928102957554758,
      "grad_norm": 0.8509013056755066,
      "learning_rate": 3.6010394753124616e-06,
      "loss": 9.5681,
      "step": 7500
    },
    {
      "epoch": 0.9287216928597946,
      "grad_norm": 0.8489198088645935,
      "learning_rate": 3.5701027100606363e-06,
      "loss": 9.5468,
      "step": 7505
    },
    {
      "epoch": 0.9293404281648311,
      "grad_norm": 0.8778402805328369,
      "learning_rate": 3.539165944808811e-06,
      "loss": 9.5536,
      "step": 7510
    },
    {
      "epoch": 0.9299591634698676,
      "grad_norm": 0.8627766370773315,
      "learning_rate": 3.5082291795569858e-06,
      "loss": 9.5521,
      "step": 7515
    },
    {
      "epoch": 0.930577898774904,
      "grad_norm": 0.8562147617340088,
      "learning_rate": 3.4772924143051605e-06,
      "loss": 9.5602,
      "step": 7520
    },
    {
      "epoch": 0.9311966340799406,
      "grad_norm": 0.8419661521911621,
      "learning_rate": 3.4463556490533352e-06,
      "loss": 9.5467,
      "step": 7525
    },
    {
      "epoch": 0.9318153693849771,
      "grad_norm": 0.8696706295013428,
      "learning_rate": 3.41541888380151e-06,
      "loss": 9.5343,
      "step": 7530
    },
    {
      "epoch": 0.9324341046900136,
      "grad_norm": 0.8663533926010132,
      "learning_rate": 3.3844821185496847e-06,
      "loss": 9.5435,
      "step": 7535
    },
    {
      "epoch": 0.9330528399950501,
      "grad_norm": 0.8408675193786621,
      "learning_rate": 3.3535453532978594e-06,
      "loss": 9.5623,
      "step": 7540
    },
    {
      "epoch": 0.9336715753000866,
      "grad_norm": 0.8580951690673828,
      "learning_rate": 3.322608588046034e-06,
      "loss": 9.5542,
      "step": 7545
    },
    {
      "epoch": 0.9342903106051231,
      "grad_norm": 0.8610774874687195,
      "learning_rate": 3.291671822794209e-06,
      "loss": 9.5557,
      "step": 7550
    },
    {
      "epoch": 0.9349090459101597,
      "grad_norm": 0.8641268014907837,
      "learning_rate": 3.2607350575423835e-06,
      "loss": 9.5515,
      "step": 7555
    },
    {
      "epoch": 0.9355277812151961,
      "grad_norm": 0.8437345027923584,
      "learning_rate": 3.2297982922905583e-06,
      "loss": 9.5487,
      "step": 7560
    },
    {
      "epoch": 0.9361465165202326,
      "grad_norm": 0.8437366485595703,
      "learning_rate": 3.198861527038733e-06,
      "loss": 9.5562,
      "step": 7565
    },
    {
      "epoch": 0.9367652518252692,
      "grad_norm": 0.8497097492218018,
      "learning_rate": 3.1679247617869077e-06,
      "loss": 9.5449,
      "step": 7570
    },
    {
      "epoch": 0.9373839871303057,
      "grad_norm": 0.8453940153121948,
      "learning_rate": 3.1369879965350824e-06,
      "loss": 9.5568,
      "step": 7575
    },
    {
      "epoch": 0.9380027224353421,
      "grad_norm": 0.8423145413398743,
      "learning_rate": 3.106051231283257e-06,
      "loss": 9.5444,
      "step": 7580
    },
    {
      "epoch": 0.9386214577403786,
      "grad_norm": 0.8437274694442749,
      "learning_rate": 3.075114466031432e-06,
      "loss": 9.5463,
      "step": 7585
    },
    {
      "epoch": 0.9392401930454152,
      "grad_norm": 0.8295159339904785,
      "learning_rate": 3.0441777007796066e-06,
      "loss": 9.5574,
      "step": 7590
    },
    {
      "epoch": 0.9398589283504517,
      "grad_norm": 0.8344331383705139,
      "learning_rate": 3.0132409355277813e-06,
      "loss": 9.5561,
      "step": 7595
    },
    {
      "epoch": 0.9404776636554881,
      "grad_norm": 0.8410665988922119,
      "learning_rate": 2.982304170275956e-06,
      "loss": 9.5682,
      "step": 7600
    },
    {
      "epoch": 0.9410963989605247,
      "grad_norm": 0.846430778503418,
      "learning_rate": 2.9513674050241308e-06,
      "loss": 9.5596,
      "step": 7605
    },
    {
      "epoch": 0.9417151342655612,
      "grad_norm": 0.8712752461433411,
      "learning_rate": 2.9204306397723055e-06,
      "loss": 9.5565,
      "step": 7610
    },
    {
      "epoch": 0.9423338695705977,
      "grad_norm": 0.8481011986732483,
      "learning_rate": 2.88949387452048e-06,
      "loss": 9.5422,
      "step": 7615
    },
    {
      "epoch": 0.9429526048756341,
      "grad_norm": 0.8604541420936584,
      "learning_rate": 2.8585571092686553e-06,
      "loss": 9.5617,
      "step": 7620
    },
    {
      "epoch": 0.9435713401806707,
      "grad_norm": 0.8524484038352966,
      "learning_rate": 2.82762034401683e-06,
      "loss": 9.5558,
      "step": 7625
    },
    {
      "epoch": 0.9441900754857072,
      "grad_norm": 0.8421114683151245,
      "learning_rate": 2.7966835787650048e-06,
      "loss": 9.5511,
      "step": 7630
    },
    {
      "epoch": 0.9448088107907437,
      "grad_norm": 0.8662052154541016,
      "learning_rate": 2.7657468135131795e-06,
      "loss": 9.5392,
      "step": 7635
    },
    {
      "epoch": 0.9454275460957803,
      "grad_norm": 0.8344717025756836,
      "learning_rate": 2.7348100482613542e-06,
      "loss": 9.5606,
      "step": 7640
    },
    {
      "epoch": 0.9460462814008167,
      "grad_norm": 0.8489993810653687,
      "learning_rate": 2.7038732830095285e-06,
      "loss": 9.5632,
      "step": 7645
    },
    {
      "epoch": 0.9466650167058532,
      "grad_norm": 0.8605545163154602,
      "learning_rate": 2.6729365177577032e-06,
      "loss": 9.5319,
      "step": 7650
    },
    {
      "epoch": 0.9472837520108898,
      "grad_norm": 0.8523867130279541,
      "learning_rate": 2.641999752505878e-06,
      "loss": 9.5536,
      "step": 7655
    },
    {
      "epoch": 0.9479024873159263,
      "grad_norm": 3.271418571472168,
      "learning_rate": 2.6110629872540527e-06,
      "loss": 9.5548,
      "step": 7660
    },
    {
      "epoch": 0.9485212226209627,
      "grad_norm": 0.8632397651672363,
      "learning_rate": 2.5801262220022274e-06,
      "loss": 9.5481,
      "step": 7665
    },
    {
      "epoch": 0.9491399579259993,
      "grad_norm": 0.8632024526596069,
      "learning_rate": 2.549189456750402e-06,
      "loss": 9.5437,
      "step": 7670
    },
    {
      "epoch": 0.9497586932310358,
      "grad_norm": 0.8599038124084473,
      "learning_rate": 2.518252691498577e-06,
      "loss": 9.5449,
      "step": 7675
    },
    {
      "epoch": 0.9503774285360723,
      "grad_norm": 0.8594868183135986,
      "learning_rate": 2.4873159262467516e-06,
      "loss": 9.5375,
      "step": 7680
    },
    {
      "epoch": 0.9509961638411087,
      "grad_norm": 0.8625823855400085,
      "learning_rate": 2.4563791609949263e-06,
      "loss": 9.54,
      "step": 7685
    },
    {
      "epoch": 0.9516148991461453,
      "grad_norm": 0.8637988567352295,
      "learning_rate": 2.425442395743101e-06,
      "loss": 9.5554,
      "step": 7690
    },
    {
      "epoch": 0.9522336344511818,
      "grad_norm": 0.8376935720443726,
      "learning_rate": 2.3945056304912757e-06,
      "loss": 9.5569,
      "step": 7695
    },
    {
      "epoch": 0.9528523697562183,
      "grad_norm": 0.8475950360298157,
      "learning_rate": 2.3635688652394504e-06,
      "loss": 9.5566,
      "step": 7700
    },
    {
      "epoch": 0.9534711050612548,
      "grad_norm": 0.8420981168746948,
      "learning_rate": 2.332632099987625e-06,
      "loss": 9.5506,
      "step": 7705
    },
    {
      "epoch": 0.9540898403662913,
      "grad_norm": 0.8520025014877319,
      "learning_rate": 2.3016953347358e-06,
      "loss": 9.5481,
      "step": 7710
    },
    {
      "epoch": 0.9547085756713278,
      "grad_norm": 0.8557135462760925,
      "learning_rate": 2.270758569483975e-06,
      "loss": 9.5488,
      "step": 7715
    },
    {
      "epoch": 0.9553273109763644,
      "grad_norm": 0.8255517482757568,
      "learning_rate": 2.2398218042321497e-06,
      "loss": 9.5777,
      "step": 7720
    },
    {
      "epoch": 0.9559460462814008,
      "grad_norm": 0.8657057881355286,
      "learning_rate": 2.2088850389803245e-06,
      "loss": 9.552,
      "step": 7725
    },
    {
      "epoch": 0.9565647815864373,
      "grad_norm": 0.8666863441467285,
      "learning_rate": 2.177948273728499e-06,
      "loss": 9.5622,
      "step": 7730
    },
    {
      "epoch": 0.9571835168914739,
      "grad_norm": 0.841400682926178,
      "learning_rate": 2.147011508476674e-06,
      "loss": 9.5555,
      "step": 7735
    },
    {
      "epoch": 0.9578022521965104,
      "grad_norm": 0.8741658329963684,
      "learning_rate": 2.1160747432248486e-06,
      "loss": 9.5454,
      "step": 7740
    },
    {
      "epoch": 0.9584209875015468,
      "grad_norm": 0.8637559413909912,
      "learning_rate": 2.0851379779730233e-06,
      "loss": 9.544,
      "step": 7745
    },
    {
      "epoch": 0.9590397228065833,
      "grad_norm": 0.8626166582107544,
      "learning_rate": 2.054201212721198e-06,
      "loss": 9.5641,
      "step": 7750
    },
    {
      "epoch": 0.9596584581116199,
      "grad_norm": 0.8602121472358704,
      "learning_rate": 2.0232644474693728e-06,
      "loss": 9.5459,
      "step": 7755
    },
    {
      "epoch": 0.9602771934166564,
      "grad_norm": 0.8730457425117493,
      "learning_rate": 1.9923276822175475e-06,
      "loss": 9.539,
      "step": 7760
    },
    {
      "epoch": 0.9608959287216928,
      "grad_norm": 0.8451535105705261,
      "learning_rate": 1.9613909169657222e-06,
      "loss": 9.5526,
      "step": 7765
    },
    {
      "epoch": 0.9615146640267294,
      "grad_norm": 0.8453549146652222,
      "learning_rate": 1.930454151713897e-06,
      "loss": 9.5691,
      "step": 7770
    },
    {
      "epoch": 0.9621333993317659,
      "grad_norm": 0.8932628631591797,
      "learning_rate": 1.8995173864620717e-06,
      "loss": 9.5268,
      "step": 7775
    },
    {
      "epoch": 0.9627521346368024,
      "grad_norm": 0.8742403984069824,
      "learning_rate": 1.8685806212102464e-06,
      "loss": 9.5355,
      "step": 7780
    },
    {
      "epoch": 0.9633708699418388,
      "grad_norm": 0.8398349285125732,
      "learning_rate": 1.8376438559584211e-06,
      "loss": 9.5673,
      "step": 7785
    },
    {
      "epoch": 0.9639896052468754,
      "grad_norm": 0.871108889579773,
      "learning_rate": 1.8067070907065958e-06,
      "loss": 9.5475,
      "step": 7790
    },
    {
      "epoch": 0.9646083405519119,
      "grad_norm": 0.8673151135444641,
      "learning_rate": 1.7757703254547705e-06,
      "loss": 9.5428,
      "step": 7795
    },
    {
      "epoch": 0.9652270758569484,
      "grad_norm": 0.8517378568649292,
      "learning_rate": 1.7448335602029455e-06,
      "loss": 9.5504,
      "step": 7800
    },
    {
      "epoch": 0.9658458111619849,
      "grad_norm": 0.856543242931366,
      "learning_rate": 1.7138967949511202e-06,
      "loss": 9.5522,
      "step": 7805
    },
    {
      "epoch": 0.9664645464670214,
      "grad_norm": 0.8222688436508179,
      "learning_rate": 1.682960029699295e-06,
      "loss": 9.5698,
      "step": 7810
    },
    {
      "epoch": 0.9670832817720579,
      "grad_norm": 0.8497979044914246,
      "learning_rate": 1.6520232644474696e-06,
      "loss": 9.5436,
      "step": 7815
    },
    {
      "epoch": 0.9677020170770945,
      "grad_norm": 0.8410312533378601,
      "learning_rate": 1.6210864991956444e-06,
      "loss": 9.5526,
      "step": 7820
    },
    {
      "epoch": 0.9683207523821309,
      "grad_norm": 0.8880281448364258,
      "learning_rate": 1.590149733943819e-06,
      "loss": 9.558,
      "step": 7825
    },
    {
      "epoch": 0.9689394876871674,
      "grad_norm": 0.8706703782081604,
      "learning_rate": 1.5592129686919936e-06,
      "loss": 9.5304,
      "step": 7830
    },
    {
      "epoch": 0.969558222992204,
      "grad_norm": 0.833598256111145,
      "learning_rate": 1.5282762034401683e-06,
      "loss": 9.56,
      "step": 7835
    },
    {
      "epoch": 0.9701769582972405,
      "grad_norm": 0.863711416721344,
      "learning_rate": 1.497339438188343e-06,
      "loss": 9.5391,
      "step": 7840
    },
    {
      "epoch": 0.9707956936022769,
      "grad_norm": 0.8544066548347473,
      "learning_rate": 1.4664026729365177e-06,
      "loss": 9.5339,
      "step": 7845
    },
    {
      "epoch": 0.9714144289073134,
      "grad_norm": 0.8497339487075806,
      "learning_rate": 1.4354659076846927e-06,
      "loss": 9.5541,
      "step": 7850
    },
    {
      "epoch": 0.97203316421235,
      "grad_norm": 0.8564884066581726,
      "learning_rate": 1.4045291424328674e-06,
      "loss": 9.555,
      "step": 7855
    },
    {
      "epoch": 0.9726518995173865,
      "grad_norm": 0.8726873993873596,
      "learning_rate": 1.3735923771810421e-06,
      "loss": 9.5375,
      "step": 7860
    },
    {
      "epoch": 0.9732706348224229,
      "grad_norm": 0.8448851108551025,
      "learning_rate": 1.3426556119292168e-06,
      "loss": 9.5626,
      "step": 7865
    },
    {
      "epoch": 0.9738893701274595,
      "grad_norm": 0.8630354404449463,
      "learning_rate": 1.3117188466773916e-06,
      "loss": 9.5464,
      "step": 7870
    },
    {
      "epoch": 0.974508105432496,
      "grad_norm": 0.8487564921379089,
      "learning_rate": 1.2807820814255663e-06,
      "loss": 9.5509,
      "step": 7875
    },
    {
      "epoch": 0.9751268407375325,
      "grad_norm": 0.8508400321006775,
      "learning_rate": 1.249845316173741e-06,
      "loss": 9.5403,
      "step": 7880
    },
    {
      "epoch": 0.975745576042569,
      "grad_norm": 0.8591268062591553,
      "learning_rate": 1.2189085509219155e-06,
      "loss": 9.5433,
      "step": 7885
    },
    {
      "epoch": 0.9763643113476055,
      "grad_norm": 0.8432477712631226,
      "learning_rate": 1.1879717856700902e-06,
      "loss": 9.5609,
      "step": 7890
    },
    {
      "epoch": 0.976983046652642,
      "grad_norm": 0.8559973239898682,
      "learning_rate": 1.1570350204182652e-06,
      "loss": 9.5456,
      "step": 7895
    },
    {
      "epoch": 0.9776017819576786,
      "grad_norm": 0.8787040114402771,
      "learning_rate": 1.1260982551664399e-06,
      "loss": 9.5366,
      "step": 7900
    },
    {
      "epoch": 0.978220517262715,
      "grad_norm": 0.8607705235481262,
      "learning_rate": 1.0951614899146146e-06,
      "loss": 9.5582,
      "step": 7905
    },
    {
      "epoch": 0.9788392525677515,
      "grad_norm": 0.8449106812477112,
      "learning_rate": 1.0642247246627893e-06,
      "loss": 9.5534,
      "step": 7910
    },
    {
      "epoch": 0.979457987872788,
      "grad_norm": 0.8692921996116638,
      "learning_rate": 1.033287959410964e-06,
      "loss": 9.5357,
      "step": 7915
    },
    {
      "epoch": 0.9800767231778246,
      "grad_norm": 0.8685254454612732,
      "learning_rate": 1.0023511941591388e-06,
      "loss": 9.5636,
      "step": 7920
    },
    {
      "epoch": 0.980695458482861,
      "grad_norm": 0.860122799873352,
      "learning_rate": 9.714144289073135e-07,
      "loss": 9.5571,
      "step": 7925
    },
    {
      "epoch": 0.9813141937878975,
      "grad_norm": 0.8707717657089233,
      "learning_rate": 9.404776636554882e-07,
      "loss": 9.5623,
      "step": 7930
    },
    {
      "epoch": 0.9819329290929341,
      "grad_norm": 0.8473576903343201,
      "learning_rate": 9.095408984036629e-07,
      "loss": 9.541,
      "step": 7935
    },
    {
      "epoch": 0.9825516643979706,
      "grad_norm": 0.8658425807952881,
      "learning_rate": 8.786041331518378e-07,
      "loss": 9.5572,
      "step": 7940
    },
    {
      "epoch": 0.983170399703007,
      "grad_norm": 0.8624762892723083,
      "learning_rate": 8.476673679000125e-07,
      "loss": 9.5582,
      "step": 7945
    },
    {
      "epoch": 0.9837891350080435,
      "grad_norm": 0.8789843320846558,
      "learning_rate": 8.167306026481872e-07,
      "loss": 9.5376,
      "step": 7950
    },
    {
      "epoch": 0.9844078703130801,
      "grad_norm": 0.8727363348007202,
      "learning_rate": 7.857938373963619e-07,
      "loss": 9.5553,
      "step": 7955
    },
    {
      "epoch": 0.9850266056181166,
      "grad_norm": 0.8724436163902283,
      "learning_rate": 7.548570721445365e-07,
      "loss": 9.5428,
      "step": 7960
    },
    {
      "epoch": 0.985645340923153,
      "grad_norm": 0.848844587802887,
      "learning_rate": 7.239203068927114e-07,
      "loss": 9.5541,
      "step": 7965
    },
    {
      "epoch": 0.9862640762281896,
      "grad_norm": 0.8545457720756531,
      "learning_rate": 6.929835416408861e-07,
      "loss": 9.5348,
      "step": 7970
    },
    {
      "epoch": 0.9868828115332261,
      "grad_norm": 0.8581328392028809,
      "learning_rate": 6.620467763890608e-07,
      "loss": 9.5441,
      "step": 7975
    },
    {
      "epoch": 0.9875015468382626,
      "grad_norm": 0.8800294399261475,
      "learning_rate": 6.311100111372355e-07,
      "loss": 9.5577,
      "step": 7980
    },
    {
      "epoch": 0.9881202821432991,
      "grad_norm": 0.849964439868927,
      "learning_rate": 6.001732458854102e-07,
      "loss": 9.5566,
      "step": 7985
    },
    {
      "epoch": 0.9887390174483356,
      "grad_norm": 0.8678667545318604,
      "learning_rate": 5.69236480633585e-07,
      "loss": 9.5527,
      "step": 7990
    },
    {
      "epoch": 0.9893577527533721,
      "grad_norm": 0.8625238537788391,
      "learning_rate": 5.382997153817598e-07,
      "loss": 9.539,
      "step": 7995
    },
    {
      "epoch": 0.9899764880584087,
      "grad_norm": 0.8717741966247559,
      "learning_rate": 5.073629501299345e-07,
      "loss": 9.5399,
      "step": 8000
    },
    {
      "epoch": 0.9905952233634451,
      "grad_norm": 0.8217761516571045,
      "learning_rate": 4.764261848781091e-07,
      "loss": 9.5562,
      "step": 8005
    },
    {
      "epoch": 0.9912139586684816,
      "grad_norm": 0.8485170006752014,
      "learning_rate": 4.454894196262839e-07,
      "loss": 9.5666,
      "step": 8010
    },
    {
      "epoch": 0.9918326939735181,
      "grad_norm": 0.8648537397384644,
      "learning_rate": 4.145526543744586e-07,
      "loss": 9.549,
      "step": 8015
    },
    {
      "epoch": 0.9924514292785547,
      "grad_norm": 0.8639712333679199,
      "learning_rate": 3.8361588912263333e-07,
      "loss": 9.5341,
      "step": 8020
    },
    {
      "epoch": 0.9930701645835911,
      "grad_norm": 0.8669228553771973,
      "learning_rate": 3.526791238708081e-07,
      "loss": 9.5443,
      "step": 8025
    },
    {
      "epoch": 0.9936888998886276,
      "grad_norm": 0.8382963538169861,
      "learning_rate": 3.217423586189828e-07,
      "loss": 9.5422,
      "step": 8030
    },
    {
      "epoch": 0.9943076351936642,
      "grad_norm": 0.8444123864173889,
      "learning_rate": 2.9080559336715754e-07,
      "loss": 9.5442,
      "step": 8035
    },
    {
      "epoch": 0.9949263704987007,
      "grad_norm": 0.8849198818206787,
      "learning_rate": 2.5986882811533226e-07,
      "loss": 9.5353,
      "step": 8040
    },
    {
      "epoch": 0.9955451058037371,
      "grad_norm": 0.8480545878410339,
      "learning_rate": 2.2893206286350699e-07,
      "loss": 9.5406,
      "step": 8045
    },
    {
      "epoch": 0.9961638411087737,
      "grad_norm": 0.8407676219940186,
      "learning_rate": 1.9799529761168173e-07,
      "loss": 9.5614,
      "step": 8050
    },
    {
      "epoch": 0.9967825764138102,
      "grad_norm": 0.84975665807724,
      "learning_rate": 1.6705853235985645e-07,
      "loss": 9.552,
      "step": 8055
    },
    {
      "epoch": 0.9974013117188467,
      "grad_norm": 0.8644752502441406,
      "learning_rate": 1.361217671080312e-07,
      "loss": 9.5641,
      "step": 8060
    },
    {
      "epoch": 0.9980200470238831,
      "grad_norm": 0.8934885263442993,
      "learning_rate": 1.0518500185620593e-07,
      "loss": 9.5462,
      "step": 8065
    },
    {
      "epoch": 0.9986387823289197,
      "grad_norm": 0.875042200088501,
      "learning_rate": 7.424823660438064e-08,
      "loss": 9.5386,
      "step": 8070
    },
    {
      "epoch": 0.9992575176339562,
      "grad_norm": 0.8324406147003174,
      "learning_rate": 4.331147135255538e-08,
      "loss": 9.5425,
      "step": 8075
    },
    {
      "epoch": 0.9998762529389927,
      "grad_norm": 0.8595783114433289,
      "learning_rate": 1.2374706100730108e-08,
      "loss": 9.5509,
      "step": 8080
    }
  ],
  "logging_steps": 5,
  "max_steps": 8081,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1886570496.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
